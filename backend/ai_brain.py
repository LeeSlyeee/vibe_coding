import numpy as np
import os
import random
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from config import Config
import json
import requests
import re
import time
TRAINING_STATE_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'training_state.json')

try:
    from emotion_codes import EMOTION_CODE_MAP
except ImportError:
    print("Warning: Could not import EMOTION_CODE_MAP from emotion_codes")
    EMOTION_CODE_MAP = {}

# TensorFlow/Keras Import (Optional)
try:
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
    from tensorflow.keras.utils import to_categorical
    import pandas as pd
    TENSORFLOW_AVAILABLE = True
    print("AI Brain: TensorFlow Available.")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"AI Brain: Local Emotion Model support skipped ({e}).")



class EmotionAnalysis:
    def __init__(self, light_mode=False):
        self.tokenizer = None
        self.model = None
        self.max_len = 50
        self.vocab_size = 0
        
        # 60 Ultra-Fine-Grained Emotion Classes
        self.sorted_codes = sorted(EMOTION_CODE_MAP.keys())
        self.classes = [EMOTION_CODE_MAP[code] for code in self.sorted_codes]
        self.code_to_idx = {code: i for i, code in enumerate(self.sorted_codes)}
        
        # We will load keywords from DB for fallback/learning
        try:
            from pymongo import MongoClient
            from config import Config
            self.mongo_client = MongoClient(Config.MONGO_URI)
            self.db = self.mongo_client.get_database() # Uses default db from URI
            print("AI Brain: Connected to MongoDB.")
        except Exception as e:
            print(f"AI Brain: MongoDB Connection Failed: {e}")
            self.db = None

        self.train_texts = []
        self.train_labels = np.array([], dtype=int)
        
        self.comment_bank = {} # Will load from file

        
        # Initialize attributes
        # Removed Polyglot-Ko attributes

               
        # === Tensorflow / LSTM Model Logic ===
        # [User Request] Switch to Full LLM (Gemma) Mode. Disabling LSTM loading.
        self.use_lstm = False 
        
        if self.use_lstm and TENSORFLOW_AVAILABLE and not light_mode:
            self.tokenizer = Tokenizer()
            # Check for saved model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            self.model_path = os.path.join(base_dir, 'emotion_model.h5')
            self.tokenizer_path = os.path.join(base_dir, 'tokenizer.pickle')
            
            # Check Training Condition
            current_count = self._get_keyword_count()
            last_count = self._get_last_trained_count()
            diff = current_count - last_count
            
            print(f"ğŸ“Š Training Check: Current Keywords={current_count}, Last Trained={last_count}, Diff={diff}")
            
            should_train = (diff >= 100)
            model_exists = os.path.exists(self.model_path) and os.path.exists(self.tokenizer_path)

            if should_train:
                print("âš ï¸ Should train, but skipping for now to rely on Local LLM/Fallback.")
                self._save_training_state(current_count)
            elif model_exists:
                print("ğŸ“¦ Models found. Loading existing models...")
                self._load_existing_models()
                print("âœ… Emotion Model loaded.")
            else:
                print("âš ï¸ No models found. Using Keyword Fallback.")
            
            print("AI Model initialization finished.")

        else: 
             print("Initializing Gemma-based Emotion Analysis (LSTM Disabled)...")

        # Load Comment Bank (Safety Net) - Always load this
        self.load_comment_bank()
        self.load_emotion_bank()

    def _sanitize_context(self, text):
        """
        Privacy Guard: Removes potential sensitive information before sending to external API.
        - Truncates long text
        - Focuses on sentiment-carrying words
        """
        if not text: return ""
        # Simple privacy protection: Remove common patterns (emails, phone numbers)
        import re
        text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', text)
        text = re.sub(r'\d{2,3}-\d{3,4}-\d{4}', '[PHONE]', text)
        
        # To further protect privacy, we could use the local emotion results 
        # instead of raw text, but for 'insight' we need some context.
        # We limit the length to prevent sending too much detail.
        return text[:100].strip() + "..." if len(text) > 100 else text

    def _get_keyword_count(self):

        """Get total count of emotion keywords from DB (MongoDB)"""
        if self.db is None:
            return 0
        try:
            count = self.db.emotion_keywords.count_documents({})
            return count
        except Exception as e:
            print(f"Error counting keywords: {e}")
            return 0

    def _get_last_trained_count(self):
        """Read last trained count from JSON"""
        if os.path.exists(TRAINING_STATE_FILE):
            try:
                with open(TRAINING_STATE_FILE, 'r') as f:
                    data = json.load(f)
                    return data.get('last_keyword_count', 0)
            except:
                return 0
        return 0

    def _save_training_state(self, count):
        """Save current keyword count to JSON"""
        try:
            with open(TRAINING_STATE_FILE, 'w') as f:
                json.dump({'last_keyword_count': count}, f)
        except Exception as e:
            print(f"Error saving training state: {e}")

    def _load_existing_models(self):
        """Helper to load existing models"""
        try:
            import pickle
            from tensorflow.keras.models import load_model
            
            self.model = load_model(self.model_path)
            
            with open(self.tokenizer_path, 'rb') as handle:
                self.tokenizer = pickle.load(handle)
                
            self.vocab_size = len(self.tokenizer.word_index) + 1
            print("Emotion Model loaded.")
                
        except Exception as e:
            print(f"Error loading models: {e}.")
            # Assuming fallback will take over

    def load_comment_bank(self):
        """Load curated advice from JSON"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.comment_bank = data.get('keywords', {})
                print(f"Loaded {len(self.comment_bank)} keyword categories from Comment Bank.")
            else:
                print("Comment Bank file not found. Skipping.")
        except Exception as e:
            print(f"Error loading comment bank: {e}")

    def generate_keyword_comment(self, user_input):
        """Phase 1: Hybrid Keyword System (Priority 1)"""
        if not self.comment_bank or not user_input:
            return None
            
        # Extract text if input is dict
        if isinstance(user_input, dict):
            text = f"{user_input.get('event', '')} {user_input.get('emotion', '')} {user_input.get('self_talk', '')}"
        else:
            text = str(user_input)
            
        for category, content in self.comment_bank.items():
            # Safety check: content must be a dict
            if not isinstance(content, dict):
                continue
                
            if category in text:
                return content.get('default', "í˜ë‚´ì„¸ìš”.")
                
            keywords = content.get('emotion_keywords', [])
            for k in keywords:
                if k in text:
                    return content.get('default', "í˜ë‚´ì„¸ìš”.")
        return None

    def load_emotion_bank(self):
        """Load 60-class emotion advice"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'emotion_comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.emotion_bank = data.get('keywords', {}) # Structure reuse
                print(f"Loaded {len(self.emotion_bank)} emotion categories.")
            else:
                self.emotion_bank = {}
        except Exception as e:
            print(f"Error loading emotion bank: {e}")
            self.emotion_bank = {}

    def generate_label_comment(self, emotion_label):
        """Phase 1.5: Label-Based Retrieval (Priority 2)"""
        if not self.emotion_bank:
            return None
            
        try:
            if '(' in emotion_label:
                label_key = emotion_label.rsplit(' (', 1)[0]
                pass
            else:
                label_key = emotion_label
        except:
             label_key = emotion_label
        
        # Exact match attempt
        if label_key in self.emotion_bank:
            return self.emotion_bank[label_key].get('default')
            
        # Fuzzy match 
        for key in self.emotion_bank:
            if key in emotion_label:
                return self.emotion_bank[key].get('default')
        
        return None
        



    
    def generate_pre_write_insight(self, recent_diaries, weather=None, weather_stats=None):
        """
        Generates a warm insight using Local Gemma 2 (Ollama).
        """
        import requests
        import json

        print(f"ğŸ” [Insight] Request received. Recent diaries count: {len(recent_diaries)}, Weather: {weather}")
        
        if not recent_diaries:
            return "ì˜¤ëŠ˜ì˜ ì²« ê¸°ë¡ì„ ì‹œì‘í•´ë³´ì„¸ìš”! ì†”ì§í•œ ë§ˆìŒì„ ë‹´ìœ¼ë©´ ë©ë‹ˆë‹¤."

        try:
            # Construct Prompt
            diary_context = ""
            for d in recent_diaries:
                sanitized_event = self._sanitize_context(d.get('event',''))
                diary_context += f"- [{d.get('date','')}] ê¸°ë¶„:{d.get('mood','')} / ë‚´ìš©:{sanitized_event}\n"

            weather_info = f"ì˜¤ëŠ˜ì˜ ë‚ ì”¨: {weather}" if weather else "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´ ì—†ìŒ"
            stats_info = f" (ê³¼ê±° ì´ ë‚ ì”¨ì— ë‹¹ì‹ ì€ ì£¼ë¡œ {weather_stats} ê°ì •ì„ ëŠë¼ì…¨ë„¤ìš”)" if weather_stats else ""

            prompt_text = (
                "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§€ë‚œ ì¼ê¸° ê¸°ë¡ê³¼ ì˜¤ëŠ˜ì˜ ë‚ ì”¨, ê·¸ë¦¬ê³  'ê³¼ê±° ë‚ ì”¨ë³„ ê°ì • íŒ¨í„´'ì„ ë¶„ì„í•˜ì—¬ ë”°ëœ»í•œ í•œ ë¬¸ì¥ì˜ ì¡°ì–¸ì„ ê±´ë„¤ëŠ” ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\n\n"
                f"### {weather_info}{stats_info}\n"
                "### ì‚¬ìš©ìì˜ ìµœê·¼ 1ì£¼ì¼ íë¦„\n"
                f"{diary_context}\n"
                "### ì§€ì‹œì‚¬í•­\n"
                "1. ë°˜ë“œì‹œ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "2. [í•„ìˆ˜] ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë‚˜ ê³„ì ˆê°ì„ ì–¸ê¸‰í•˜ë©° ì‹œì‘í•˜ì„¸ìš”. (ì˜ˆ: 'ë¹„ê°€ ì˜¤ëŠ” ë‚ ì—”...', 'ë§‘ì€ í–‡ì‚´ì²˜ëŸ¼...')\n"
                "3. ìµœê·¼ 1ì£¼ì¼ê°„ì˜ ê°ì • íë¦„ì´ ì¢‹ì€ì§€ ë‚˜ìœì§€ë¥¼ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ í•˜ì„¸ìš”.\n"
                "4. 'ì˜¤ëŠ˜ í•˜ë£¨ ì‘ì›í•©ë‹ˆë‹¤' ê°™ì€ ë»”í•œ ë§ì€ ê¸ˆì§€ì…ë‹ˆë‹¤.\n"
                "5. 40ì~80ì ë‚´ì™¸ë¡œ ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§(í•´ìš”ì²´)ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n\n"
                "ìƒë‹´ì‚¬ ì¡°ì–¸(ë‚ ì”¨ì™€ ê°ì • íë¦„ì´ í†µí•©ëœ í•œ ë¬¸ì¥):"
            )

            # Ollama Payload
            payload = {
                "model": "maum-on-gemma",
                "prompt": prompt_text,
                "stream": False,
                # No 'format': 'json' here because we want free text
                "options": {
                    "temperature": 0.5,
                    "num_predict": 100 
                }
            }
            
            print(f"ğŸ¦™ [Insight] Requesting Ollama (Maum-On Gemma)...")
            url = "http://localhost:11434/api/generate"
            
            # Timeout Increased to 60s (OCI CPU might be slow or busy)
            response = requests.post(url, json=payload, timeout=60)
            
            if response.status_code != 200:
                print(f"âŒ Ollama Insight Error {response.status_code}: {response.text}")
                return None
                
            result = response.json()
            response_text = result.get('response', '').strip()
            
            # Cleanup quotes if model adds them
            if response_text.startswith('"') and response_text.endswith('"'):
                response_text = response_text[1:-1]
                
            print(f"âœ… [Insight] Gemma Success: {response_text}")
            return response_text

        except Exception as e:
            print(f"âŒ [Insight] Inference Failed: {str(e)}")
            return None

    def generate_chat_reaction(self, user_text):
        """
        Generates a rich, empathetic reaction to the user's chat input.
        """
        if not user_text: return None
        
        sanitized = self._sanitize_context(user_text)
        
        prompt_text = (
            f"ë‚´ë‹´ìì˜ ë§: \"{sanitized}\"\n\n"
            "ë„ˆëŠ” ë‹¤ì •í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ì‹¬ë¦¬ ìƒë‹´ì‚¬ì•¼. ë‚´ë‹´ìì˜ ë§ì„ ë“£ê³  **í’ë¶€í•œ ê³µê°ê³¼ ê¸ì •ì ì¸ í”¼ë“œë°±**ì„ í•´ì¤˜.\n"
            "ì§€ì‹œì‚¬í•­:\n"
            "1. ë‚´ë‹´ìì˜ ê°ì •ì´ë‚˜ í–‰ë™ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ë©° 'ê·¸ë ‡êµ°ìš”', 'ì •ë§ ~í•˜ì…¨ê² ì–´ìš”'ë¼ê³  ê³µê°í•´.\n"
            "2. ê·¸ í–‰ë™ì´ë‚˜ ê°ì •ì´ ì–¼ë§ˆë‚˜ ì†Œì¤‘í•œì§€, ë˜ëŠ” ì˜ ëŒ€ì²˜í–ˆëŠ”ì§€ ì¹­ì°¬í•´ì¤˜. (í”¼ë“œë°±)\n"
            "3. ë§íˆ¬ëŠ” ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ 'í•´ìš”ì²´'ë¥¼ ì¨.\n"
            "4. ì§ˆë¬¸ì€ í•˜ì§€ ë§ˆ. (ë‹¤ìŒ ì§ˆë¬¸ì€ ì •í•´ì ¸ ìˆì–´)\n"
            "5. 2~3ë¬¸ì¥ìœ¼ë¡œ 100ì ì´ë‚´ë¡œ ì‘ì„±í•´.\n\n"
            "ë¦¬ì•¡ì…˜:"
        )
        
        try:
            payload = {
                "model": "maum-on-gemma",
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 150
                }
            }
            
            url = "http://localhost:11434/api/generate"
            response = requests.post(url, json=payload, timeout=20)
            
            if response.status_code == 200:
                result = response.json()
                reaction = result.get('response', '').strip()
                if reaction.startswith('"') and reaction.endswith('"'):
                    reaction = reaction[1:-1]
                return reaction
            return None
        except Exception as e:
            print(f"âŒ Reaction Gen Error: {e}")
            # Fallback Reactions (Safety Net)
            fallbacks = [
                "ê·¸ë ‡êµ°ìš”. ì´ì•¼ê¸°í•´ì£¼ì…”ì„œ ì •ë§ ê³ ë§ˆì›Œìš”. ğŸ˜Œ",
                "ì €ëŸ°, ê·¸ëŸ° ì¼ì´ ìˆìœ¼ì…¨êµ°ìš”. ë§ˆìŒì´ ì“°ì´ë„¤ìš”. ğŸ¥º",
                "ì •ë§ ê³µê°ì´ ê°€ìš”. ğŸ’­",
                "ë„¤, ê³„ì† ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”. ì œê°€ ë“£ê³  ìˆì–´ìš”. ğŸ‘‚",
                "ì–´ë–¤ ê¸°ë¶„ì¸ì§€ ì¡°ê¸ˆì€ ì•Œ ê²ƒ ê°™ì•„ìš”. ğŸ’«"
            ]
            return random.choice(fallbacks)

    def _rebuild_inference_models(self):
        # Seq2Seq Removed
        pass

    # Function moved to end of file to fix indentation 

    def train_comment_model(self):
        # Seq2Seq Removed
        pass

    def predict(self, text):
        import time
        start_time = time.time()
        
        if not text: 
            return {"emotion": "ë¶„ì„ ë¶ˆê°€", "comment": "ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."}

        emotion_result = "ë¶„ì„ ë¶ˆê°€"
        
        # [User Request] Gemma-First Analysis
        try:
            llm_emotion, llm_comment = self.analyze_diary_with_local_llm(text)
            if llm_emotion:
                 emotion_result = llm_emotion
            else:
                 emotion_result = self._fallback_predict(text)
            
            # Store comment for next step
            self.temp_llm_comment = llm_comment
        except:
            emotion_result = self._fallback_predict(text)
            self.temp_llm_comment = None
            
        # 2. Comment Generation
        comment_result = getattr(self, 'temp_llm_comment', None)
        



        
        # Final Fallback
        if not comment_result:
            comment_result = self.generate_keyword_comment(text) or "ì˜¤ëŠ˜ í•˜ë£¨ë„ ì •ë§ ìˆ˜ê³ í•˜ì…¨ì–´ìš”."
            
        print(f"âœ¨ [Timer] Total AI Analysis took: {time.time() - start_time:.3f}s")
        return {
            "emotion": emotion_result,
            "comment": comment_result
        }

    def _fallback_predict(self, text):
        # Load keywords from MongoDB
        if self.db is None:
             return "ë¶„ì„ ë¶ˆê°€"

        try:
            # Fetch all keywords from Mongo
            # This is inefficient for large datasets but ok for small keywords bank
            keywords = list(self.db.emotion_keywords.find())
            
            scores = [0] * 5
            found_any = False
            
            for kw in keywords:
                if kw['keyword'] in text:
                    scores[kw['emotion_label']] += kw['frequency']
                    found_any = True
            
            if found_any:
                max_score = max(scores)
                total_score = sum(scores)
                confidence = (max_score / total_score * 100) if total_score > 0 else 85.0
                best_idx = scores.index(max_score)
                return f"{self.classes[best_idx]} ({confidence:.1f}%)"
            else:
                return "ê·¸ì €ê·¸ë˜ (40.0%)" 
        except Exception as e:
            print(f"Fallback error: {e}")
            return "ë¶„ì„ ë¶ˆê°€"

    def load_sentiment_corpus(self):
        """
        Load 'Sentiment Dialogue Corpus' (Training & Validation).
        Use Full 60-Class Granularity (E10 ~ E69).
        """
        import json
        
        files = [
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json',
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.json'
        ]
        
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        import glob
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Use glob to avoid Unicode normalization issues (NFC vs NFD) on macOS
        files = glob.glob(os.path.join(base_dir, "*Training.json")) + glob.glob(os.path.join(base_dir, "*Validation.json"))
        
        if not files:
             print("No corpus files found via glob!")
        
        for fpath in files:
            fname = os.path.basename(fpath)
            # fpath is already absolute from glob
             
            print(f"Loading corpus: {fname}...")
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                new_texts = []
                new_labels = []
                
                for entry in data:
                    try:
                        etype = entry.get('profile', {}).get('emotion', {}).get('type', '')
                        
                        # Only use codes defined in our map (E10-E69)
                        if etype in self.code_to_idx:
                            # Extract user text
                            content = entry.get('talk', {}).get('content', {})
                            text = content.get('HS01', '')
                            system_text = content.get('SS01', '')
                            
                            if text:
                                label_idx = self.code_to_idx[etype]
                                new_texts.append(text)
                                new_labels.append(label_idx)
                                
                                # Store for comment training if pair exists
                                if system_text:
                                    self.conversation_pairs.append((text, system_text))
                                    
                    except Exception as e:
                        continue
                        
                # Add to training set
                self.train_texts.extend(new_texts)
                self.train_labels = np.concatenate((self.train_labels, np.array(new_labels)))
                
                print(f"Added {len(new_texts)} samples from {fname}")
                
            except Exception as e:
                print(f"Error loading corpus {fname}: {e}")


    def load_db_data(self):
        """
        Load data from the Diary table to fine-tune the model.
        Returns:
            X (list): Combined text (event + emotion_desc + self_talk)
            y (list): Emotion labels (0-4)
        """
        print("Loading data from Database...")
        session = self.Session()
        X = []
        y = []
        try:
            from models import Diary
            diaries = session.query(Diary).all()
            
            # Mood Level to Label Mapping
            # 5(Happy)->0, 4(Calm)->1, 3(Neutral)->2, 2(Depressed)->3, 1(Angry)->4
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            
            for d in diaries:
                if not d.mood_level: continue
                
                label = mapping.get(d.mood_level)
                if label is None: continue
                
                # Combine text fields for rich context
                text = f"{d.event} {d.emotion_desc} {d.self_talk}"
                X.append(text)
                y.append(label)
                
            print(f"Loaded {len(X)} samples from Database.")
            return X, y
            
        except Exception as e:
            print(f"Error loading DB data: {e}")
            return [], []
        finally:
            session.close()





    def train_comment_model(self):
        """
        Train a Seq2Seq model using ChatbotData.csv AND Sentiment Dialogue Corpus
        """
        if not TENSORFLOW_AVAILABLE: return

        print("Training Comment Generation Model (Seq2Seq)...")
        try:
            import os
            import pickle
            
            # 1. Load ChatbotData.csv
            base_dir = os.path.dirname(os.path.abspath(__file__))
            data_path = os.path.join(base_dir, 'ChatbotData.csv')
            questions = []
            answers = []
            
            if os.path.exists(data_path):
                df = pd.read_csv(data_path)
                # Take a sample to keep training time reasonable, or all? 
                # Let's take mixed sample.
                df = df.sample(frac=1).reset_index(drop=True)
                questions = df['Q'].astype(str).tolist()[:5000] # Cap at 5000 for speed
                # Char-level: Use \t for Start, \n for End
                answers = df['A'].apply(lambda x: '\t' + str(x) + '\n').tolist()[:5000]
            
            # 2. Add Sentiment Dialogue Corpus
            if hasattr(self, 'conversation_pairs'):
                print(f"Integrating {len(self.conversation_pairs)} pairs from Sentiment Corpus...")
                # Sample 5000 from here too to balance
                import random
                pairs = self.conversation_pairs
                if len(pairs) > 5000:
                    pairs = random.sample(pairs, 5000)
                
                for q, a in pairs:
                    questions.append(str(q))
                    answers.append('\t' + str(a) + '\n')
            
            print(f"Total Comment Training Samples: {len(questions)}")

            # Shared Tokenizer (Character Level for better Korean convergence without Mecab)
            # Filters: Don't filter \t and \n as they are our tokens!
            self.comment_tokenizer = Tokenizer(char_level=True, filters='!"#$%&()*+,-./:;<=>?@[\\]^`{|}~') 
            self.comment_tokenizer.fit_on_texts(questions + answers)
            
            vocab_size = len(self.comment_tokenizer.word_index) + 1
            print(f"Comment Vocab Size (Char-level): {vocab_size}")

            # Encoder Data
            tokenized_Q = self.comment_tokenizer.texts_to_sequences(questions)
            encoder_input_data = pad_sequences(tokenized_Q, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Data
            tokenized_A = self.comment_tokenizer.texts_to_sequences(answers)
            decoder_input_data = pad_sequences(tokenized_A, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Target (Shifted-by-one)
            decoder_target_data = np.zeros_like(decoder_input_data, dtype="float32")
            decoder_target_data[:, :-1] = decoder_input_data[:, 1:]
            decoder_target_data = np.expand_dims(decoder_target_data, -1)

            # Model Architecture
            latent_dim = 256
            
            # Encoder
            encoder_inputs = Input(shape=(None,), name='enc_input')
            enc_emb_layer = Embedding(vocab_size, latent_dim, name='enc_embedding')
            enc_emb = enc_emb_layer(encoder_inputs)
            encoder_lstm = LSTM(latent_dim, return_state=True, name='enc_lstm')
            encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
            encoder_states = [state_h, state_c]
            
            # Decoder
            decoder_inputs = Input(shape=(None,), name='dec_input')
            dec_emb_layer = Embedding(vocab_size, latent_dim, name='dec_embedding')
            dec_emb = dec_emb_layer(decoder_inputs)
            decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='dec_lstm')
            decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
            decoder_dense = Dense(vocab_size, activation='softmax', name='dec_dense')
            decoder_outputs = decoder_dense(decoder_outputs)
            
            self.comment_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
            self.comment_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
            print("Fitting Seq2Seq Model (Char-level)...")
            self.comment_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
                                   batch_size=64, epochs=15, validation_split=0.2, verbose=1)
                                   
            print("Comment Model Trained. Saving...")
            
            # Save Main Model
            self.comment_model.save(self.comment_model_path)
            
            # Save Tokenizer
            with open(self.comment_tokenizer_path, 'wb') as handle:
                pickle.dump(self.comment_tokenizer, handle)

            # Construct & Save Inference Models
            self.enc_model = Model(encoder_inputs, encoder_states)
            self.enc_model.save(os.path.join(base_dir, 'comment_enc_model.h5'))
            
            dec_state_input_h = Input(shape=(latent_dim,), name='dec_input_h')
            dec_state_input_c = Input(shape=(latent_dim,), name='dec_input_c')
            dec_states_inputs = [dec_state_input_h, dec_state_input_c]
            
            dec_emb2 = dec_emb_layer(decoder_inputs)
            dec_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states_inputs)
            dec_states2 = [state_h2, state_c2]
            dec_outputs2 = decoder_dense(dec_outputs2)
            
            self.dec_model = Model([decoder_inputs] + dec_states_inputs, [dec_outputs2] + dec_states2)
            self.dec_model.save(os.path.join(base_dir, 'comment_dec_model.h5'))
            
            print("Inference models saved.")
            
        except Exception as e:
            print(f"Error training comment model: {e}")

    def _rebuild_inference_models(self):
        """Rebuild inference models from loaded main model or load separate files"""
        try:
            from tensorflow.keras.models import load_model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            enc_path = os.path.join(base_dir, 'comment_enc_model.h5')
            dec_path = os.path.join(base_dir, 'comment_dec_model.h5')
            
            if os.path.exists(enc_path) and os.path.exists(dec_path):
                self.enc_model = load_model(enc_path)
                self.dec_model = load_model(dec_path)
                print("Inference models loaded successfully.")
            else:
                print("Inference model files missing. Comment generation may fail.")
        except Exception as e:
            print(f"Error loading inference models: {e}")

    # Seq2Seq Generation helpers removed



    def analyze_diary_with_local_llm(self, text, history_context=None):
        # [Local AI Mode] Uses Local Ollama (Gemma 2) for Analysis.
        # Free, Unlimited, Private.
        import requests
        import json
        
        # Local Ollama URL
        print(f"ğŸ¦™ [Local AI] Requesting Ollama (Maum-On Gemma)...", end=" ", flush=True)
        try:
            url = "http://localhost:11434/api/generate"
            
            # Context Injection
            context_section = ""
            if history_context:
                context_section = (
                    f"### [ì°¸ê³ : ë‚´ë‹´ìì˜ ê³¼ê±° ê¸°ë¡]\n"
                    f"{history_context}\n"
                    f"(ì§€ì¹¨: ìœ„ ê³¼ê±° ê¸°ë¡ì˜ íë¦„ì„ ì°¸ê³ í•˜ì—¬, ë§¥ë½ì´ ì´ì–´ì§€ëŠ” ê¹Šì´ ìˆëŠ” ê³µê° ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.)\n\n"
                )

            # Extract Sleep Info if available in text (which logic passes as simple text usually? No, caller passes string)
            # Actually, caller passes combined string. Let's assume input text has structure or we rely on the text content.
            # But the user asked to tune analysis based on required sleep input.
            # Let's see how 'analyze_diary_logic' calls this. It calls with `full_text`.
            # We should probably update the prompt to explicitly look for sleep mentions if possible, or just emphasize it.
            
            # Better: The prompt format below.
            
            prompt_text = (
                f"ë„ˆëŠ” ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ë‚´ë‹´ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ë¶„ì„í•´ì¤˜.\n"
                f"{context_section}"
                f"### [ì˜¤ëŠ˜ì˜ ì¼ê¸° ë°ì´í„°]:\n{text}\n\n"
                f"### [ë¶„ì„ ì§€ì¹¨]:\n"
                f"1. ë‚´ë‹´ìì˜ 'ìˆ˜ë©´ ìƒíƒœ(ì )'ì™€ 'ê°ì •'ì˜ ì—°ê´€ì„±ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•´ì¤˜. (ì˜ˆ: ì ì„ ëª» ìì„œ ì˜ˆë¯¼í•œì§€, í‘¹ ìì„œ ê°œìš´í•œì§€)\n"
                f"2. ë‹¨ìˆœí•œ ê³µê°ì„ ë„˜ì–´, ìˆ˜ë©´ íŒ¨í„´ì´ ê°ì •ì— ë¯¸ì¹œ ì˜í–¥ì„ ì–¸ê¸‰í•˜ë©° ìœ„ë¡œí•´ì¤˜.\n\n"
                f"### [ë‹µë³€ í˜•ì‹]:\n"
                f"Emotion: (happy, sad, angry, neutral, panic ì¤‘ í•˜ë‚˜)\n"
                f"Confidence: (0~100 ìˆ«ìë§Œ)\n"
                f"Comment: (ìˆ˜ë©´ ìƒíƒœë¥¼ ì–¸ê¸‰í•˜ë©° 50ì ì´ë‚´ì˜ ë”°ëœ»í•œ í•œêµ­ì–´ ìœ„ë¡œ)\n"
                f"ë°˜ë“œì‹œ ìœ„ í˜•ì‹ë§Œ ì§€ì¼œì„œ ë‹µë³€í•´."
            )
            
            payload = {
                "model": "maum-on-gemma",
                "prompt": prompt_text,
                "stream": False,
                # "format": "json"  <-- REMOVED: Cause of hanging
                "options": {
                    "temperature": 0.3, # Low temp for stability
                    "num_predict": 150
                }
            }
            
            # Timeout 60s
            response = requests.post(url, json=payload, timeout=60)
            
            if response.status_code != 200:
                print(f"âŒ Ollama Error {response.status_code}: {response.text}")
                return None, None
                
            result = response.json()
            response_text = result.get('response', '').strip()
            print(f"ğŸ” Raw Output: {response_text}")
            
            # Regex Parsing
            import re
            
            # 1. Emotion
            emotion_match = re.search(r"Emotion:\s*([a-zA-Z]+)", response_text, re.IGNORECASE)
            emotion_str = emotion_match.group(1).lower() if emotion_match else "neutral"
            
            # 2. Confidence
            conf_match = re.search(r"Confidence:\s*(\d+)", response_text)
            confidence = int(conf_match.group(1)) if conf_match else 80
            
            # 3. Comment
            comment_match = re.search(r"Comment:\s*(.*)", response_text, re.DOTALL)
            comment = comment_match.group(1).strip() if comment_match else "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”."
            
            # Remove any trailing quotes if model added them
            if comment.startswith('"') and comment.endswith('"'):
                comment = comment[1:-1]

            # Map to Korean
            emotion_map = {
                "happy": "í–‰ë³µí•´", "joy": "í–‰ë³µí•´", 
                "sad": "ìš°ìš¸í•´", "depressed": "ìš°ìš¸í•´", 
                "neutral": "í‰ì˜¨í•´", "calm": "í‰ì˜¨í•´", "soso": "ê·¸ì €ê·¸ë˜",
                "angry": "í™”ê°€ë‚˜", "annoyed": "í™”ê°€ë‚˜", 
                "panic": "ìš°ìš¸í•´", "anxious": "ìš°ìš¸í•´"
            }
            
            korean_emotion = emotion_map.get(emotion_str, "í‰ì˜¨í•´")
            formatted_prediction = f"'{korean_emotion} ({confidence}%)'"
            
            return formatted_prediction, comment
                
        except Exception as e:
            print(f"âŒ Local AI Error: {e}")
            return None, None

    def generate_comment(self, prediction_text, user_text=None):
        # Generate a supportive comment.
        # Priority: 1. Keyword Bank (Safety Net) 2. AI Generation (Seq2Seq) 3. Fallback
        if user_text:
             # Try Local LLM First if available (or assume it was already tried in predict)
             # But here this method is likely legacy or fallback. 
             pass


        # 1. Phase 1: Keyword Safety Net (Highest Priority)
        if user_text:
            keyword_comment = self.generate_keyword_comment(user_text)
            if keyword_comment:
                 return keyword_comment

        if not prediction_text or "ë¶„ì„ ë¶ˆê°€" in prediction_text:
            return "ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“¤ë ¤ì£¼ì„¸ìš”. í•­ìƒ ë“£ê³  ìˆì„ê²Œìš”."

        # Extract strict label (remove confidence score)
        # e.g. "ë¶„ë…¸ (ë°°ì‹ ê°) (85.0%)" -> "ë¶„ë…¸ (ë°°ì‹ ê°)"
        try:
             emotion_label_only = prediction_text.rsplit(' (', 1)[0]
        except:
             emotion_label_only = prediction_text.split()[0]



        # 3. Phase 1.5: Label-based Specific Advice (Fallback)
        label_comment = self.generate_label_comment(emotion_label_only)
        if label_comment:
             return label_comment

        try:
            label = prediction_text.split()[0] # e.g. "í–‰ë³µí•´"
            
            # 4. Fallback
            return "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”."
            
        except Exception as e:
            print(f"Comment Gen Error: {e}")
            return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ì´í•´í•´ìš”."


    def generate_comprehensive_report(self, diary_summary):
        """
        Generates a detailed 10-paragraph psychological report using Local Gemma 2.
        """
        import requests
        print("ğŸ§  [Brain] Generating Comprehensive Report...")
        
        try:
            url = "http://localhost:11434/api/generate"
            
            prompt_text = (
                "## SYSTEM: You represent a thoughtful, empathetic counselor with 20 years of experience. You must ANSWER IN KOREAN ONLY.\n"
                "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë² í…Œë‘ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ë‹´ì(ì‚¬ìš©ì)ì˜ ì¼ê¸° ê¸°ë¡ê³¼ í†µê³„ë¥¼ ìì„¸íˆ ì½ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
                f"### [ì‚¬ìš©ì ë°ì´í„°]\n{diary_summary}\n\n"
                "### [ì‘ì„± ì§€ì¹¨]\n"
                "1. **ì–¸ì–´**: ë°˜ë“œì‹œ **í•œêµ­ì–´(Korean)**ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "2. **í˜•ì‹**: ì‚¬ìš©ìì—ê²Œ ë³´ë‚´ëŠ” 'ì‹¬ì¸µ ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸' í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "3. **ë¶„ëŸ‰**: ë°˜ë“œì‹œ **ì„œë¡ -ë³¸ë¡ (ì§„ë‹¨)-ê²°ë¡ (ì²˜ë°©)**ì˜ íë¦„ì„ ê°–ì¶˜ **ì´ 10ë¬¸ë‹¨ ì´ìƒì˜ ê¸´ ê¸€**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
                "4. **ì–´ì¡°**: ì „ë¬¸ì ì¸ ì‹¬ë¦¬í•™ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ë˜, ë”°ëœ»í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ í’€ì–´ì£¼ì„¸ìš”.\n\n"
                "### [ë¦¬í¬íŠ¸ êµ¬ì¡°]\n"
                "1ë¶€. **ë§ˆìŒì˜ ì§€ë„ (í˜„ìƒ ì§„ë‹¨)** (5ë¬¸ë‹¨)\n"
                "   - ë‚´ë‹´ìê°€ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê°ì • ì–¸ì–´ì™€ ë‚´ë©´ì˜ ìƒíƒœ ë¶„ì„\n"
                "   - ë°˜ë³µë˜ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ íŒ¨í„´ì´ë‚˜ ê°ì •ì˜ íŠ¸ë¦¬ê±° íŒŒì•…\n"
                "   - ìˆ¨ê²¨ì§„ ê¸ì •ì ì¸ ìì›ì´ë‚˜ ê°•ì  ë°œêµ´\n\n"
                "2ë¶€. **ë‚˜ì•„ê°€ì•¼ í•  ê¸¸ (ë¯¸ë˜ ì²˜ë°©)** (5ë¬¸ë‹¨)\n"
                "   - í˜„ì¬ ìƒíƒœì—ì„œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‹¬ë¦¬ ê¸°ë²• 3ê°€ì§€ (ACT, CBT ë“± í™œìš©)\n"
                "   - ê°ì •ì˜ íŒŒë„ë¥¼ ë‹¤ìŠ¤ë¦¬ëŠ” ìƒí™œ ìŠµê´€ ì œì•ˆ\n"
                "   - ìƒë‹´ì‚¬ë¡œì„œ ì „í•˜ëŠ” ì§„ì‹¬ ì–´ë¦° ê²©ë ¤ì™€ í¬ë§ì˜ ë©”ì‹œì§€\n\n"
                "**ì¤‘ìš”: ëª¨ë“  ë‹µë³€ì€ ì™„ë²½í•œ í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë²ˆì—­íˆ¬ê°€ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.**\n"
                "ì§€ê¸ˆ ë°”ë¡œ í•œêµ­ì–´ë¡œ ë¦¬í¬íŠ¸ ì‘ì„±ì„ ì‹œì‘í•˜ì„¸ìš”."
            )
            
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 4096, # Maximum length
                    "repeat_penalty": 1.1,
                    "top_k": 40,
                    "top_p": 0.9
                }
            }
            
            # Timeout 600s (10 mins) - Increased for OCI environment
            response = requests.post(url, json=payload, timeout=600)
            
            if response.status_code == 200:
                result = response.json().get('response', '')
                return result
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ë„ì¤‘ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ Report Generation Error: {e}")
            return "ë¦¬í¬íŠ¸ ìƒì„± ì‹œìŠ¤í…œì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_long_term_insight(self, report_history):
        """
        [Meta-Analysis] Analyzes multiple past reports to find long-term patterns.
        """
        import requests
        print(f"ğŸ§  [Brain] Generating Long-Term Insight from {len(report_history)} reports...")
        
        if not report_history:
            return "ë¶„ì„í•  ê³¼ê±° ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
        try:
            url = "http://localhost:11434/api/generate"
            
            # Construct context from history
            history_context = ""
            for i, r in enumerate(report_history):
                date = r.get('date', 'Unknown Date')
                content = r.get('content', '')[:500] # Truncate to save context window
                history_context += f"### [ë¦¬í¬íŠ¸ {i+1} - {date}]\n{content}...\n\n"
                
            prompt_text = (
                "## SYSTEM: You represent a wise psychologist specializing in long-term therapy. Answer in KOREAN ONLY.\n"
                "ë‹¹ì‹ ì€ ë‚´ë‹´ìì˜ 'ê³¼ê±° ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸ë“¤'ì„ ì¢…í•©í•˜ì—¬ ì¥ê¸°ì ì¸ ë³€í™”ì™€ íë¦„ì„ ë¶„ì„í•˜ëŠ” 'ë©”íƒ€ ë¶„ì„ê°€'ì…ë‹ˆë‹¤.\n"
                "ì•„ë˜ ì œê³µëœ ê³¼ê±° ë¦¬í¬íŠ¸ ê¸°ë¡ë“¤ì„ ì½ê³ , ë‚´ë‹´ìì˜ ì‹¬ë¦¬ ìƒíƒœê°€ ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
                f"{history_context}\n"
                "### [ì‘ì„± ì§€ì¹¨]\n"
                "1. **ì–¸ì–´**: ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "2. **êµ¬ì¡°**:\n"
                "   - **ë³€í™”ì˜ íë¦„**: ê°ì •ì´ë‚˜ íƒœë„ê°€ ì–´ë–»ê²Œ ë³€í•´ì™”ëŠ”ì§€ (ê¸ì •ì /ë¶€ì •ì  ë³€í™”)\n"
                "   - **ë°˜ë³µë˜ëŠ” íŒ¨í„´**: ì‹œê°„ì´ ì§€ë‚˜ë„ ì—¬ì „íˆ í•´ê²°ë˜ì§€ ì•Šê³  ë°˜ë³µë˜ëŠ” ë¬¸ì œì \n"
                "   - **ì¥ê¸° ì œì–¸**: ì•ìœ¼ë¡œì˜ 1ê°œì›”ì„ ìœ„í•œ í•µì‹¬ ì¡°ì–¸\n"
                "3. **ë¶„ëŸ‰**: 3~4ë¬¸ë‹¨ ë‚´ì™¸ë¡œ ê¹Šì´ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
                "ë©”íƒ€ ë¶„ì„ ê²°ê³¼:"
            )
            
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.6,
                    "num_predict": 2048
                }
            }
            
            response = requests.post(url, json=payload, timeout=300)
            
            if response.status_code == 200:
                return response.json().get('response', '')
            else:
                return "ë©”íƒ€ ë¶„ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ Long-Term Insight Error: {e}")
            return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


    def update_keywords(self, text, mood_level):
        # Learn new keywords from the text based on the user's provided mood_level.
        if not text: return

        # Map mood_level to label
        # mood_level is expected to be int
        try:
            mood_val = int(mood_level)
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            target_label = mapping.get(mood_val)
            
            if target_label is None:
                return # Invalid mood level
        except:
            return

        session = self.Session()
        try:
            from models import EmotionKeyword
            
            # Simple Tokenization (Space-based)
            # In a real KR app, use Mecab/Konlpy. Here we split by space.
            words = text.split()
            
            for w in words:
                # Basic cleaning
                w = w.strip('.,?!~"\'')
                if len(w) < 2: continue # Skip single chars
                
                # Check if exists
                existing = session.query(EmotionKeyword).filter_by(keyword=w).first()
                
                if existing:
                    # If exists, increment frequency if label matches
                    # If label differs, maybe decrease freq or ignore? 
                    # Let's just track co-occurrence. Complex logic omitted for simplicity.
                    if existing.emotion_label == target_label:
                        existing.frequency += 1
                else:
                    # NEW WORD -> LEARN IT!
                    print(f"Learning new keyword: {w} -> {self.classes[target_label]}")
                    new_kw = EmotionKeyword(
                        keyword=w,
                        emotion_label=target_label,
                        frequency=1
                    )
                    session.add(new_kw)
            
            session.commit()
            
            # If Model is active, we might want to update tokenizer or retrain eventually.
            # Rerunning Tokenizer.fit_on_texts would be needed. 
            # For this session, we won't retrain the LSTM live (too slow), but the Fallback logic will immediately benefit.
            
        except Exception as e:
            print(f"Learning error: {e}")
            session.rollback()
        finally:
            session.close()

# Standalone Function: Bypasses EmotionAnalysis class init for stability
def generate_analysis_reaction_standalone(user_text, mode='reaction'):
    print(f"DEBUG: generate_analysis_reaction_standalone called. Mode={mode}, Text={user_text[:20]}...")
    if not user_text: return None
    import re
    import requests
    import random
    
    # 1. Sanitize
    text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', user_text)
    sanitized = text[:300]
    
    # 2. Prompt Switching
    if mode == 'question':
        # Follow-up Question Prompt
        prompt_text = (
            f"ë‚´ë‹´ìì˜ ë§: \"{sanitized}\"\n\n"
            "ë‚´ë‹´ìê°€ ë„ˆë¬´ ì§§ê³  ë‹¨ë‹µí˜•ìœ¼ë¡œ ëŒ€ë‹µí–ˆì–´. ëŒ€í™”ë¥¼ ë” ê¹Šê²Œ ì´ëŒì–´ë‚´ê¸° ìœ„í•´ **ìì—°ìŠ¤ëŸ¬ìš´ ê¼¬ë¦¬ ì§ˆë¬¸**ì„ í•˜ë‚˜ ë˜ì ¸ì¤˜.\n"
            "ì§€ì‹œì‚¬í•­:\n"
            "1. ë‚´ë‹´ìì˜ ë§ì„ ë°˜ë³µí•˜ê¸°ë³´ë‹¤, ê·¸ ì´ë©´ì˜ ì´ìœ ë‚˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ë¬¼ì–´ë´.\n"
            "2. 'ê·¸ë ‡êµ°ìš”' ê°™ì€ ì§§ì€ ê³µê° í›„ ë°”ë¡œ ì§ˆë¬¸í•´.\n"
            "3. ë§íˆ¬ëŠ” ë‹¤ì •í•˜ê³  ê¶ê¸ˆí•´í•˜ëŠ” 'í•´ìš”ì²´'ë¥¼ ì¨.\n"
            "4. 100ì ì´ë‚´ë¡œ.\n\n"
            "ê¼¬ë¦¬ ì§ˆë¬¸:"
        )
    else:
        # Standard Reaction Prompt
        prompt_text = (
            f"ë‚´ë‹´ìì˜ ë§: \"{sanitized}\"\n\n"
            "ë„ˆëŠ” ê¹Šì€ í†µì°°ë ¥ì„ ì§€ë‹Œ ë”°ëœ»í•œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì•¼. ë‚´ë‹´ìì˜ ë§ì„ ë“£ê³  **ìƒí™©ì„ ë¶„ì„**í•˜ê³  **ì§€ì§€í•˜ëŠ” ì½”ë©˜íŠ¸**ë¥¼ í•´ì¤˜.\n"
            "ì§€ì‹œì‚¬í•­:\n"
            "1. ë¨¼ì € ë‚´ë‹´ìì˜ ë§ ì†ì— ìˆ¨ê²¨ì§„ ê°ì •ì´ë‚˜ ìš•êµ¬ë¥¼ ë¶„ì„í•´ì„œ ì–¸ê¸‰í•´ì¤˜. (ì˜ˆ: 'ê¸°ëŒ€ê°ê³¼ ë™ì‹œì— ê±±ì •ë„ ìˆìœ¼ì‹  ê²ƒ ê°™êµ°ìš”.')\n"
            "2. ê·¸ ë‹¤ìŒ, ê·¸ ê°ì •ì´ íƒ€ë‹¹í•¨ì„ ì§€ì§€í•´ì£¼ê³  ë”°ëœ»í•˜ê²Œ ê²©ë ¤í•´ì¤˜.\n"
            "3. ë§íˆ¬ëŠ” ì „ë¬¸ì ì´ê³  ë¶€ë“œëŸ¬ìš´ 'í•´ìš”ì²´'ë¥¼ ì¨.\n"
            "4. ì§ˆë¬¸ì€ í•˜ì§€ ë§ˆ.\n"
            "5. 150ì ì´ë‚´ë¡œ.\n\n"
            "ë¶„ì„ ë° ë¦¬ì•¡ì…˜:"
        )
    
    try:
        payload = {
            "model": "maum-on-gemma",
            "prompt": prompt_text,
            "stream": False,
            "options": {
                "temperature": 0.7, 
                "num_predict": 180
            }
        }
        res = requests.post("http://localhost:11434/api/generate", json=payload, timeout=60)
        
        if res.status_code == 200:
            result = res.json().get('response', '').strip()
            if result.startswith('"') and result.endswith('"'):
                result = result[1:-1]
            if result: return result
            
    except Exception as e:
        print(f"âŒ Standalone AI Error: {e}")
        
    # 3. Fallback (Mode Specific)
    if mode == 'question':
        fallbacks = [
            "ê·¸ë ‡êµ°ìš”. í˜¹ì‹œ ì¡°ê¸ˆ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”? ê¶ê¸ˆí•´ìš”.",
            "ì €ëŸ°, íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆì—ˆëŠ”ì§€ ë“£ê³  ì‹¶ì–´ìš”.",
            "ì§§ê²Œ ë§ì”€í•˜ì‹œë‹ˆ ë” ê¹Šì€ ì†ë§ˆìŒì´ ê¶ê¸ˆí•´ì§€ë„¤ìš”. í¸í•˜ê²Œ í„¸ì–´ë†“ì•„ì£¼ì„¸ìš”.",
            "ê·¸ ì¼ì´ ë‚´ë‹´ìë‹˜ê»˜ ì–´ë–¤ ì˜ë¯¸ì˜€ëŠ”ì§€ ì¡°ê¸ˆë§Œ ë” ë“¤ë ¤ì£¼ì„¸ìš”."
        ]
    else:
        fallbacks = [
            "ë§ì”€í•˜ì‹  ë‚´ìš©ì—ì„œ ê¹Šì€ ê³ ë¯¼ê³¼ ì§„ì‹¬ì´ ëŠê»´ì§€ë„¤ìš”. ì˜í•˜ê³  ê³„ì‹­ë‹ˆë‹¤.",
            "ìƒí™©ì„ ì°¨ë¶„íˆ ë“¤ì—¬ë‹¤ë³´ë©´, ê·¸ ì•ˆì—ì„œ ìŠ¤ìŠ¤ë¡œì˜ ì„±ì¥ì„ ë°œê²¬í•˜ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.",
            "ì§€ê¸ˆ ëŠë¼ì‹œëŠ” ê°ì •ì€ ë§¤ìš° ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ì‘ì´ì—ìš”. ìŠ¤ìŠ¤ë¡œë¥¼ ë¯¿ì–´ë³´ì„¸ìš”.",
            "ì´ì•¼ê¸°ë¥¼ ë“¤ì–´ë³´ë‹ˆ, ê·¸ë™ì•ˆ ë§ˆìŒì†ì— ë‹´ì•„ë‘ì…¨ë˜ ìƒê°ë“¤ì´ ë§ìœ¼ì…¨ë˜ ê²ƒ ê°™ì•„ ë§ˆìŒì´ ì“°ì´ë„¤ìš”."
        ]
        
    return random.choice(fallbacks)

