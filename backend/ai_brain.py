import numpy as np
import os
import random
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from config import Config
import json
import requests
import re
import time
TRAINING_STATE_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'training_state.json')

try:
    from emotion_codes import EMOTION_CODE_MAP
except ImportError:
    print("Warning: Could not import EMOTION_CODE_MAP from emotion_codes")
    EMOTION_CODE_MAP = {}

# TensorFlow/Keras Import (Optional)
try:
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
    from tensorflow.keras.utils import to_categorical
    import pandas as pd
    TENSORFLOW_AVAILABLE = True
    print("AI Brain: TensorFlow Available.")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"AI Brain: Local Emotion Model support skipped ({e}).")

# Transformers/PyTorch Import (Critical for Insight)
try:
    from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
    print("AI Brain: Transformers/PyTorch Available.")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"AI Brain: Local GenAI support skipped ({e}).")

class EmotionAnalysis:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.max_len = 50
        self.vocab_size = 0
        
        # 60 Ultra-Fine-Grained Emotion Classes
        self.sorted_codes = sorted(EMOTION_CODE_MAP.keys())
        self.classes = [EMOTION_CODE_MAP[code] for code in self.sorted_codes]
        self.code_to_idx = {code: i for i, code in enumerate(self.sorted_codes)}
        
        # We will load keywords from DB for fallback/learning
        try:
            from pymongo import MongoClient
            from config import Config
            self.mongo_client = MongoClient(Config.MONGO_URI)
            self.db = self.mongo_client.get_database() # Uses default db from URI
            print("AI Brain: Connected to MongoDB.")
        except Exception as e:
            print(f"AI Brain: MongoDB Connection Failed: {e}")
            self.db = None

        self.train_texts = []
        self.train_labels = np.array([], dtype=int)
        
        self.comment_bank = {} # Will load from file

        # Initialize attributes
        self.gpt_model = None
        self.gpt_tokenizer = None
        
        # Safe device init (Avoid 'torch' not defined error)
        try:
            import torch
            self.device = torch.device("cpu")
        except ImportError:
            self.device = "cpu"

        # Local Generative AI Loading (Polyglot-Ko)
        # Verify torch/transformers is actually available first
        if TRANSFORMERS_AVAILABLE:
            print("Initializing Generative AI (Polyglot-Ko) for Insight (Fallback)...")
            try:
                # Optimized for OCI (CUDA/CPU) and Local (MPS)
                if torch.cuda.is_available():
                    device = torch.device("cuda")
                    torch_dtype = torch.float16
                    print("ğŸš€ Using CUDA for AI acceleration (Cloud/GPU).")
                elif torch.backends.mps.is_available():
                    device = torch.device("mps")
                    torch_dtype = torch.float16
                    print("ğŸš€ Using MPS for AI acceleration (Local Mac).")
                else:
                    device = torch.device("cpu")
                    torch_dtype = torch.float32 
                    print("âš ï¸ Using CPU for AI (Cloud/Standard). Performance may be lower.")
                
                model_name = "EleutherAI/polyglot-ko-1.3b"
                print(f"Loading Polyglot-Ko-1.3B Model (Dtype: {torch_dtype}, Device: {device})...")
                
                self.gpt_tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.gpt_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch_dtype,
                    low_cpu_mem_usage=True
                ).to(device)
                self.device = device 
                print("âœ… Polyglot-Ko-1.3B Loaded successfully.")
            except Exception as e:
                print(f"âŒ Polyglot Load Failed: {e}")
                self.gpt_model = None
                self.gpt_tokenizer = None
        else:
            print("âš ï¸ Transformers library not available. Skipping GenAI load.")
               
        # === Tensorflow / LSTM Model Logic ===
        if TENSORFLOW_AVAILABLE:
            self.tokenizer = Tokenizer()
            # Check for saved model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            self.model_path = os.path.join(base_dir, 'emotion_model.h5')
            self.tokenizer_path = os.path.join(base_dir, 'tokenizer.pickle')
            
            # Check Training Condition
            current_count = self._get_keyword_count()
            last_count = self._get_last_trained_count()
            diff = current_count - last_count
            
            print(f"ğŸ“Š Training Check: Current Keywords={current_count}, Last Trained={last_count}, Diff={diff}")
            
            should_train = (diff >= 100)
            model_exists = os.path.exists(self.model_path) and os.path.exists(self.tokenizer_path)

            if should_train:
                print("âš ï¸ Should train, but skipping for now to rely on Local LLM/Fallback.")
                self._save_training_state(current_count)
            elif model_exists:
                print("ğŸ“¦ Models found. Loading existing models...")
                self._load_existing_models()
                print("âœ… Emotion Model loaded.")
            else:
                print("âš ï¸ No models found. Using Keyword Fallback.")
            
            print("AI Model initialization finished.")

        else: # TENSORFLOW NOT AVAILABLE
             print("Initializing Fallback Emotion Analysis (Keyword based - 5 classes)...")

        # Load Comment Bank (Safety Net) - Always load this
        self.load_comment_bank()
        self.load_emotion_bank()

    def _sanitize_context(self, text):
        if not text: return ""
        text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', text)
        text = re.sub(r'\d{2,3}-\d{3,4}-\d{4}', '[PHONE]', text)
        return text[:100].strip() + "..." if len(text) > 100 else text

    # ... (Keep helper methods like _get_keyword_count, _load_existing_models, load_comment_bank, etc. unchanged)
    
    def _get_keyword_count(self): return 0 # Simplified for brevity in replacement, but should keep original logic if possible. 
    # Actually, let's just paste the original logic helpers if we are replacing the whole file. 
    # Wait, replace_file_content is huge. I should try to target chunks or just be careful.
    # The user asked to remove Gemini code.
    
    # Let's keep the helper methods by NOT replacing them if they are outside the target range?
    # No, I must provide replacement content. I will include the helpers.
    
    def _get_keyword_count(self):
        if self.db is None: return 0
        try: return self.db.emotion_keywords.count_documents({})
        except: return 0

    def _get_last_trained_count(self):
        if os.path.exists(TRAINING_STATE_FILE):
            try:
                with open(TRAINING_STATE_FILE, 'r') as f: return json.load(f).get('last_keyword_count', 0)
            except: return 0
        return 0

    def _save_training_state(self, count):
        try:
            with open(TRAINING_STATE_FILE, 'w') as f: json.dump({'last_keyword_count': count}, f)
        except: pass

    def _load_existing_models(self):
        try:
            import pickle
            from tensorflow.keras.models import load_model
            self.model = load_model(self.model_path)
            with open(self.tokenizer_path, 'rb') as handle: self.tokenizer = pickle.load(handle)
            self.vocab_size = len(self.tokenizer.word_index) + 1
            print("Emotion Model loaded.")
        except Exception as e:
            print(f"Error loading models: {e}.")

    def load_comment_bank(self):
        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.comment_bank = data.get('keywords', {})
                print(f"Loaded {len(self.comment_bank)} keyword categories.")
            else:
                self.comment_bank = {}
        except Exception as e:
            print(f"Error loading comment bank: {e}")

    def generate_keyword_comment(self, user_input):
        if not self.comment_bank or not user_input: return None
        if isinstance(user_input, dict):
            text = f"{user_input.get('event', '')} {user_input.get('emotion', '')} {user_input.get('self_talk', '')}"
        else:
            text = str(user_input)
        for category, content in self.comment_bank.items():
            if not isinstance(content, dict): continue
            if category in text: return content.get('default', "í˜ë‚´ì„¸ìš”.")
            keywords = content.get('emotion_keywords', [])
            for k in keywords:
                if k in text: return content.get('default', "í˜ë‚´ì„¸ìš”.")
        return None

    def load_emotion_bank(self):
        try:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'emotion_comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.emotion_bank = data.get('keywords', {})
                print(f"Loaded {len(self.emotion_bank)} emotion categories.")
            else:
                self.emotion_bank = {}
        except:
             self.emotion_bank = {}

    def generate_label_comment(self, emotion_label):
        if not self.emotion_bank: return None
        try:
            label_key = emotion_label.rsplit(' (', 1)[0] if '(' in emotion_label else emotion_label
        except:
            label_key = emotion_label
        
        if label_key in self.emotion_bank: return self.emotion_bank[label_key].get('default')
        for key in self.emotion_bank:
            if key in emotion_label: return self.emotion_bank[key].get('default')
        return None
        
    def generate_polyglot_comment(self, user_input, emotion_label):
        # Implementation kept for fallback, but practically usage is low if local LLM works
        if not self.gpt_model or not self.gpt_tokenizer: return None
        try:
            if isinstance(user_input, dict):
                event = user_input.get('event', '')
                emotion = user_input.get('emotion', '')
                self_talk = user_input.get('self_talk', '')
                prompt = (
                    "ì—­í™œ: ë‹¹ì‹ ì€ ë‹¤ì •í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë‚´ë‹´ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ë”°ëœ»í•œ ìœ„ë¡œì™€ ê³µê°ì˜ ë§ì„ ê±´ë„¤ì£¼ì„¸ìš”.\n\n"
                    f"ìƒí™©: {event} {emotion} {self_talk}\n"
                    f"ê°ì •: {emotion_label}\n"
                    "ìƒë‹´ì‚¬:"
                )
            else:
                text = str(user_input)
                prompt = (
                    "ì—­í™œ: ë‹¹ì‹ ì€ ë‹¤ì •í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\n\n"
                    f"ì¼ê¸°: {text}\n"
                    f"ê°ì •: {emotion_label}\n"
                    "ìƒë‹´ì‚¬:"
                )
            
            encoded = self.gpt_tokenizer(prompt, return_tensors='pt').to(self.device)
            encoded.pop('token_type_ids', None)
            
            with torch.no_grad():
                gen_ids = self.gpt_model.generate(
                    encoded['input_ids'],
                    max_length=len(encoded['input_ids'][0]) + 100,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.gpt_tokenizer.eos_token_id
                )
            generated = self.gpt_tokenizer.decode(gen_ids[0], skip_special_tokens=True)
            if "ìƒë‹´ì‚¬:" in generated:
                response = generated.split("ìƒë‹´ì‚¬:")[-1].strip()
            else:
                response = generated
            return response.split('.')[0] + "." # Take first sentence
        except Exception as e:
            print(f"KoGPT Error: {e}")
            return None

    def generate_pre_write_insight(self, recent_diaries, weather=None, weather_stats=None):
        print(f"ğŸ” [Insight] Request received. Recent diaries: {len(recent_diaries)}")
        if not recent_diaries: return "ì˜¤ëŠ˜ì˜ ì²« ê¸°ë¡ì„ ì‹œì‘í•´ë³´ì„¸ìš”! ì†”ì§í•œ ë§ˆìŒì„ ë‹´ìœ¼ë©´ ë©ë‹ˆë‹¤."
        try:
            diary_context = ""
            for d in recent_diaries:
                sanitized_event = self._sanitize_context(d.get('event',''))
                diary_context += f"- [{d.get('date','')}] ê¸°ë¶„:{d.get('mood','')} / ë‚´ìš©:{sanitized_event}\n"

            weather_info = f"ì˜¤ëŠ˜ì˜ ë‚ ì”¨: {weather}" if weather else "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´ ì—†ìŒ"
            prompt_text = (
                f"### {weather_info}\n"
                f"### ì‚¬ìš©ìì˜ ìµœê·¼ 1ì£¼ì¼ íë¦„\n{diary_context}\n"
                "ì‚¬ìš©ìì—ê²Œ ê±´ë„¬ ë”°ëœ»í•œ í•œ ë§ˆë””ì˜ ì¡°ì–¸ì„ ì‘ì„±í•´ì¤˜ (40ì ì´ë‚´, ë‚ ì”¨ ì–¸ê¸‰ í•„ìˆ˜)."
            )

            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {"temperature": 0.7, "num_predict": 100}
            }
            
            print(f"ğŸ¦™ [Insight] Requesting Ollama (Gemma 2:2b)...")
            url = "http://localhost:11434/api/generate"
            response = requests.post(url, json=payload, timeout=60)
            
            if response.status_code != 200: return None
            return response.json().get('response', '').strip().strip('"')

        except Exception as e:
            print(f"âŒ [Insight] Failed: {str(e)}")
            return None

    def predict(self, text):
        import time
        start_time = time.time()
        
        if not text: return {"emotion": "ë¶„ì„ ë¶ˆê°€", "comment": "ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."}

        emotion_result = "ë¶„ì„ ë¶ˆê°€"
        
        # 1. Emotion Classification
        if TENSORFLOW_AVAILABLE and self.model:
            try:
                sequences = self.tokenizer.texts_to_sequences([text])
                padded = pad_sequences(sequences, maxlen=self.max_len)
                prediction = self.model.predict(padded, verbose=0)[0]
                idx = np.argmax(prediction)
                emotion_result = f"{self.classes[idx]} ({(prediction[idx] * 100):.1f}%)"
            except:
                emotion_result = self._fallback_predict(text)
        else:
            emotion_result = self._fallback_predict(text)
            
        print(f"ğŸ” Emotion: {emotion_result}")
        
        # 2. Comment Generation (LOCAl OLLAMA PRIORITY)
        comment_result = ""
        
        # Try Local LLM (Gemma 2) First
        try:
            print(f"ğŸš€ [Comment] Requests Local Ollama (Gemma 2)...")
            # We can reuse the analyze logic here or just call it directly
            llm_emotion, llm_comment = self.analyze_diary_with_local_llm(text)
            
            if llm_comment:
                comment_result = llm_comment
                # Optionally update emotion_result if confidence is high? 
                # For now let's keep LSTM emotion if it worked, or use LLM emotion if LSTM failed?
                # Actually, user wants "Mental Report" style, so LLM comment is key.
            else:
                 print("âš ï¸ Local LLM returned no comment.")
        except Exception as e:
            print(f"âŒ Local LLM Analysis Failed: {e}")

        # Fallback to Polyglot
        if not comment_result and self.gpt_model:
            comment_result = self.generate_polyglot_comment(text, emotion_result)
        
        # Final Fallback
        if not comment_result:
            comment_result = self.generate_keyword_comment(text) or "ì˜¤ëŠ˜ í•˜ë£¨ë„ ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”."
            
        print(f"âœ¨ [Total] Analysis took: {time.time() - start_time:.3f}s")
        return {
            "emotion": emotion_result, # Or llm_emotion if prefer
            "comment": comment_result
        }

    def _fallback_predict(self, text):
        if self.db is None: return "ë¶„ì„ ë¶ˆê°€"
        try:
            keywords = list(self.db.emotion_keywords.find())
            scores = [0] * 5
            found = False
            for kw in keywords:
                if kw['keyword'] in text:
                    scores[kw['emotion_label']] += kw['frequency']
                    found = True
            
            if found:
                max_s = max(scores)
                idx = scores.index(max_s)
                return f"{self.classes[idx]} ({(max_s/sum(scores)*100):.1f}%)"
            return "ê·¸ì €ê·¸ë˜ (40.0%)"
        except: return "ë¶„ì„ ë¶ˆê°€"

    def analyze_diary_with_local_llm(self, text):
        # [Local AI Mode] Uses Local Ollama (Gemma 2)
        print(f"ğŸ¦™ [Local AI] Calling Gemma 2:2b...", end=" ", flush=True)
        try:
            url = "http://localhost:11434/api/generate"
            prompt_text = (
                f"ë‹¤ìŒ ì¼ê¸°ë¥¼ ì½ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.\n"
                f"ì¼ê¸°:\n{text}\n\n"
                f"í˜•ì‹:\n"
                f"Emotion: (happy, sad, angry, neutral, panic ì¤‘ í•˜ë‚˜)\n"
                f"Confidence: (0~100 ìˆ«ìë§Œ)\n"
                f"Comment: (50ì ì´ë‚´ì˜ ë”°ëœ»í•œ í•œêµ­ì–´ ìœ„ë¡œ)\n"
                f"ë°˜ë“œì‹œ ìœ„ í˜•ì‹ë§Œ ì§€ì¼œì„œ ë‹µë³€í•´."
            )
            payload = {
                "model": "gemma2:2b", 
                "prompt": prompt_text, 
                "stream": False,
                "options": {"temperature": 0.3, "num_predict": 150}
            }
            response = requests.post(url, json=payload, timeout=60)
            if response.status_code != 200: return None, None
            
            result = response.json().get('response', '').strip()
            
            # Regex Parsing
            emotion_match = re.search(r"Emotion:\s*([a-zA-Z]+)", result, re.IGNORECASE)
            emotion_str = emotion_match.group(1).lower() if emotion_match else "neutral"
            
            comment_match = re.search(r"Comment:\s*(.*)", result, re.DOTALL)
            comment = comment_match.group(1).strip() if comment_match else result
            
            # Remove quotes
            if comment.startswith('"') and comment.endswith('"'): comment = comment[1:-1]
            
            # Map Emotion
            emotion_map = {
                "happy": "í–‰ë³µí•´", "joy": "í–‰ë³µí•´", 
                "sad": "ìš°ìš¸í•´", "depressed": "ìš°ìš¸í•´", 
                "neutral": "í‰ì˜¨í•´", "calm": "í‰ì˜¨í•´", "soso": "ê·¸ì €ê·¸ë˜",
                "angry": "í™”ê°€ë‚˜", "annoyed": "í™”ê°€ë‚˜", 
                "panic": "ìš°ìš¸í•´", "anxious": "ìš°ìš¸í•´"
            }
            korean_emotion = emotion_map.get(emotion_str, "í‰ì˜¨í•´")
            
            return f"'{korean_emotion} (85%)'", comment # Mock confidence for now
            
        except Exception as e:
            print(f"âŒ Local AI Error: {e}")
            return None, None

    # ... (Keep generate_comprehensive_report and generate_long_term_insight as they are, they already use Gemma)
    
    def generate_comprehensive_report(self, diary_summary):
        print("ğŸ§  [Brain] Generating Comprehensive Report (Gemma)...")
        try:
            url = "http://localhost:11434/api/generate"
            prompt_text = (
                "## SYSTEM: Answer in KOREAN ONLY.\n"
                f"### [ì‚¬ìš©ì ë°ì´í„°]\n{diary_summary}\n\n"
                "ì‹¬ì¸µ ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš” (10ë¬¸ë‹¨ ì´ìƒ)."
            )
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {"temperature": 0.7, "num_predict": 4096}
            }
            response = requests.post(url, json=payload, timeout=600)
            if response.status_code == 200: return response.json().get('response', '')
            return "ì˜¤ë¥˜ ë°œìƒ"
        except: return "ì˜¤ë¥˜ ë°œìƒ"

    def generate_long_term_insight(self, report_history):
        print(f"ğŸ§  [Brain] Generating Long-Term Insight (Gemma)...")
        try:
            url = "http://localhost:11434/api/generate"
            history_context = ""
            for i, r in enumerate(report_history):
                history_context += f"### [ë¦¬í¬íŠ¸ {i+1}]\n{r.get('content', '')[:500]}...\n\n"
            
            prompt_text = (
                "## SYSTEM: Answer in KOREAN ONLY.\n"
                f"{history_context}\n"
                "ê³¼ê±° ë¦¬í¬íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¥ê¸°ì ì¸ ì‹¬ë¦¬ ë³€í™”ë¥¼ ë¶„ì„í•˜ì„¸ìš”."
            )
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {"temperature": 0.6, "num_predict": 2048}
            }
            response = requests.post(url, json=payload, timeout=300)
            if response.status_code == 200: return response.json().get('response', '')
            return "ì˜¤ë¥˜ ë°œìƒ"
        except: return "ì˜¤ë¥˜ ë°œìƒ"

    def _sanitize_context(self, text):
        """
        Privacy Guard: Removes potential sensitive information before sending to external API.
        - Truncates long text
        - Focuses on sentiment-carrying words
        """
        if not text: return ""
        # Simple privacy protection: Remove common patterns (emails, phone numbers)
        import re
        text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', text)
        text = re.sub(r'\d{2,3}-\d{3,4}-\d{4}', '[PHONE]', text)
        
        # To further protect privacy, we could use the local emotion results 
        # instead of raw text, but for 'insight' we need some context.
        # We limit the length to prevent sending too much detail.
        return text[:100].strip() + "..." if len(text) > 100 else text

    def _get_keyword_count(self):

        """Get total count of emotion keywords from DB (MongoDB)"""
        if self.db is None:
            return 0
        try:
            count = self.db.emotion_keywords.count_documents({})
            return count
        except Exception as e:
            print(f"Error counting keywords: {e}")
            return 0

    def _get_last_trained_count(self):
        """Read last trained count from JSON"""
        if os.path.exists(TRAINING_STATE_FILE):
            try:
                with open(TRAINING_STATE_FILE, 'r') as f:
                    data = json.load(f)
                    return data.get('last_keyword_count', 0)
            except:
                return 0
        return 0

    def _save_training_state(self, count):
        """Save current keyword count to JSON"""
        try:
            with open(TRAINING_STATE_FILE, 'w') as f:
                json.dump({'last_keyword_count': count}, f)
        except Exception as e:
            print(f"Error saving training state: {e}")

    def _load_existing_models(self):
        """Helper to load existing models"""
        try:
            import pickle
            from tensorflow.keras.models import load_model
            
            self.model = load_model(self.model_path)
            
            with open(self.tokenizer_path, 'rb') as handle:
                self.tokenizer = pickle.load(handle)
                
            self.vocab_size = len(self.tokenizer.word_index) + 1
            print("Emotion Model loaded.")
                
        except Exception as e:
            print(f"Error loading models: {e}.")
            # Assuming fallback will take over

    def load_comment_bank(self):
        """Load curated advice from JSON"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.comment_bank = data.get('keywords', {})
                print(f"Loaded {len(self.comment_bank)} keyword categories from Comment Bank.")
            else:
                print("Comment Bank file not found. Skipping.")
        except Exception as e:
            print(f"Error loading comment bank: {e}")

    def generate_keyword_comment(self, user_input):
        """Phase 1: Hybrid Keyword System (Priority 1)"""
        if not self.comment_bank or not user_input:
            return None
            
        # Extract text if input is dict
        if isinstance(user_input, dict):
            text = f"{user_input.get('event', '')} {user_input.get('emotion', '')} {user_input.get('self_talk', '')}"
        else:
            text = str(user_input)
            
        for category, content in self.comment_bank.items():
            # Safety check: content must be a dict
            if not isinstance(content, dict):
                continue
                
            if category in text:
                return content.get('default', "í˜ë‚´ì„¸ìš”.")
                
            keywords = content.get('emotion_keywords', [])
            for k in keywords:
                if k in text:
                    return content.get('default', "í˜ë‚´ì„¸ìš”.")
        return None

    def load_emotion_bank(self):
        """Load 60-class emotion advice"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'emotion_comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.emotion_bank = data.get('keywords', {}) # Structure reuse
                print(f"Loaded {len(self.emotion_bank)} emotion categories.")
            else:
                self.emotion_bank = {}
        except Exception as e:
            print(f"Error loading emotion bank: {e}")
            self.emotion_bank = {}

    def generate_label_comment(self, emotion_label):
        """Phase 1.5: Label-Based Retrieval (Priority 2)"""
        if not self.emotion_bank:
            return None
            
        try:
            if '(' in emotion_label:
                label_key = emotion_label.rsplit(' (', 1)[0]
                pass
            else:
                label_key = emotion_label
        except:
             label_key = emotion_label
        
        # Exact match attempt
        if label_key in self.emotion_bank:
            return self.emotion_bank[label_key].get('default')
            
        # Fuzzy match 
        for key in self.emotion_bank:
            if key in emotion_label:
                return self.emotion_bank[key].get('default')
        
        return None
        
    def generate_polyglot_comment(self, user_input, emotion_label):
        """Phase 2: Polyglot-Ko-1.3B Generation (Priority 2)"""
        if not self.gpt_model or not self.gpt_tokenizer:
            return None
            
        try:
            # Handle user_input (str or dict)
            if isinstance(user_input, dict):
                event = user_input.get('event', '')
                emotion = user_input.get('emotion', '')
                self_talk = user_input.get('self_talk', '')
                
                prompt = (
                    "ì—­í™œ: ë‹¹ì‹ ì€ ë‹¤ì •í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë‚´ë‹´ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ë”°ëœ»í•œ ìœ„ë¡œì™€ ê³µê°ì˜ ë§ì„ ê±´ë„¤ì£¼ì„¸ìš”. ë‹µë³€ì€ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”.\n\n"
                    "ìƒí™©: ì‹œí—˜ì— ë–¨ì–´ì ¸ì„œ ìš¸ì—ˆë‹¤.\n"
                    "ê°ì •: ìŠ¬í”” (ì¢Œì ˆ)\n"
                    "ìƒë‹´ì‚¬: ì •ë§ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì—´ì‹¬íˆ ì¤€ë¹„í–ˆì„ í…ë° ê²°ê³¼ê°€ ì¢‹ì§€ ì•Šì•„ ë§ˆìŒì´ ì•„í”„ì‹œì£ . í•˜ì§€ë§Œ ì´ë²ˆ ì‹¤íŒ¨ê°€ ë‹¹ì‹ ì˜ ëª¨ë“  ê²ƒì„ ê²°ì •í•˜ì§€ëŠ” ì•Šì•„ìš”. ì˜¤ëŠ˜ì€ í‘¹ ì‰¬ë©´ì„œ ìŠ¤ìŠ¤ë¡œë¥¼ ìœ„ë¡œí•´ì£¼ì„¸ìš”.\n\n"
                    f"ìƒí™©: {event} {emotion} {self_talk}\n"
                    f"ê°ì •: {emotion_label}\n"
                    "ìƒë‹´ì‚¬:"
                )
            else:
                text = str(user_input)
                prompt = (
                    "ì—­í™œ: ë‹¹ì‹ ì€ ë‹¤ì •í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë‚´ë‹´ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ë”°ëœ»í•œ ìœ„ë¡œì™€ ê³µê°ì˜ ë§ì„ ê±´ë„¤ì£¼ì„¸ìš”. ë‹µë³€ì€ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”.\n\n"
                    "ì¼ê¸°: ì˜¤ëŠ˜ í•˜ë£¨ì¢…ì¼ ë„ˆë¬´ í˜ë“¤ì—ˆë‹¤.\n"
                    "ê°ì •: ìš°ìš¸ (ì§€ì¹¨)\n"
                    "ìƒë‹´ì‚¬: ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”. ì§€ì¹œ ëª¸ê³¼ ë§ˆìŒì„ í¸ì•ˆí•˜ê²Œ ë‚´ë ¤ë†“ê³  íœ´ì‹ì„ ì·¨í•´ë³´ì„¸ìš”. ë‹¹ì‹ ì€ ì¶©ë¶„íˆ ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì¼ê¸°: {text}\n"
                    f"ê°ì •: {emotion_label}\n"
                    "ìƒë‹´ì‚¬:"
                )
            # Encode and move to device
            encoded = self.gpt_tokenizer(prompt, return_tensors='pt').to(self.device)
            # Remove token_type_ids if present (GPT models don't use it)
            encoded.pop('token_type_ids', None)
            
            input_ids = encoded['input_ids']
            attention_mask = encoded['attention_mask']
            
            # Ensure pad_token_id is set 
            pad_token_id = self.gpt_tokenizer.pad_token_id
            if pad_token_id is None:
                pad_token_id = self.gpt_tokenizer.eos_token_id

            with torch.no_grad():
                gen_ids = self.gpt_model.generate(
                    input_ids,
                    attention_mask=attention_mask, 
                    max_length=len(input_ids[0]) + 50,
                    do_sample=True,
                    temperature=0.6,
                    top_p=0.90,
                    repetition_penalty=1.2,
                    pad_token_id=pad_token_id,
                    eos_token_id=self.gpt_tokenizer.eos_token_id,
                    bos_token_id=self.gpt_tokenizer.bos_token_id,
                    use_cache=True
                )
                
            generated = self.gpt_tokenizer.decode(gen_ids[0], skip_special_tokens=True)
            
            # Explicitly remove specific tokens just in case
            generated = generated.replace("<|endoftext|>", "").replace("<s>", "").replace("</s>", "")

            # Extract response
            if "ìƒë‹´ì‚¬ ë‹µë³€:" in generated:
                response = generated.split("ìƒë‹´ì‚¬ ë‹µë³€:")[-1].strip()
            elif "ìƒë‹´ì‚¬:" in generated:
                 response = generated.split("ìƒë‹´ì‚¬:")[-1].strip()
            else:
                response = generated
                
            # Post-process: Take first 2 sentences only to avoid hallucination
            sentences = response.split('.')
            # Filter out empty strings
            sentences = [s.strip() for s in sentences if s.strip()]
            
            if len(sentences) > 0:
                clean_response = '. '.join(sentences[:2]).strip()
                if not clean_response.endswith(('!', '?', '.')):
                    clean_response += "."
            else:
                clean_response = ""
            
            return clean_response
            
        except Exception as e:
            print(f"KoGPT Generation Error: {e}")
            return None


    
    def generate_pre_write_insight(self, recent_diaries, weather=None, weather_stats=None):
        """
        Generates a warm insight using Local Gemma 2 (Ollama).
        """
        import requests
        import json

        print(f"ğŸ” [Insight] Request received. Recent diaries count: {len(recent_diaries)}, Weather: {weather}")
        
        if not recent_diaries:
            return "ì˜¤ëŠ˜ì˜ ì²« ê¸°ë¡ì„ ì‹œì‘í•´ë³´ì„¸ìš”! ì†”ì§í•œ ë§ˆìŒì„ ë‹´ìœ¼ë©´ ë©ë‹ˆë‹¤."

        try:
            # Construct Prompt
            diary_context = ""
            for d in recent_diaries:
                sanitized_event = self._sanitize_context(d.get('event',''))
                diary_context += f"- [{d.get('date','')}] ê¸°ë¶„:{d.get('mood','')} / ë‚´ìš©:{sanitized_event}\n"

            weather_info = f"ì˜¤ëŠ˜ì˜ ë‚ ì”¨: {weather}" if weather else "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´ ì—†ìŒ"
            stats_info = f" (ê³¼ê±° ì´ ë‚ ì”¨ì— ë‹¹ì‹ ì€ ì£¼ë¡œ {weather_stats} ê°ì •ì„ ëŠë¼ì…¨ë„¤ìš”)" if weather_stats else ""

            prompt_text = (
                "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§€ë‚œ ì¼ê¸° ê¸°ë¡ê³¼ ì˜¤ëŠ˜ì˜ ë‚ ì”¨, ê·¸ë¦¬ê³  'ê³¼ê±° ë‚ ì”¨ë³„ ê°ì • íŒ¨í„´'ì„ ë¶„ì„í•˜ì—¬ ë”°ëœ»í•œ í•œ ë¬¸ì¥ì˜ ì¡°ì–¸ì„ ê±´ë„¤ëŠ” ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\n\n"
                f"### {weather_info}{stats_info}\n"
                "### ì‚¬ìš©ìì˜ ìµœê·¼ 1ì£¼ì¼ íë¦„\n"
                f"{diary_context}\n"
                "### ì§€ì‹œì‚¬í•­\n"
                "1. ë°˜ë“œì‹œ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "2. [í•„ìˆ˜] ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë‚˜ ê³„ì ˆê°ì„ ì–¸ê¸‰í•˜ë©° ì‹œì‘í•˜ì„¸ìš”. (ì˜ˆ: 'ë¹„ê°€ ì˜¤ëŠ” ë‚ ì—”...', 'ë§‘ì€ í–‡ì‚´ì²˜ëŸ¼...')\n"
                "3. ìµœê·¼ 1ì£¼ì¼ê°„ì˜ ê°ì • íë¦„ì´ ì¢‹ì€ì§€ ë‚˜ìœì§€ë¥¼ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ í•˜ì„¸ìš”.\n"
                "4. 'ì˜¤ëŠ˜ í•˜ë£¨ ì‘ì›í•©ë‹ˆë‹¤' ê°™ì€ ë»”í•œ ë§ì€ ê¸ˆì§€ì…ë‹ˆë‹¤.\n"
                "5. 40ì~80ì ë‚´ì™¸ë¡œ ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§(í•´ìš”ì²´)ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n\n"
                "ìƒë‹´ì‚¬ ì¡°ì–¸(ë‚ ì”¨ì™€ ê°ì • íë¦„ì´ í†µí•©ëœ í•œ ë¬¸ì¥):"
            )

            # Ollama Payload
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                # No 'format': 'json' here because we want free text
                "options": {
                    "temperature": 0.7,
                    "num_predict": 100 
                }
            }
            
            print(f"ğŸ¦™ [Insight] Requesting Ollama (Gemma 2:2b)...")
            url = "http://localhost:11434/api/generate"
            
            # Timeout Increased to 60s (OCI CPU might be slow or busy)
            response = requests.post(url, json=payload, timeout=60)
            
            if response.status_code != 200:
                print(f"âŒ Ollama Insight Error {response.status_code}: {response.text}")
                return None
                
            result = response.json()
            response_text = result.get('response', '').strip()
            
            # Cleanup quotes if model adds them
            if response_text.startswith('"') and response_text.endswith('"'):
                response_text = response_text[1:-1]
                
            print(f"âœ… [Insight] Gemma Success: {response_text}")
            return response_text

        except Exception as e:
            print(f"âŒ [Insight] Inference Failed: {str(e)}")
            return None

    def _rebuild_inference_models(self):
        # Seq2Seq Removed
        pass 

    def train_comment_model(self):
        # Seq2Seq Removed
        pass

    def predict(self, text):
        import time
        start_time = time.time()
        
        if not text: 
            return {"emotion": "ë¶„ì„ ë¶ˆê°€", "comment": "ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."}

        emotion_result = "ë¶„ì„ ë¶ˆê°€"
        
        # 1. Emotion Classification (LSTM or Keyword)
        if TENSORFLOW_AVAILABLE and self.model:
            try:
                tf_start = time.time()
                sequences = self.tokenizer.texts_to_sequences([text])
                padded = pad_sequences(sequences, maxlen=self.max_len)
                prediction = self.model.predict(padded, verbose=0)[0]
                predicted_class_idx = np.argmax(prediction)
                confidence = prediction[predicted_class_idx]
                predicted_label = self.classes[predicted_class_idx]
                emotion_result = f"{predicted_label} ({(confidence * 100):.1f}%)"
                print(f"â±ï¸ [Timer] TensorFlow Prediction took: {time.time() - tf_start:.3f}s")
            except Exception as e:
                print(f"Prediction error: {e}")
                emotion_result = self._fallback_predict(text)
        else:
            emotion_result = self._fallback_predict(text)
            
        # 2. Comment Generation (Priority: Gemini -> Polyglot -> Keyword)
        comment_result = ""
        
        # try Gemini First
        if self.gemini_model:
            try:
                gemini_start = time.time()
                print(f"ğŸš€ [Comment] Generating letter using Gemini...")
                comment_result = self.generate_gemini_comment(text, emotion_result)
                print(f"â±ï¸ [Timer] Gemini Comment took: {time.time() - gemini_start:.3f}s")
            except Exception as e:
                print(f"âŒ [Comment] Gemini failed: {e}")

        # Fallback to Polyglot if Gemini failed
        if not comment_result and self.gpt_model:
            try:
                comment_result = self.generate_polyglot_comment(text, emotion_result)
            except Exception as e:
                print(f"âŒ [Comment] Polyglot failed: {e}")
        
        # Final Fallback
        if not comment_result:
            comment_result = self.generate_keyword_comment(text) or "ì˜¤ëŠ˜ í•˜ë£¨ë„ ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”."
            
        print(f"âœ¨ [Timer] Total AI Analysis took: {time.time() - start_time:.3f}s")
        return {
            "emotion": emotion_result,
            "comment": comment_result
        }

    def _fallback_predict(self, text):
        # Load keywords from MongoDB
        if self.db is None:
             return "ë¶„ì„ ë¶ˆê°€"

        try:
            # Fetch all keywords from Mongo
            # This is inefficient for large datasets but ok for small keywords bank
            keywords = list(self.db.emotion_keywords.find())
            
            scores = [0] * 5
            found_any = False
            
            for kw in keywords:
                if kw['keyword'] in text:
                    scores[kw['emotion_label']] += kw['frequency']
                    found_any = True
            
            if found_any:
                max_score = max(scores)
                total_score = sum(scores)
                confidence = (max_score / total_score * 100) if total_score > 0 else 85.0
                best_idx = scores.index(max_score)
                return f"{self.classes[best_idx]} ({confidence:.1f}%)"
            else:
                return "ê·¸ì €ê·¸ë˜ (40.0%)" 
        except Exception as e:
            print(f"Fallback error: {e}")
            return "ë¶„ì„ ë¶ˆê°€"

    def load_sentiment_corpus(self):
        """
        Load 'Sentiment Dialogue Corpus' (Training & Validation).
        Use Full 60-Class Granularity (E10 ~ E69).
        """
        import json
        
        files = [
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json',
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.json'
        ]
        
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        import glob
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Use glob to avoid Unicode normalization issues (NFC vs NFD) on macOS
        files = glob.glob(os.path.join(base_dir, "*Training.json")) + glob.glob(os.path.join(base_dir, "*Validation.json"))
        
        if not files:
             print("No corpus files found via glob!")
        
        for fpath in files:
            fname = os.path.basename(fpath)
            # fpath is already absolute from glob
             
            print(f"Loading corpus: {fname}...")
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                new_texts = []
                new_labels = []
                
                for entry in data:
                    try:
                        etype = entry.get('profile', {}).get('emotion', {}).get('type', '')
                        
                        # Only use codes defined in our map (E10-E69)
                        if etype in self.code_to_idx:
                            # Extract user text
                            content = entry.get('talk', {}).get('content', {})
                            text = content.get('HS01', '')
                            system_text = content.get('SS01', '')
                            
                            if text:
                                label_idx = self.code_to_idx[etype]
                                new_texts.append(text)
                                new_labels.append(label_idx)
                                
                                # Store for comment training if pair exists
                                if system_text:
                                    self.conversation_pairs.append((text, system_text))
                                    
                    except Exception as e:
                        continue
                        
                # Add to training set
                self.train_texts.extend(new_texts)
                self.train_labels = np.concatenate((self.train_labels, np.array(new_labels)))
                
                print(f"Added {len(new_texts)} samples from {fname}")
                
            except Exception as e:
                print(f"Error loading corpus {fname}: {e}")


    def load_db_data(self):
        """
        Load data from the Diary table to fine-tune the model.
        Returns:
            X (list): Combined text (event + emotion_desc + self_talk)
            y (list): Emotion labels (0-4)
        """
        print("Loading data from Database...")
        session = self.Session()
        X = []
        y = []
        try:
            from models import Diary
            diaries = session.query(Diary).all()
            
            # Mood Level to Label Mapping
            # 5(Happy)->0, 4(Calm)->1, 3(Neutral)->2, 2(Depressed)->3, 1(Angry)->4
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            
            for d in diaries:
                if not d.mood_level: continue
                
                label = mapping.get(d.mood_level)
                if label is None: continue
                
                # Combine text fields for rich context
                text = f"{d.event} {d.emotion_desc} {d.self_talk}"
                X.append(text)
                y.append(label)
                
            print(f"Loaded {len(X)} samples from Database.")
            return X, y
            
        except Exception as e:
            print(f"Error loading DB data: {e}")
            return [], []
        finally:
            session.close()





    def train_comment_model(self):
        """
        Train a Seq2Seq model using ChatbotData.csv AND Sentiment Dialogue Corpus
        """
        if not TENSORFLOW_AVAILABLE: return

        print("Training Comment Generation Model (Seq2Seq)...")
        try:
            import os
            import pickle
            
            # 1. Load ChatbotData.csv
            base_dir = os.path.dirname(os.path.abspath(__file__))
            data_path = os.path.join(base_dir, 'ChatbotData.csv')
            questions = []
            answers = []
            
            if os.path.exists(data_path):
                df = pd.read_csv(data_path)
                # Take a sample to keep training time reasonable, or all? 
                # Let's take mixed sample.
                df = df.sample(frac=1).reset_index(drop=True)
                questions = df['Q'].astype(str).tolist()[:5000] # Cap at 5000 for speed
                # Char-level: Use \t for Start, \n for End
                answers = df['A'].apply(lambda x: '\t' + str(x) + '\n').tolist()[:5000]
            
            # 2. Add Sentiment Dialogue Corpus
            if hasattr(self, 'conversation_pairs'):
                print(f"Integrating {len(self.conversation_pairs)} pairs from Sentiment Corpus...")
                # Sample 5000 from here too to balance
                import random
                pairs = self.conversation_pairs
                if len(pairs) > 5000:
                    pairs = random.sample(pairs, 5000)
                
                for q, a in pairs:
                    questions.append(str(q))
                    answers.append('\t' + str(a) + '\n')
            
            print(f"Total Comment Training Samples: {len(questions)}")

            # Shared Tokenizer (Character Level for better Korean convergence without Mecab)
            # Filters: Don't filter \t and \n as they are our tokens!
            self.comment_tokenizer = Tokenizer(char_level=True, filters='!"#$%&()*+,-./:;<=>?@[\\]^`{|}~') 
            self.comment_tokenizer.fit_on_texts(questions + answers)
            
            vocab_size = len(self.comment_tokenizer.word_index) + 1
            print(f"Comment Vocab Size (Char-level): {vocab_size}")

            # Encoder Data
            tokenized_Q = self.comment_tokenizer.texts_to_sequences(questions)
            encoder_input_data = pad_sequences(tokenized_Q, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Data
            tokenized_A = self.comment_tokenizer.texts_to_sequences(answers)
            decoder_input_data = pad_sequences(tokenized_A, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Target (Shifted-by-one)
            decoder_target_data = np.zeros_like(decoder_input_data, dtype="float32")
            decoder_target_data[:, :-1] = decoder_input_data[:, 1:]
            decoder_target_data = np.expand_dims(decoder_target_data, -1)

            # Model Architecture
            latent_dim = 256
            
            # Encoder
            encoder_inputs = Input(shape=(None,), name='enc_input')
            enc_emb_layer = Embedding(vocab_size, latent_dim, name='enc_embedding')
            enc_emb = enc_emb_layer(encoder_inputs)
            encoder_lstm = LSTM(latent_dim, return_state=True, name='enc_lstm')
            encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
            encoder_states = [state_h, state_c]
            
            # Decoder
            decoder_inputs = Input(shape=(None,), name='dec_input')
            dec_emb_layer = Embedding(vocab_size, latent_dim, name='dec_embedding')
            dec_emb = dec_emb_layer(decoder_inputs)
            decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='dec_lstm')
            decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
            decoder_dense = Dense(vocab_size, activation='softmax', name='dec_dense')
            decoder_outputs = decoder_dense(decoder_outputs)
            
            self.comment_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
            self.comment_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
            print("Fitting Seq2Seq Model (Char-level)...")
            self.comment_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
                                   batch_size=64, epochs=15, validation_split=0.2, verbose=1)
                                   
            print("Comment Model Trained. Saving...")
            
            # Save Main Model
            self.comment_model.save(self.comment_model_path)
            
            # Save Tokenizer
            with open(self.comment_tokenizer_path, 'wb') as handle:
                pickle.dump(self.comment_tokenizer, handle)

            # Construct & Save Inference Models
            self.enc_model = Model(encoder_inputs, encoder_states)
            self.enc_model.save(os.path.join(base_dir, 'comment_enc_model.h5'))
            
            dec_state_input_h = Input(shape=(latent_dim,), name='dec_input_h')
            dec_state_input_c = Input(shape=(latent_dim,), name='dec_input_c')
            dec_states_inputs = [dec_state_input_h, dec_state_input_c]
            
            dec_emb2 = dec_emb_layer(decoder_inputs)
            dec_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states_inputs)
            dec_states2 = [state_h2, state_c2]
            dec_outputs2 = decoder_dense(dec_outputs2)
            
            self.dec_model = Model([decoder_inputs] + dec_states_inputs, [dec_outputs2] + dec_states2)
            self.dec_model.save(os.path.join(base_dir, 'comment_dec_model.h5'))
            
            print("Inference models saved.")
            
        except Exception as e:
            print(f"Error training comment model: {e}")

    def _rebuild_inference_models(self):
        """Rebuild inference models from loaded main model or load separate files"""
        try:
            from tensorflow.keras.models import load_model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            enc_path = os.path.join(base_dir, 'comment_enc_model.h5')
            dec_path = os.path.join(base_dir, 'comment_dec_model.h5')
            
            if os.path.exists(enc_path) and os.path.exists(dec_path):
                self.enc_model = load_model(enc_path)
                self.dec_model = load_model(dec_path)
                print("Inference models loaded successfully.")
            else:
                print("Inference model files missing. Comment generation may fail.")
        except Exception as e:
            print(f"Error loading inference models: {e}")

    # Seq2Seq Generation helpers removed



    def analyze_diary_with_local_llm(self, text):
        # [Local AI Mode] Uses Local Ollama (Gemma 2) for Analysis.
        # Free, Unlimited, Private.
        import requests
        import json
        
        # Local Ollama URL
        print(f"ğŸ¦™ [Local AI] Requesting Ollama (Gemma 2:2b)...", end=" ", flush=True)
        try:
            url = "http://localhost:11434/api/generate"
            
            # Simple Structured Text Prompt (Faster & Safer than JSON mode for 2B models)
            prompt_text = (
                f"ë‹¤ìŒ ì¼ê¸°ë¥¼ ì½ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.\n"
                f"ì¼ê¸°:\n{text}\n\n"
                f"í˜•ì‹:\n"
                f"Emotion: (happy, sad, angry, neutral, panic ì¤‘ í•˜ë‚˜)\n"
                f"Confidence: (0~100 ìˆ«ìë§Œ)\n"
                f"Comment: (50ì ì´ë‚´ì˜ ë”°ëœ»í•œ í•œêµ­ì–´ ìœ„ë¡œ)\n"
                f"ë°˜ë“œì‹œ ìœ„ í˜•ì‹ë§Œ ì§€ì¼œì„œ ë‹µë³€í•´."
            )
            
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                # "format": "json"  <-- REMOVED: Cause of hanging
                "options": {
                    "temperature": 0.3, # Low temp for stability
                    "num_predict": 150
                }
            }
            
            # Timeout 60s
            response = requests.post(url, json=payload, timeout=60)
            
            if response.status_code != 200:
                print(f"âŒ Ollama Error {response.status_code}: {response.text}")
                return None, None
                
            result = response.json()
            response_text = result.get('response', '').strip()
            print(f"ğŸ” Raw Output: {response_text}")
            
            # Regex Parsing
            import re
            
            # 1. Emotion
            emotion_match = re.search(r"Emotion:\s*([a-zA-Z]+)", response_text, re.IGNORECASE)
            emotion_str = emotion_match.group(1).lower() if emotion_match else "neutral"
            
            # 2. Confidence
            conf_match = re.search(r"Confidence:\s*(\d+)", response_text)
            confidence = int(conf_match.group(1)) if conf_match else 80
            
            # 3. Comment
            comment_match = re.search(r"Comment:\s*(.*)", response_text, re.DOTALL)
            comment = comment_match.group(1).strip() if comment_match else "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”."
            
            # Remove any trailing quotes if model added them
            if comment.startswith('"') and comment.endswith('"'):
                comment = comment[1:-1]

            # Map to Korean
            emotion_map = {
                "happy": "í–‰ë³µí•´", "joy": "í–‰ë³µí•´", 
                "sad": "ìš°ìš¸í•´", "depressed": "ìš°ìš¸í•´", 
                "neutral": "í‰ì˜¨í•´", "calm": "í‰ì˜¨í•´", "soso": "ê·¸ì €ê·¸ë˜",
                "angry": "í™”ê°€ë‚˜", "annoyed": "í™”ê°€ë‚˜", 
                "panic": "ìš°ìš¸í•´", "anxious": "ìš°ìš¸í•´"
            }
            
            korean_emotion = emotion_map.get(emotion_str, "í‰ì˜¨í•´")
            formatted_prediction = f"'{korean_emotion} ({confidence}%)'"
            
            return formatted_prediction, comment
                
        except Exception as e:
            print(f"âŒ Local AI Error: {e}")
            return None, None

    def generate_comment(self, prediction_text, user_text=None):
        # Generate a supportive comment.
        # Priority: 1. Keyword Bank (Safety Net) 2. AI Generation (Seq2Seq) 3. Fallback
        # If we have user_text and gemini, try fast path
        if user_text and self.gemini_model:
             _, comment = self.analyze_diary_with_local_llm(user_text)
             if comment: return comment

        # 1. Phase 1: Keyword Safety Net (Highest Priority)
        if user_text:
            keyword_comment = self.generate_keyword_comment(user_text)
            if keyword_comment:
                 return keyword_comment

        if not prediction_text or "ë¶„ì„ ë¶ˆê°€" in prediction_text:
            return "ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“¤ë ¤ì£¼ì„¸ìš”. í•­ìƒ ë“£ê³  ìˆì„ê²Œìš”."

        # Extract strict label (remove confidence score)
        # e.g. "ë¶„ë…¸ (ë°°ì‹ ê°) (85.0%)" -> "ë¶„ë…¸ (ë°°ì‹ ê°)"
        try:
             emotion_label_only = prediction_text.rsplit(' (', 1)[0]
        except:
             emotion_label_only = prediction_text.split()[0]

        # 2. Phase 2: KoGPT-2 Generation (High Priority for Context)
        if user_text:
            # Pass the full specific label to KoGPT for context-aware generation
            gpt_comment = self.generate_kogpt2_comment(user_text, emotion_label_only)
            if gpt_comment:
                return f"{gpt_comment}"

        # 3. Phase 1.5: Label-based Specific Advice (Fallback)
        label_comment = self.generate_label_comment(emotion_label_only)
        if label_comment:
             return label_comment

        try:
            label = prediction_text.split()[0] # e.g. "í–‰ë³µí•´"
            
            # 4. Fallback
            return "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”."
            
        except Exception as e:
            print(f"Comment Gen Error: {e}")
            return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ì´í•´í•´ìš”."


    def generate_comprehensive_report(self, diary_summary):
        """
        Generates a detailed 10-paragraph psychological report using Local Gemma 2.
        """
        import requests
        print("ğŸ§  [Brain] Generating Comprehensive Report...")
        
        try:
            url = "http://localhost:11434/api/generate"
            
            prompt_text = (
                "## SYSTEM: You represent a thoughtful, empathetic counselor with 20 years of experience. You must ANSWER IN KOREAN ONLY.\n"
                "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë² í…Œë‘ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ë‹´ì(ì‚¬ìš©ì)ì˜ ì¼ê¸° ê¸°ë¡ê³¼ í†µê³„ë¥¼ ìì„¸íˆ ì½ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
                f"### [ì‚¬ìš©ì ë°ì´í„°]\n{diary_summary}\n\n"
                "### [ì‘ì„± ì§€ì¹¨]\n"
                "1. **ì–¸ì–´**: ë°˜ë“œì‹œ **í•œêµ­ì–´(Korean)**ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "2. **í˜•ì‹**: ì‚¬ìš©ìì—ê²Œ ë³´ë‚´ëŠ” 'ì‹¬ì¸µ ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸' í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "3. **ë¶„ëŸ‰**: ë°˜ë“œì‹œ **ì„œë¡ -ë³¸ë¡ (ì§„ë‹¨)-ê²°ë¡ (ì²˜ë°©)**ì˜ íë¦„ì„ ê°–ì¶˜ **ì´ 10ë¬¸ë‹¨ ì´ìƒì˜ ê¸´ ê¸€**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
                "4. **ì–´ì¡°**: ì „ë¬¸ì ì¸ ì‹¬ë¦¬í•™ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ë˜, ë”°ëœ»í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ í’€ì–´ì£¼ì„¸ìš”.\n\n"
                "### [ë¦¬í¬íŠ¸ êµ¬ì¡°]\n"
                "1ë¶€. **ë§ˆìŒì˜ ì§€ë„ (í˜„ìƒ ì§„ë‹¨)** (5ë¬¸ë‹¨)\n"
                "   - ë‚´ë‹´ìê°€ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê°ì • ì–¸ì–´ì™€ ë‚´ë©´ì˜ ìƒíƒœ ë¶„ì„\n"
                "   - ë°˜ë³µë˜ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ íŒ¨í„´ì´ë‚˜ ê°ì •ì˜ íŠ¸ë¦¬ê±° íŒŒì•…\n"
                "   - ìˆ¨ê²¨ì§„ ê¸ì •ì ì¸ ìì›ì´ë‚˜ ê°•ì  ë°œêµ´\n\n"
                "2ë¶€. **ë‚˜ì•„ê°€ì•¼ í•  ê¸¸ (ë¯¸ë˜ ì²˜ë°©)** (5ë¬¸ë‹¨)\n"
                "   - í˜„ì¬ ìƒíƒœì—ì„œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‹¬ë¦¬ ê¸°ë²• 3ê°€ì§€ (ACT, CBT ë“± í™œìš©)\n"
                "   - ê°ì •ì˜ íŒŒë„ë¥¼ ë‹¤ìŠ¤ë¦¬ëŠ” ìƒí™œ ìŠµê´€ ì œì•ˆ\n"
                "   - ìƒë‹´ì‚¬ë¡œì„œ ì „í•˜ëŠ” ì§„ì‹¬ ì–´ë¦° ê²©ë ¤ì™€ í¬ë§ì˜ ë©”ì‹œì§€\n\n"
                "**ì¤‘ìš”: ëª¨ë“  ë‹µë³€ì€ ì™„ë²½í•œ í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë²ˆì—­íˆ¬ê°€ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.**\n"
                "ì§€ê¸ˆ ë°”ë¡œ í•œêµ­ì–´ë¡œ ë¦¬í¬íŠ¸ ì‘ì„±ì„ ì‹œì‘í•˜ì„¸ìš”."
            )
            
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 4096, # Maximum length
                    "repeat_penalty": 1.1,
                    "top_k": 40,
                    "top_p": 0.9
                }
            }
            
            # Timeout 600s (10 mins) - Increased for OCI environment
            response = requests.post(url, json=payload, timeout=600)
            
            if response.status_code == 200:
                result = response.json().get('response', '')
                return result
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ë„ì¤‘ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ Report Generation Error: {e}")
            return "ë¦¬í¬íŠ¸ ìƒì„± ì‹œìŠ¤í…œì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_long_term_insight(self, report_history):
        """
        [Meta-Analysis] Analyzes multiple past reports to find long-term patterns.
        """
        import requests
        print(f"ğŸ§  [Brain] Generating Long-Term Insight from {len(report_history)} reports...")
        
        if not report_history:
            return "ë¶„ì„í•  ê³¼ê±° ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
        try:
            url = "http://localhost:11434/api/generate"
            
            # Construct context from history
            history_context = ""
            for i, r in enumerate(report_history):
                date = r.get('date', 'Unknown Date')
                content = r.get('content', '')[:500] # Truncate to save context window
                history_context += f"### [ë¦¬í¬íŠ¸ {i+1} - {date}]\n{content}...\n\n"
                
            prompt_text = (
                "## SYSTEM: You represent a wise psychologist specializing in long-term therapy. Answer in KOREAN ONLY.\n"
                "ë‹¹ì‹ ì€ ë‚´ë‹´ìì˜ 'ê³¼ê±° ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸ë“¤'ì„ ì¢…í•©í•˜ì—¬ ì¥ê¸°ì ì¸ ë³€í™”ì™€ íë¦„ì„ ë¶„ì„í•˜ëŠ” 'ë©”íƒ€ ë¶„ì„ê°€'ì…ë‹ˆë‹¤.\n"
                "ì•„ë˜ ì œê³µëœ ê³¼ê±° ë¦¬í¬íŠ¸ ê¸°ë¡ë“¤ì„ ì½ê³ , ë‚´ë‹´ìì˜ ì‹¬ë¦¬ ìƒíƒœê°€ ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
                f"{history_context}\n"
                "### [ì‘ì„± ì§€ì¹¨]\n"
                "1. **ì–¸ì–´**: ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "2. **êµ¬ì¡°**:\n"
                "   - **ë³€í™”ì˜ íë¦„**: ê°ì •ì´ë‚˜ íƒœë„ê°€ ì–´ë–»ê²Œ ë³€í•´ì™”ëŠ”ì§€ (ê¸ì •ì /ë¶€ì •ì  ë³€í™”)\n"
                "   - **ë°˜ë³µë˜ëŠ” íŒ¨í„´**: ì‹œê°„ì´ ì§€ë‚˜ë„ ì—¬ì „íˆ í•´ê²°ë˜ì§€ ì•Šê³  ë°˜ë³µë˜ëŠ” ë¬¸ì œì \n"
                "   - **ì¥ê¸° ì œì–¸**: ì•ìœ¼ë¡œì˜ 1ê°œì›”ì„ ìœ„í•œ í•µì‹¬ ì¡°ì–¸\n"
                "3. **ë¶„ëŸ‰**: 3~4ë¬¸ë‹¨ ë‚´ì™¸ë¡œ ê¹Šì´ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
                "ë©”íƒ€ ë¶„ì„ ê²°ê³¼:"
            )
            
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.6,
                    "num_predict": 2048
                }
            }
            
            response = requests.post(url, json=payload, timeout=300)
            
            if response.status_code == 200:
                return response.json().get('response', '')
            else:
                return "ë©”íƒ€ ë¶„ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ Long-Term Insight Error: {e}")
            return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


    def update_keywords(self, text, mood_level):
        # Learn new keywords from the text based on the user's provided mood_level.
        if not text: return

        # Map mood_level to label
        # mood_level is expected to be int
        try:
            mood_val = int(mood_level)
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            target_label = mapping.get(mood_val)
            
            if target_label is None:
                return # Invalid mood level
        except:
            return

        session = self.Session()
        try:
            from models import EmotionKeyword
            
            # Simple Tokenization (Space-based)
            # In a real KR app, use Mecab/Konlpy. Here we split by space.
            words = text.split()
            
            for w in words:
                # Basic cleaning
                w = w.strip('.,?!~"\'')
                if len(w) < 2: continue # Skip single chars
                
                # Check if exists
                existing = session.query(EmotionKeyword).filter_by(keyword=w).first()
                
                if existing:
                    # If exists, increment frequency if label matches
                    # If label differs, maybe decrease freq or ignore? 
                    # Let's just track co-occurrence. Complex logic omitted for simplicity.
                    if existing.emotion_label == target_label:
                        existing.frequency += 1
                else:
                    # NEW WORD -> LEARN IT!
                    print(f"Learning new keyword: {w} -> {self.classes[target_label]}")
                    new_kw = EmotionKeyword(
                        keyword=w,
                        emotion_label=target_label,
                        frequency=1
                    )
                    session.add(new_kw)
            
            session.commit()
            
            # If Model is active, we might want to update tokenizer or retrain eventually.
            # Rerunning Tokenizer.fit_on_texts would be needed. 
            # For this session, we won't retrain the LSTM live (too slow), but the Fallback logic will immediately benefit.
            
        except Exception as e:
            print(f"Learning error: {e}")
            session.rollback()
        finally:
            session.close()

