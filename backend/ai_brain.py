import numpy as np
import os
import random
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from config import Config
import json
import requests
import re
import time
import ast # Added for safe literal eval
TRAINING_STATE_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'training_state.json')

try:
    from emotion_codes import EMOTION_CODE_MAP
except ImportError:
    print("Warning: Could not import EMOTION_CODE_MAP from emotion_codes")
    EMOTION_CODE_MAP = {}

# TensorFlow/Keras Import (Optional)
try:
    raise ImportError("Disabled to prevent mutex crash on macOS")
    if False and os.getenv("ENABLE_TF"): # Disabled by default to fix Mac M1/M2 mutex crash
        from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
    from tensorflow.keras.utils import to_categorical
    import pandas as pd
    TENSORFLOW_AVAILABLE = True
    print("AI Brain: TensorFlow Available.")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"AI Brain: Local Emotion Model support skipped ({e}).")



class EmotionAnalysis:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.max_len = 50
        self.vocab_size = 0
        
        # 60 Ultra-Fine-Grained Emotion Classes
        self.sorted_codes = sorted(EMOTION_CODE_MAP.keys())
        self.classes = [EMOTION_CODE_MAP[code] for code in self.sorted_codes]
        self.code_to_idx = {code: i for i, code in enumerate(self.sorted_codes)}
        
        # We will load keywords from DB for fallback/learning
        try:
            from pymongo import MongoClient
            from config import Config
            self.mongo_client = MongoClient(Config.MONGO_URI)
            self.db = self.mongo_client.get_database() # Uses default db from URI
            print("AI Brain: Connected to MongoDB.")
        except Exception as e:
            print(f"AI Brain: MongoDB Connection Failed: {e}")
            self.db = None

        self.train_texts = []
        self.train_labels = np.array([], dtype=int)
        
        self.comment_bank = {} # Will load from file

        
        # Initialize attributes
        # Removed Polyglot-Ko attributes

               
        # === Tensorflow / LSTM Model Logic ===
        if TENSORFLOW_AVAILABLE:
            self.tokenizer = Tokenizer()
            # Check for saved model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            self.model_path = os.path.join(base_dir, 'emotion_model.h5')
            self.tokenizer_path = os.path.join(base_dir, 'tokenizer.pickle')
            
            # Check Training Condition
            current_count = self._get_keyword_count()
            last_count = self._get_last_trained_count()
            diff = current_count - last_count
            
            print(f"ğŸ“Š Training Check: Current Keywords={current_count}, Last Trained={last_count}, Diff={diff}")
            
            should_train = (diff >= 100)
            model_exists = os.path.exists(self.model_path) and os.path.exists(self.tokenizer_path)

            if should_train:
                print("âš ï¸ Should train, but skipping for now to rely on Local LLM/Fallback.")
                self._save_training_state(current_count)
            elif model_exists:
                print("ğŸ“¦ Models found. Loading existing models...")
                self._load_existing_models()
                print("âœ… Emotion Model loaded.")
            else:
                print("âš ï¸ No models found. Using Keyword Fallback.")
            
            print("AI Model initialization finished.")

        else: # TENSORFLOW NOT AVAILABLE
             print("Initializing Fallback Emotion Analysis (Keyword based - 5 classes)...")

        # Load Comment Bank (Safety Net) - Always load this
        self.load_comment_bank()
        self.load_emotion_bank()

    def _sanitize_context(self, text):
        """
        Privacy Guard: Removes potential sensitive information before sending to external API.
        - Truncates long text
        - Focuses on sentiment-carrying words
        """
        if not text: return ""
        # Simple privacy protection: Remove common patterns (emails, phone numbers)
        import re
        text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', text)
        text = re.sub(r'\d{2,3}-\d{3,4}-\d{4}', '[PHONE]', text)
        
        # To further protect privacy, we could use the local emotion results 
        # instead of raw text, but for 'insight' we need some context.
        # We limit the length to prevent sending too much detail.
        return text[:100].strip() + "..." if len(text) > 100 else text

    def _get_keyword_count(self):

        """Get total count of emotion keywords from DB (MongoDB)"""
        if self.db is None:
            return 0
        try:
            count = self.db.emotion_keywords.count_documents({})
            return count
        except Exception as e:
            print(f"Error counting keywords: {e}")
            return 0

    def _get_last_trained_count(self):
        """Read last trained count from JSON"""
        if os.path.exists(TRAINING_STATE_FILE):
            try:
                with open(TRAINING_STATE_FILE, 'r') as f:
                    data = json.load(f)
                    return data.get('last_keyword_count', 0)
            except:
                return 0
        return 0

    def _save_training_state(self, count):
        """Save current keyword count to JSON"""
        try:
            with open(TRAINING_STATE_FILE, 'w') as f:
                json.dump({'last_keyword_count': count}, f)
        except Exception as e:
            print(f"Error saving training state: {e}")

    def _load_existing_models(self):
        """Helper to load existing models"""
        try:
            import pickle
            from tensorflow.keras.models import load_model
            
            self.model = load_model(self.model_path)
            
            with open(self.tokenizer_path, 'rb') as handle:
                self.tokenizer = pickle.load(handle)
                
            self.vocab_size = len(self.tokenizer.word_index) + 1
            print("Emotion Model loaded.")
                
        except Exception as e:
            print(f"Error loading models: {e}.")
            # Assuming fallback will take over

    def load_comment_bank(self):
        """Load curated advice from JSON"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.comment_bank = data.get('keywords', {})
                print(f"Loaded {len(self.comment_bank)} keyword categories from Comment Bank.")
            else:
                print("Comment Bank file not found. Skipping.")
        except Exception as e:
            print(f"Error loading comment bank: {e}")

    def generate_keyword_comment(self, user_input):
        """Phase 1: Hybrid Keyword System (Priority 1)"""
        if not self.comment_bank or not user_input:
            return None
            
        # Extract text if input is dict
        if isinstance(user_input, dict):
            text = f"{user_input.get('event', '')} {user_input.get('emotion', '')} {user_input.get('self_talk', '')}"
        else:
            text = str(user_input)
            
        for category, content in self.comment_bank.items():
            # Safety check: content must be a dict
            if not isinstance(content, dict):
                continue
                
            if category in text:
                return content.get('default', "í˜ë‚´ì„¸ìš”.")
                
            keywords = content.get('emotion_keywords', [])
            for k in keywords:
                if k in text:
                    return content.get('default', "í˜ë‚´ì„¸ìš”.")
        return None

    def load_emotion_bank(self):
        """Load 60-class emotion advice"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'emotion_comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.emotion_bank = data.get('keywords', {}) # Structure reuse
                print(f"Loaded {len(self.emotion_bank)} emotion categories.")
            else:
                self.emotion_bank = {}
        except Exception as e:
            print(f"Error loading emotion bank: {e}")
            self.emotion_bank = {}

    def generate_label_comment(self, emotion_label):
        """Phase 1.5: Label-Based Retrieval (Priority 2)"""
        if not self.emotion_bank:
            return None
            
        try:
            if '(' in emotion_label:
                label_key = emotion_label.rsplit(' (', 1)[0]
                pass
            else:
                label_key = emotion_label
        except:
             label_key = emotion_label
        
        # Exact match attempt
        if label_key in self.emotion_bank:
            return self.emotion_bank[label_key].get('default')
            
        # Fuzzy match 
        for key in self.emotion_bank:
            if key in emotion_label:
                return self.emotion_bank[key].get('default')
        
        return None
        



    
    def generate_pre_write_insight(self, recent_diaries, weather=None, weather_stats=None):
        """
        Generates a warm insight using Local Gemma 2 (Ollama).
        """
        import requests
        import json

        print(f"ğŸ” [Insight] Request received. Recent diaries count: {len(recent_diaries)}, Weather: {weather}")
        
        if not recent_diaries:
            return "ì˜¤ëŠ˜ì˜ ì²« ê¸°ë¡ì„ ì‹œì‘í•´ë³´ì„¸ìš”! ì†”ì§í•œ ë§ˆìŒì„ ë‹´ìœ¼ë©´ ë©ë‹ˆë‹¤."

        try:
            # Construct Prompt
            diary_context = ""
            for d in recent_diaries:
                sanitized_event = self._sanitize_context(d.get('event',''))
                diary_context += f"- [{d.get('date','')}] ê¸°ë¶„:{d.get('mood','')} / ë‚´ìš©:{sanitized_event}\n"

            weather_info = f"ì˜¤ëŠ˜ì˜ ë‚ ì”¨: {weather}" if weather else "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´ ì—†ìŒ"
            stats_info = f" (ê³¼ê±° ì´ ë‚ ì”¨ì— ë‹¹ì‹ ì€ ì£¼ë¡œ {weather_stats} ê°ì •ì„ ëŠë¼ì…¨ë„¤ìš”)" if weather_stats else ""

            prompt_text = (
                "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§€ë‚œ ì¼ê¸° ê¸°ë¡ê³¼ ì˜¤ëŠ˜ì˜ ë‚ ì”¨, ê·¸ë¦¬ê³  'ê³¼ê±° ë‚ ì”¨ë³„ ê°ì • íŒ¨í„´'ì„ ë¶„ì„í•˜ì—¬ ë”°ëœ»í•œ í•œ ë¬¸ì¥ì˜ ì¡°ì–¸ì„ ê±´ë„¤ëŠ” ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\n\n"
                f"### {weather_info}{stats_info}\n"
                "### ì‚¬ìš©ìì˜ ìµœê·¼ 1ì£¼ì¼ íë¦„\n"
                f"{diary_context}\n"
                "### ì§€ì‹œì‚¬í•­\n"
                "1. ë°˜ë“œì‹œ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "2. [í•„ìˆ˜] ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë‚˜ ê³„ì ˆê°ì„ ì–¸ê¸‰í•˜ë©° ì‹œì‘í•˜ì„¸ìš”. (ì˜ˆ: 'ë¹„ê°€ ì˜¤ëŠ” ë‚ ì—”...', 'ë§‘ì€ í–‡ì‚´ì²˜ëŸ¼...')\n"
                "3. ìµœê·¼ 1ì£¼ì¼ê°„ì˜ ê°ì • íë¦„ì´ ì¢‹ì€ì§€ ë‚˜ìœì§€ë¥¼ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ í•˜ì„¸ìš”.\n"
                "4. 'ì˜¤ëŠ˜ í•˜ë£¨ ì‘ì›í•©ë‹ˆë‹¤' ê°™ì€ ë»”í•œ ë§ì€ ê¸ˆì§€ì…ë‹ˆë‹¤.\n"
                "5. 40ì~80ì ë‚´ì™¸ë¡œ ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§(í•´ìš”ì²´)ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n\n"
                "ìƒë‹´ì‚¬ ì¡°ì–¸(ë‚ ì”¨ì™€ ê°ì • íë¦„ì´ í†µí•©ëœ í•œ ë¬¸ì¥):"
            )

            # Ollama Payload
            payload = {
                "model": "maumON-gemma", # Use Custom Model Name
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.5,
                    "num_predict": 60 
                }
            }
            
            print(f"ğŸ¦™ [Insight] Requesting Ollama (Maum-On Gemma)...")
            url = "http://localhost:11434/api/generate"
            
            # Timeout Increased to 60s (OCI CPU might be slow or busy)
            response = requests.post(url, json=payload, timeout=60)
            
            if response.status_code != 200:
                print(f"âŒ Ollama Insight Error {response.status_code}: {response.text}")
                return None
                
            result = response.json()
            response_text = result.get('response', '').strip()
            
            # Cleanup quotes if model adds them
            if response_text.startswith('"') and response_text.endswith('"'):
                response_text = response_text[1:-1]
                
            print(f"âœ… [Insight] Gemma Success: {response_text}")
            return response_text

        except Exception as e:
            print(f"âŒ [Insight] Inference Failed: {str(e)}")
            return None

    def analyze_diary_with_local_llm(self, text, history_context=None, user_risk_level=1):
        # [Local AI Mode] Uses Local Ollama (Gemma 2) for Analysis.
        import requests
        import json
        
        # Local Ollama URL
        print(f"ğŸ¦™ [Local AI] Requesting Ollama (Maum-On Gemma)...", end=" ", flush=True)
        try:
            url = "http://localhost:11434/api/generate"
            
            # Context Injection
            context_section = ""
            if history_context:
                context_section = (
                    f"### [ì°¸ê³ : ë‚´ë‹´ìì˜ ê³¼ê±° ê¸°ë¡]\n"
                    f"{history_context}\n"
                    f"(ì§€ì¹¨: ìœ„ ê³¼ê±° ê¸°ë¡ì˜ íë¦„ì„ ì°¸ê³ í•˜ì—¬, ë§¥ë½ì´ ì´ì–´ì§€ëŠ” ê¹Šì´ ìˆëŠ” ê³µê° ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.)\n\n"
                )

            # Risk Level Context
            risk_desc = "ì•ˆì •(Normal)"
            if user_risk_level >= 4: risk_desc = "ë§¤ìš° ìœ„í—˜(Severe Risk)"
            elif user_risk_level == 3: risk_desc = "ìœ„í—˜(Moderate Risk)"
            
            # [Hybrid Safeguard] Python-side Keyword Check (Priority 0)
            print(f"ğŸ•µï¸ [DEBUG] Analyzing Text (Type: {type(text)}): {text}")
            DANGER_KEYWORDS = ["ì£½ê³ ", "ìì‚´", "ë›°ì–´", "ì‚¬ë¼ì§€ê³ ", "ëë‚´", "ë§í–ˆ", "ì‚´ê¸° ì‹«", "ì¹¼", "ì•½", "ìˆ˜ë©´ì œ"]
            found_keywords = [k for k in DANGER_KEYWORDS if k in text]
            is_urgent_risk = len(found_keywords) > 0
            
            danger_note = ""
            if is_urgent_risk:
                print(f"ğŸš¨ [Analysis] Danger Keywords Detected: {found_keywords}")
                danger_note = f"\n[ê¸´ê¸‰ ì•Œë¦¼: ë‚´ë‹´ìê°€ '{found_keywords}'ì™€ ê°™ì€ ìœ„í—˜í•œ í‘œí˜„ì„ ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë¬´ì¡°ê±´ Followupì„ YESë¡œ í•˜ê³  ì•ˆì „ì„ í™•ì¸í•˜ëŠ” ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”.]"

            prompt_text = (
                f"ë„ˆëŠ” ì‹¬ë¦¬ ìƒë‹´ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ë‚´ë‹´ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ë¶„ì„í•´ì¤˜.\n"
                f"ë‚´ë‹´ìì˜ í˜„ì¬ ìƒíƒœ: {risk_desc} (Level {user_risk_level})\n"
                f"{context_section}"
                f"{danger_note}\n"
                f"### [ì˜¤ëŠ˜ì˜ ì¼ê¸° ë°ì´í„°]:\n{text}\n\n"
                f"### [ë¶„ì„ ì§€ì¹¨]:\n"
                f"1. ë‚´ë‹´ìì˜ 'ìˆ˜ë©´ ìƒíƒœ'ì™€ 'ê°ì •'ì˜ ì—°ê´€ì„±ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•´ì¤˜.\n"
                f"2. ë§Œì•½ ë‚´ë‹´ìê°€ 'ì£½ê³  ì‹¶ë‹¤' ë“± ìœ„í—˜í•œ í‘œí˜„ì„ í–ˆê±°ë‚˜({found_keywords}), ê°ì •ì´ ì˜¤ë«ë™ì•ˆ ê°€ë¼ì•‰ì•„ ìˆë‹¤ë©´ 'ì¶”ê°€ ì§ˆë¬¸'ì„ ìƒì„±í•´ì¤˜.\n"
                f"3. ë‹¨ìˆœíˆ ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ì „ë¬¸ì ì¸ ì‹¬ë¦¬ ë¶„ì„ ì½”ë©˜íŠ¸ë¥¼ í•´ì¤˜.\n\n"
                f"### [í•„ìˆ˜ ë‹µë³€ í˜•ì‹]:\n"
                f"Emotion: ('í–‰ë³µ', 'ìš°ìš¸', 'ë¶„ë…¸', 'í‰ì˜¨', 'ë¶ˆì•ˆ', 'ë‹¹í™©' ì¤‘ í•˜ë‚˜, ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ)\n"
                f"Confidence: (0~100 ìˆ«ìë§Œ)\n"
                f"NeedFollowup: (YES ë˜ëŠ” NO)\n"
                f"Question: (NeedFollowupì´ YESì¼ ë•Œë§Œ, ë‚´ë‹´ìì—ê²Œ ë¬¼ì–´ë³¼ ì¶”ê°€ ì§ˆë¬¸ 1ë¬¸ì¥. ì•„ë‹ˆë©´ 'None')\n"
                f"Comment: (ìˆ˜ë©´ ìƒíƒœë¥¼ ì–¸ê¸‰í•˜ë©° 100ì ì´ë‚´ì˜ ë”°ëœ»í•œ í•œêµ­ì–´ ìœ„ë¡œ)\n"
                f"ë°˜ë“œì‹œ ìœ„ í˜•ì‹ë§Œ ì§€ì¼œì„œ ë‹µë³€í•´."
            )
            
            payload = {
                "model": "maumON-gemma",
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.3, 
                    "num_predict": 160 # Optimized for Speed (OCI CPU)
                }
            }
            
            # Timeout 60s
            response = requests.post(url, json=payload, timeout=60)
            
            if response.status_code != 200:
                print(f"âŒ Ollama Error {response.status_code}: {response.text}")
                return None, None
                
            result = response.json()
            response_text = result.get('response', '').strip()
            print(f"ğŸ” Raw Output: {response_text}")
            
            # Use Regex to parse Korean output
            import re
            
            # 1. Emotion (Korean)
            emotion_match = re.search(r"Emotion:\s*([^\n]+)", response_text)
            emotion_str = emotion_match.group(1).strip().replace("'", "").replace('"', "") if emotion_match else "í‰ì˜¨"
            
            # 2. Confidence
            conf_match = re.search(r"Confidence:\s*(\d+)", response_text)
            confidence = int(conf_match.group(1)) if conf_match else 80

            # 3. Followup
            need_followup_match = re.search(r"NeedFollowup:\s*([^\n]+)", response_text, re.IGNORECASE)
            
            # 4. Comment
            comment_match = re.search(r"Comment:\s*(.*)", response_text, re.DOTALL)
            comment = comment_match.group(1).strip() if comment_match else response_text[:100]
            
            if comment.startswith('"') and comment.endswith('"'): comment = comment[1:-1]
            
            # Final Formatting
            formatted_prediction = f"'{emotion_str} ({confidence}%)'"
            
            return formatted_prediction, comment
                
        except Exception as e:
            print(f"âŒ Local AI Error: {e}")
            return None, None

    def predict(self, text):
        import time
        start_time = time.time()
        
        if not text: 
            return {"emotion": "ë¶„ì„ ë¶ˆê°€", "comment": "ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."}

        emotion_result = "ë¶„ì„ ë¶ˆê°€"
        
        # [User Request] Gemma-First Analysis
        try:
            llm_emotion, llm_comment = self.analyze_diary_with_local_llm(text)
            if llm_emotion:
                 emotion_result = llm_emotion
            else:
                 emotion_result = self._fallback_predict(text)
            
            # Store comment for next step
            self.temp_llm_comment = llm_comment
        except:
            emotion_result = self._fallback_predict(text)
            self.temp_llm_comment = None
            
        # 2. Comment Generation
        comment_result = getattr(self, 'temp_llm_comment', None)
        
        # Final Fallback
        if not comment_result:
            comment_result = self.generate_keyword_comment(text) or "ì˜¤ëŠ˜ í•˜ë£¨ë„ ì •ë§ ìˆ˜ê³ í•˜ì…¨ì–´ìš”."
            
        print(f"âœ¨ [Timer] Total AI Analysis took: {time.time() - start_time:.3f}s")
        return {
            "emotion": emotion_result,
            "comment": comment_result
        }

    def _fallback_predict(self, text):
        # Load keywords from MongoDB
        if self.db is None:
             return "ë¶„ì„ ë¶ˆê°€"

        try:
            # Fetch all keywords from Mongo
            keywords = list(self.db.emotion_keywords.find())
            
            scores = [0] * 5
            found_any = False
            
            for kw in keywords:
                if kw['keyword'] in text:
                    scores[kw['emotion_label']] += kw['frequency']
                    found_any = True
            
            if found_any:
                max_score = max(scores)
                total_score = sum(scores)
                confidence = (max_score / total_score * 100) if total_score > 0 else 85.0
                best_idx = scores.index(max_score)
                return f"{self.classes[best_idx]} ({confidence:.1f}%)"
            else:
                return "ê·¸ì €ê·¸ë˜ (40.0%)" 
        except Exception as e:
            print(f"Fallback error: {e}")
            return "ë¶„ì„ ë¶ˆê°€"

    def generate_comment(self, prediction_text, user_text=None):
        # Generate a supportive comment.
        # Priority: 1. Keyword Bank (Safety Net) 2. AI Generation (Seq2Seq) 3. Fallback
        if user_text:
             # Try Local LLM First if available (or assume it was already tried in predict)
             # But here this method is likely legacy or fallback. 
             pass


        # 1. Phase 1: Keyword Safety Net (Highest Priority)
        if user_text:
            keyword_comment = self.generate_keyword_comment(user_text)
            if keyword_comment:
                 return keyword_comment

        if not prediction_text or "ë¶„ì„ ë¶ˆê°€" in prediction_text:
            return "ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“¤ë ¤ì£¼ì„¸ìš”. í•­ìƒ ë“£ê³  ìˆì„ê²Œìš”."

        # Extract strict label (remove confidence score)
        # e.g. "ë¶„ë…¸ (ë°°ì‹ ê°) (85.0%)" -> "ë¶„ë…¸ (ë°°ì‹ ê°)"
        try:
             emotion_label_only = prediction_text.rsplit(' (', 1)[0]
        except:
             emotion_label_only = prediction_text.split()[0]



        # 3. Phase 1.5: Label-based Specific Advice (Fallback)
        label_comment = self.generate_label_comment(emotion_label_only)
        if label_comment:
             return label_comment

        try:
            label = prediction_text.split()[0] # e.g. "í–‰ë³µí•´"
            
            # 4. Fallback
            return "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”."
            
        except Exception as e:
            print(f"Comment Gen Error: {e}")
            return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ì´í•´í•´ìš”."

    def _call_llm(self, prompt, options=None):
        """
        Hybrid/Async LLM Caller for Brain (RunPod Priority)
        """
        if options is None: options = {}
        
        # RunPod Config
        try:
            from config import Config
            RUNPOD_API_KEY = Config.RUNPOD_API_KEY
            RUNPOD_LLM_URL = Config.RUNPOD_LLM_URL
        except:
            RUNPOD_API_KEY = os.environ.get('RUNPOD_API_KEY')
            RUNPOD_LLM_URL = os.environ.get('RUNPOD_LLM_URL')

        # 1. RunPod Serverless (Priority)
        if RUNPOD_API_KEY and RUNPOD_LLM_URL and "YOUR_POD_ID" not in RUNPOD_LLM_URL:
            try:
                print("ğŸš€ [Brain] Sending Async request to RunPod...")
                
                # Normalize Base URL
                base_url = RUNPOD_LLM_URL.replace('/runsync', '').replace('/run', '').rstrip('/')
                submit_url = f"{base_url}/run"
                
                headers = {
                    "Authorization": f"Bearer {RUNPOD_API_KEY}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "input": {
                        "prompt": prompt,
                        "max_tokens": options.get('num_predict', 2048),
                        "temperature": options.get('temperature', 0.7),
                        "stream": False
                    }
                }
                
                res = requests.post(submit_url, json=payload, headers=headers, timeout=30)
                if res.status_code != 200:
                    print(f"âŒ RunPod Submit Failed: {res.text}")
                    raise Exception("RunPod Submit Failed")
                    
                job_id = res.json()['id']
                print(f"â³ [Brain] Job Submitted: {job_id}. Polling...")
                
                status_url = f"{base_url}/status/{job_id}"
                start_time = time.time()
                
                while True:
                    if time.time() - start_time > 600: # 10 min timeout for reports
                         raise Exception("RunPod Timeout")
                         
                    status_res = requests.get(status_url, headers=headers, timeout=30)
                    status_data = status_res.json()
                    status = status_data.get('status')
                    
                    if status == 'COMPLETED':
                        output = status_data.get('output')
                        print("âœ… [Brain] RunPod Job Completed!")
                        
                        # Process Output
                        if isinstance(output, dict):
                             # Try to get text field
                             if 'reaction' in output:
                                 clean_str = output['reaction'].strip()
                                 # Clean Markdown
                                 if clean_str.startswith('```'):
                                     clean_str = re.sub(r'^```(?:json)?\s*|\s*```$', '', clean_str, flags=re.MULTILINE)
                                 return clean_str.strip()
                             elif 'text' in output:
                                 return output['text']
                             elif 'response' in output:
                                 return output['response']
                             else:
                                 return json.dumps(output, ensure_ascii=False)
                        else:
                             return str(output)
                             
                    elif status in ['FAILED', 'CANCELLED']:
                        print(f"âŒ RunPod Job Failed: {status}")
                        raise Exception(f"RunPod Failed: {status}")
                        
                    time.sleep(2)
                    
            except Exception as e:
                print(f"âŒ RunPod Async Failed: {e}")
                # Fallthrough

        # 2. Local Ollama (Fallback)
        try:
            print("ğŸ¦™ [Brain] Fallback to Local Ollama...")
            url = "http://localhost:11434/api/generate"
            payload = {
                "model": options.get('model', 'gemma2:2b'),
                "prompt": prompt,
                "stream": False,
                "options": options
            }
            response = requests.post(url, json=payload, timeout=600)
            if response.status_code == 200:
                result = response.json().get('response', '')
                return result
        except Exception as e:
             print(f"âŒ Local AI Failed: {e}")
             
        return None
    def generate_comprehensive_report(self, diary_summary):
        """
        Generates a detailed 10-paragraph psychological report using Hybrid AI (RunPod Priority).
        """
        print("ğŸ§  [Brain] Generating Comprehensive Report...")
        
        prompt_text = (
            "## SYSTEM: You represent a thoughtful, empathetic counselor with 20 years of experience. You must ANSWER IN KOREAN ONLY.\n"
            "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë² í…Œë‘ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ë‹´ì(ì‚¬ìš©ì)ì˜ ì¼ê¸° ê¸°ë¡ê³¼ í†µê³„ë¥¼ ìì„¸íˆ ì½ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
            f"### [ì‚¬ìš©ì ë°ì´í„°]\n{diary_summary}\n\n"
            "### [ì‘ì„± ì§€ì¹¨]\n"
            "1. **ì–¸ì–´**: ë°˜ë“œì‹œ **í•œêµ­ì–´(Korean)**ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”. ì˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
            "2. **í˜•ì‹**: ì‚¬ìš©ìì—ê²Œ ë³´ë‚´ëŠ” 'ì‹¬ì¸µ ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸' í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
            "3. **ë¶„ëŸ‰**: ë°˜ë“œì‹œ **ì„œë¡ -ë³¸ë¡ (ì§„ë‹¨)-ê²°ë¡ (ì²˜ë°©)**ì˜ íë¦„ì„ ê°–ì¶˜ **ì´ 10ë¬¸ë‹¨ ì´ìƒì˜ ê¸´ ê¸€**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
            "4. **ì–´ì¡°**: ì „ë¬¸ì ì¸ ì‹¬ë¦¬í•™ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ë˜, ë”°ëœ»í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ í’€ì–´ì£¼ì„¸ìš”.\n\n"
            "### [ë¦¬í¬íŠ¸ êµ¬ì¡°]\n"
            "1ë¶€. **ë§ˆìŒì˜ ì§€ë„ (í˜„ìƒ ì§„ë‹¨)** (5ë¬¸ë‹¨)\n"
            "   - ë‚´ë‹´ìê°€ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê°ì • ì–¸ì–´ì™€ ë‚´ë©´ì˜ ìƒíƒœ ë¶„ì„\n"
            "   - ë°˜ë³µë˜ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ íŒ¨í„´ì´ë‚˜ ê°ì •ì˜ íŠ¸ë¦¬ê±° íŒŒì•…\n"
            "   - ìˆ¨ê²¨ì§„ ê¸ì •ì ì¸ ìì›ì´ë‚˜ ê°•ì  ë°œêµ´\n\n"
            "2ë¶€. **ë‚˜ì•„ê°€ì•¼ í•  ê¸¸ (ë¯¸ë˜ ì²˜ë°©)** (5ë¬¸ë‹¨)\n"
            "   - í˜„ì¬ ìƒíƒœì—ì„œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì‹¬ë¦¬ ê¸°ë²• 3ê°€ì§€ (ACT, CBT ë“± í™œìš©)\n"
            "   - ê°ì •ì˜ íŒŒë„ë¥¼ ë‹¤ìŠ¤ë¦¬ëŠ” ìƒí™œ ìŠµê´€ ì œì•ˆ\n"
            "   - ìƒë‹´ì‚¬ë¡œì„œ ì „í•˜ëŠ” ì§„ì‹¬ ì–´ë¦° ê²©ë ¤ì™€ í¬ë§ì˜ ë©”ì‹œì§€\n\n"
            "**ì¤‘ìš”: ëª¨ë“  ë‹µë³€ì€ ì™„ë²½í•œ í•œêµ­ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë²ˆì—­íˆ¬ê°€ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.**\n"
            "ì§€ê¸ˆ ë°”ë¡œ í•œêµ­ì–´ë¡œ ë¦¬í¬íŠ¸ ì‘ì„±ì„ ì‹œì‘í•˜ì„¸ìš”."
        )
        
        try:
            options = {
                "model": "gemma2:2b", # For fallback
                "temperature": 0.7,
                "num_predict": 4096, # Max length for report
                "repeat_penalty": 1.1,
                "top_k": 40,
                "top_p": 0.9
            }
            
            result = self._call_llm(prompt_text, options)
            
            if result:
                return result
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ë„ì¤‘ ì—°ê²°ì´ ëŠê²¼ìŠµë‹ˆë‹¤."

        except Exception as e:
            print(f"âŒ Report Generation Error: {e}")
            return "ë¦¬í¬íŠ¸ ìƒì„± ì‹œìŠ¤í…œì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def generate_long_term_insight(self, report_history):
        """
        [Meta-Analysis] Analyzes multiple past reports to find long-term patterns.
        """
        import requests
        print(f"ğŸ§  [Brain] Generating Long-Term Insight from {len(report_history)} reports...")
        
        if not report_history:
            return "ë¶„ì„í•  ê³¼ê±° ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            
        try:
            url = "http://localhost:11434/api/generate"
            
            # Construct context from history
            history_context = ""
            for i, r in enumerate(report_history):
                date = r.get('date', 'Unknown Date')
                content = r.get('content', '')[:500] # Truncate to save context window
                history_context += f"### [ë¦¬í¬íŠ¸ {i+1} - {date}]\n{content}...\n\n"
                
            prompt_text = (
                "## SYSTEM: You represent a wise psychologist specializing in long-term therapy. Answer in KOREAN ONLY.\n"
                "ë‹¹ì‹ ì€ ë‚´ë‹´ìì˜ 'ê³¼ê±° ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸ë“¤'ì„ ì¢…í•©í•˜ì—¬ ì¥ê¸°ì ì¸ ë³€í™”ì™€ íë¦„ì„ ë¶„ì„í•˜ëŠ” 'ë©”íƒ€ ë¶„ì„ê°€'ì…ë‹ˆë‹¤.\n"
                "ì•„ë˜ ì œê³µëœ ê³¼ê±° ë¦¬í¬íŠ¸ ê¸°ë¡ë“¤ì„ ì½ê³ , ë‚´ë‹´ìì˜ ì‹¬ë¦¬ ìƒíƒœê°€ ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
                f"{history_context}\n"
                "### [ì‘ì„± ì§€ì¹¨]\n"
                "1. **ì–¸ì–´**: ë°˜ë“œì‹œ **í•œêµ­ì–´**ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "2. **êµ¬ì¡°**:\n"
                "   - **ë³€í™”ì˜ íë¦„**: ê°ì •ì´ë‚˜ íƒœë„ê°€ ì–´ë–»ê²Œ ë³€í•´ì™”ëŠ”ì§€ (ê¸ì •ì /ë¶€ì •ì  ë³€í™”)\n"
                "   - **ë°˜ë³µë˜ëŠ” íŒ¨í„´**: ì‹œê°„ì´ ì§€ë‚˜ë„ ì—¬ì „íˆ í•´ê²°ë˜ì§€ ì•Šê³  ë°˜ë³µë˜ëŠ” ë¬¸ì œì \n"
                "   - **ì¥ê¸° ì œì–¸**: ì•ìœ¼ë¡œì˜ 1ê°œì›”ì„ ìœ„í•œ í•µì‹¬ ì¡°ì–¸\n"
                "3. **ë¶„ëŸ‰**: 3~4ë¬¸ë‹¨ ë‚´ì™¸ë¡œ ê¹Šì´ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
                "ë©”íƒ€ ë¶„ì„ ê²°ê³¼:"
            )
            
            payload = {
                "model": "gemma2:2b",
                "prompt": prompt_text,
                "stream": False,
                "options": {
                    "temperature": 0.6,
                    "num_predict": 2048
                }
            }
            
            response = requests.post(url, json=payload, timeout=300)
            
            if response.status_code == 200:
                return response.json().get('response', '')
            else:
                return "ë©”íƒ€ ë¶„ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âŒ Long-Term Insight Error: {e}")
            return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


    def update_keywords(self, text, mood_level):
        # Learn new keywords from the text based on the user's provided mood_level.
        if not text: return

        # Map mood_level to label
        # mood_level is expected to be int
        try:
            mood_val = int(mood_level)
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            target_label = mapping.get(mood_val)
            
            if target_label is None:
                return # Invalid mood level
        except:
            return

        session = self.Session()
        try:
            from models import EmotionKeyword
            
            # Simple Tokenization (Space-based)
            # In a real KR app, use Mecab/Konlpy. Here we split by space.
            words = text.split()
            
            for w in words:
                # Basic cleaning
                w = w.strip('.,?!~"\'')
                if len(w) < 2: continue # Skip single chars
                
                # Check if exists
                existing = session.query(EmotionKeyword).filter_by(keyword=w).first()
                
                if existing:
                    # If exists, increment frequency if label matches
                    # If label differs, maybe decrease freq or ignore? 
                    # Let's just track co-occurrence. Complex logic omitted for simplicity.
                    if existing.emotion_label == target_label:
                        existing.frequency += 1
                else:
                    # NEW WORD -> LEARN IT!
                    print(f"Learning new keyword: {w} -> {self.classes[target_label]}")
                    new_kw = EmotionKeyword(
                        keyword=w,
                        emotion_label=target_label,
                        frequency=1
                    )
                    session.add(new_kw)
            
            session.commit()
            
            # If Model is active, we might want to update tokenizer or retrain eventually.
            # Rerunning Tokenizer.fit_on_texts would be needed. 
            # For this session, we won't retrain the LSTM live (too slow), but the Fallback logic will immediately benefit.
            
        except Exception as e:
            print(f"Learning error: {e}")
            session.rollback()
        finally:
            session.close()

