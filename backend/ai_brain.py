import numpy as np
import os
import google.generativeai as genai
import random
import random
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from config import Config
import json
TRAINING_STATE_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'training_state.json')
try:
    from emotion_codes import EMOTION_CODE_MAP
except ImportError:
    print("Warning: Could not import EMOTION_CODE_MAP from emotion_codes")
    EMOTION_CODE_MAP = {}

# Optimization for Resource-Constrained Environments (OCI Free Tier)
# If GEMINI_API_KEY is present, we disable heavy local models to save 2.6GB+ RAM.
FORCE_LOCAL_AI_DISABLE = (os.environ.get('GEMINI_API_KEY') is not None)

# TensorFlow/Keras Import (Optional)
try:
    if FORCE_LOCAL_AI_DISABLE:
        raise ImportError("Local AI Disabled to save memory (Gemini mode active)")
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
    from tensorflow.keras.utils import to_categorical
    import pandas as pd
    TENSORFLOW_AVAILABLE = True
    print("AI Brain: TensorFlow Available.")
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"AI Brain: Local Emotion Model support skipped ({e}).")

# Transformers/PyTorch Import (Critical for Insight)
try:
    if FORCE_LOCAL_AI_DISABLE:
         raise ImportError("Local AI Disabled to save memory (Gemini mode active)")
    from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
    print("AI Brain: Transformers/PyTorch Available.")
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"AI Brain: Local GenAI support skipped ({e}).")

class EmotionAnalysis:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.max_len = 50
        self.vocab_size = 0
        
        # 60 Ultra-Fine-Grained Emotion Classes
        # Sort codes to ensure consistent indexing (E10, E11, ... E69)
        self.sorted_codes = sorted(EMOTION_CODE_MAP.keys())
        self.classes = [EMOTION_CODE_MAP[code] for code in self.sorted_codes]
        self.code_to_idx = {code: i for i, code in enumerate(self.sorted_codes)}
        
        # We will load keywords from DB for fallback/learning
        try:
            from pymongo import MongoClient
            from config import Config
            self.mongo_client = MongoClient(Config.MONGO_URI)
            self.db = self.mongo_client.get_database() # Uses default db from URI
            print("AI Brain: Connected to MongoDB.")
        except Exception as e:
            print(f"AI Brain: MongoDB Connection Failed: {e}")
            self.db = None

        # Initial Training Data
        # Replaced hardcoded 5-class data with empty lists. 
        # We rely 100% on the 60-class Sentiment Dialogue Corpus.
        self.train_texts = []
        self.train_labels = np.array([], dtype=int)

        # Fallback Comment Bank
        self.comment_bank = {
            "í–‰ë³µí•´": [
                "ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ í–‰ë³µí•˜ì…¨êµ°ìš”! ì´ ê¸ì •ì ì¸ ì—ë„ˆì§€ê°€ ë‚´ì¼ë„ ì´ì–´ì§€ê¸¸ ë°”ë„ê²Œìš”. ğŸ˜Š",
                "ë“£ê¸°ë§Œ í•´ë„ ê¸°ë¶„ì´ ì¢‹ì•„ì§€ëŠ” ì´ì•¼ê¸°ë„¤ìš”! í–‰ë³µí•œ ìˆœê°„ì„ ì˜¤ë˜ ê°„ì§í•˜ì„¸ìš”.",
                "ì›ƒìŒì´ ê°€ë“í•œ í•˜ë£¨ì˜€ë„¤ìš”. ë‚´ì¼ë„ ì´ë ‡ê²Œ ì›ƒì„ ì¼ì´ ë§ì•˜ìœ¼ë©´ ì¢‹ê² ì–´ìš”!",
                "ì •ë§ ë©‹ì§„ í•˜ë£¨ì˜€êµ°ìš”! ìŠ¤ìŠ¤ë¡œì—ê²Œ ì¹­ì°¬ í•œë§ˆë”” í•´ì£¼ì„¸ìš”. ğŸ‘",
                "í–‰ë³µì€ ì „ì—¼ëœë‹¤ê³  í•˜ì£ . ë‹¹ì‹ ì˜ í–‰ë³µì´ ì£¼ë³€ê¹Œì§€ ë°ê²Œ ë¹„ì¶œ ê±°ì˜ˆìš”.",
                "ê¸°ë¶„ ì¢‹ì€ ì—ë„ˆì§€ê°€ ê°€ë“í•˜ë„¤ìš”! ë§›ìˆëŠ” ê±° ë“œì‹œë©´ì„œ ì˜¤ëŠ˜ì„ ê¸°ë…í•´ë³´ì„¸ìš”.",
                "ìµœê³ ì˜ í•˜ë£¨ë¥¼ ë³´ë‚´ì…¨ë„¤ìš”! ì ë“¤ê¸° ì „ í–‰ë³µí–ˆë˜ ìˆœê°„ì„ ë‹¤ì‹œ ë– ì˜¬ë ¤ë³´ì„¸ìš”.",
                "ì˜¤ëŠ˜ì˜ ì¦ê±°ì›€ì´ ë§ˆìŒì†ì— ì˜¤ë˜ì˜¤ë˜ ë‚¨ê¸°ë¥¼ ë°”ë¼ìš”. ğŸ’–",
                "ì„¸ìƒì´ ë‹¹ì‹ ì„ ì¶•ë³µí•˜ëŠ” ë‚ ì´ì—ˆë‚˜ ë´ìš”! ì •ë§ ê¸°ìœ ì†Œì‹ì´ì—ìš”.",
                "í–‰ë³µí•œ ë‹¹ì‹ ì˜ ëª¨ìŠµì„ ë³´ë‹ˆ ì €ë„ ê¸°ë¶„ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤! íŒŒì´íŒ…!"
            ],
            "í‰ì˜¨í•´": [
                "ë§ˆìŒì´ í¸ì•ˆí•˜ë‹¤ë‹ˆ ë‹¤í–‰ì´ì—ìš”. ë”°ëœ»í•œ ì°¨ í•œ ì”ìœ¼ë¡œ í•˜ë£¨ë¥¼ ë§ˆë¬´ë¦¬í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ğŸµ",
                "ì”ì”í•œ í˜¸ìˆ˜ ê°™ì€ ë§ˆìŒì´ë„¤ìš”. ì´ í‰í™”ë¡œì›€ì´ ê³„ì†ë˜ê¸¸ ë°”ë¼ìš”.",
                "ì—¬ìœ ë¡œìš´ í•˜ë£¨ë¥¼ ë³´ë‚´ì…¨êµ°ìš”. ë³µì¡í•œ ìƒê°ì€ ì ì‹œ ë‚´ë ¤ë†“ê³  ì‰¬ì–´ê°€ì„¸ìš”.",
                "í‰ë²”í•˜ì§€ë§Œ ì†Œì¤‘í•œ í‰ì˜¨í•¨ì´ë„¤ìš”. ì¢‹ì•„í•˜ëŠ” ìŒì•…ì„ ë“¤ìœ¼ë©° íë§í•´ë´ìš”. ğŸµ",
                "ë§ˆìŒì˜ ì‰¼í‘œê°€ í•„ìš”í•œ ìˆœê°„, ë”± ì ì ˆí•œ íœ´ì‹ì„ ì·¨í•˜ì‹  ê²ƒ ê°™ì•„ìš”.",
                "í‰í™”ë¡œìš´ ë§ˆìŒìœ¼ë¡œ ì ìë¦¬ì— ë“¤ ìˆ˜ ìˆê² ë„¤ìš”. ì¢‹ì€ ê¿ˆ ê¾¸ì„¸ìš”. ğŸŒ™",
                "ì¡°ìš©í•œ í–‰ë³µì´ ê¹ƒë“  í•˜ë£¨ì˜€ë„¤ìš”. ì´ëŸ° ë‚ ë“¤ì´ ìŒ“ì—¬ ì‚¶ì„ ë‹¨ë‹¨í•˜ê²Œ ë§Œë“¤ì–´ìš”.",
                "ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ì— ëª¸ì„ ë§¡ê¸´ ë‹¹ì‹ , ì°¸ í¸ì•ˆí•´ ë³´ì—¬ìš”.",
                "ê¸´ì¥ì´ í’€ë¦¬ê³  ë§ˆìŒì´ ë†“ì´ëŠ” ê¸°ë¶„, ì •ë§ ì†Œì¤‘í•˜ì£ .",
                "ì˜¤ëŠ˜ì˜ í‰ì˜¨í•¨ì´ ë‚´ì¼ì„ ì‚´ì•„ê°ˆ í˜ì´ ë˜ì–´ì¤„ ê±°ì˜ˆìš”."
            ],
            "ê·¸ì €ê·¸ë˜": [
                "í‰ë²”í•œ í•˜ë£¨ì˜€êµ°ìš”. ë‚´ì¼ì€ ì¢€ ë” íŠ¹ë³„í•œ ì¼ì´ ìƒê¸¸ì§€ë„ ëª°ë¼ìš”! íŒŒì´íŒ… ğŸ’ª",
                "ë³„ì¼ ì—†ëŠ” í•˜ë£¨ë„ ì†Œì¤‘í•˜ì£ . ë¬´íƒˆí•˜ê²Œ ë³´ë‚¸ ê²ƒì— ê°ì‚¬í•´ë´ìš”.",
                "ë•Œë¡œëŠ” ì”ì”í•œ í•˜ë£¨ê°€ ê°€ì¥ í° íœ´ì‹ì´ ë˜ê¸°ë„ í•œë‹µë‹ˆë‹¤.",
                "ì‹¬ì‹¬í•œ ë‚ ì´ì—ˆë‹¤ë©´, ë‚´ì¼ì€ ì‘ì€ ëª¨í—˜ì„ ê³„íší•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ê·¸ì € ê·¸ëŸ° ë‚ ë„ ì§€ë‚˜ê³  ë³´ë©´ ì¶”ì–µì´ ë  ê±°ì˜ˆìš”. í¸ì•ˆí•œ ë°¤ ë³´ë‚´ì„¸ìš”.",
                "íŠ¹ë³„í•œ ì¼ì€ ì—†ì—ˆì§€ë§Œ, ë‹¹ì‹ ì€ ì˜¤ëŠ˜ë„ ë‹¹ì‹ ì˜ ìë¦¬ë¥¼ ì˜ ì§€ì¼°ì–´ìš”.",
                "ë¬´ë‚œí•œ í•˜ë£¨ì˜€ë„¤ìš”. ë‚´ì¼ì€ ì¢‹ì•„í•˜ëŠ” ê°„ì‹ì„ ë¨¹ìœ¼ë©° ê¸°ë¶„ì„ ì „í™˜í•´ë³¼ê¹Œìš”?",
                "ì˜¤ëŠ˜ì€ ì ì‹œ ì‰¬ì–´ê°€ëŠ” í˜ì´ì§€ë¼ê³  ìƒê°í•´ìš”. ë‚´ì¼ì€ ë˜ ë‹¤ë¥¸ ì´ì•¼ê¸°ê°€ ì“°ì¼ ê±°ì˜ˆìš”.",
                "ê°ì •ì˜ ê¸°ë³µ ì—†ì´ í‰íƒ„í•œ í•˜ë£¨, ê·¸ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ê´œì°®ì•„ìš”.",
                "ë‚´ì¼ì€ ì˜ˆìƒì¹˜ ëª»í•œ ì¦ê±°ì›€ì´ ê¸°ë‹¤ë¦¬ê³  ìˆì„ì§€ë„ ëª°ë¼ìš”!"
            ],
            "ìš°ìš¸í•´": [
                "ë§ì´ í˜ë“œì…¨êµ°ìš”. ì˜¤ëŠ˜ í•˜ë£¨ëŠ” í‘¹ ì‰¬ë©´ì„œ ìì‹ ì„ í† ë‹¥ì—¬ì£¼ì„¸ìš”. ë‹¹ì‹ ì€ ì†Œì¤‘í•œ ì‚¬ëŒì…ë‹ˆë‹¤. ğŸ’™",
                "ë§ˆìŒì´ ë¬´ê±°ìš´ ë‚ ì´ë„¤ìš”. ìš¸ê³  ì‹¶ë‹¤ë©´ ì‹¤ì»· ìš¸ì–´ë„ ê´œì°®ì•„ìš”. ì œê°€ ê³ì— ìˆì„ê²Œìš”.",
                "ê´œì°®ì§€ ì•Šì•„ë„ ê´œì°®ì•„ìš”. ì˜¤ëŠ˜ì€ ë¬´ë¦¬í•˜ì§€ ë§ê³  ìê¸° ìì‹ ë§Œ ìƒê°í•˜ì„¸ìš”.",
                "ë‹¹ì‹ ì˜ ìŠ¬í””ì´ ê¹Šì€ ë§Œí¼, ë‹¹ì‹ ì€ ë”°ëœ»í•œ ë§ˆìŒì„ ê°€ì§„ ì‚¬ëŒì¼ ê±°ì˜ˆìš”.",
                "ì–´ë‘ìš´ ë°¤ì´ ì§€ë‚˜ë©´ ë°˜ë“œì‹œ í•´ê°€ ëœ¹ë‹ˆë‹¤. ì ì‹œ ì›…í¬ë ¤ ìˆì–´ë„ ê´œì°®ì•„ìš”.",
                "í˜ë“  í•˜ë£¨ë¥¼ ë²„í…¨ë‚¸ ë‹¹ì‹ , ì •ë§ ê³ ìƒ ë§ì•˜ì–´ìš”. ë”°ëœ»í•œ ì´ë¶ˆ ì†ì—ì„œ í‘¹ ì£¼ë¬´ì„¸ìš”.",
                "ë§ˆìŒì˜ ë¹„ê°€ ê·¸ì¹˜ê¸°ë¥¼ ê¸°ë‹¤ë¦´ê²Œìš”. í˜¼ìë¼ê³  ìƒê°í•˜ì§€ ë§ˆì„¸ìš”.",
                "ì§€ê¸ˆ ëŠë¼ëŠ” ê°ì •ë„ ë‹¹ì‹ ì˜ ì¼ë¶€ì˜ˆìš”. ë¶€ì •í•˜ì§€ ë§ê³  ê°€ë§Œíˆ ì•ˆì•„ì£¼ì„¸ìš”.",
                "ë§›ìˆëŠ” ê±°ë¼ë„ ë¨¹ê³  ê¸°ìš´ ì°¨ë¦¬ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”. ë‚´ì¼ì€ ì¡°ê¸ˆ ë” ë‚˜ì•„ì§ˆ ê±°ì˜ˆìš”.",
                "ë‹¹ì‹ ì€ í˜¼ìê°€ ì•„ë‹ˆì—ìš”. í˜ë“  ìˆœê°„ì´ ì§€ë‚˜ê°€ê¸¸ í•¨ê»˜ ì‘ì›í• ê²Œìš”."
            ],
            "í™”ê°€ë‚˜": [
                "ì†ìƒí•œ ì¼ì´ ìˆìœ¼ì…¨ë‚˜ ë´ìš”. ì ì‹œ ì‹¬í˜¸í¡ì„ í•˜ë©° ë§ˆìŒì„ ê°€ë¼ì•‰í˜€ë³´ë©´ ì–´ë–¨ê¹Œìš”? í˜ë‚´ì„¸ìš”! ğŸ”¥",
                "ì •ë§ í™”ê°€ ë‚  ë§Œí•œ ìƒí™©ì´ì—ˆêµ°ìš”. ê·¸ ê°ì •ì„ ì–µëˆ„ë¥´ì§€ ë§ê³  ê±´ì „í•˜ê²Œ í’€ì–´ë³´ì„¸ìš”.",
                "ì—´ë°›ëŠ” í•˜ë£¨ì˜€ë„¤ìš”! ì‹œì›í•œ ë¬¼ í•œ ì” ë§ˆì‹œê³  í„¸ì–´ë²„ë¦¬ì„¸ìš”.",
                "ëˆ„êµ¬ë¼ë„ í™”ê°€ ë‚¬ì„ ê±°ì˜ˆìš”. ë‹¹ì‹  ì˜ëª»ì´ ì•„ë‹ˆë‹ˆ ë„ˆë¬´ ìì±…í•˜ì§€ ë§ˆì„¸ìš”.",
                "ë¶„ë…¸ëŠ” ì—ë„ˆì§€ê°€ ë  ìˆ˜ë„ ìˆì–´ìš”. ìš´ë™ì´ë‚˜ ì·¨ë¯¸ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë‚ ë ¤ë²„ë ¤ìš”! ğŸ¥Š",
                "ë§ì´ ì–µìš¸í•˜ì…¨ì£ . ë‹¹ì‹ ì˜ ë§ˆìŒ ë‹¤ ì´í•´í•´ìš”.",
                "í™”ê°€ ë‚  ë•ŒëŠ” ì ì‹œ ê·¸ ìƒí™©ì—ì„œ ë²—ì–´ë‚˜ í™˜ê¸°ë¥¼ ì‹œí‚¤ëŠ” ê²Œ ë„ì›€ì´ ë¼ìš”.",
                "ì˜¤ëŠ˜ì˜ ë‚˜ìœ ê¸°ë¶„ì€ ì˜¤ëŠ˜ë¡œ ëë‚´ë²„ë ¤ìš”. ë‚´ì¼ì€ ê¸°ë¶„ ì¢‹ì€ ì¼ë§Œ ìˆì„ ê±°ì˜ˆìš”.",
                "ì†Œë¦¬ë¼ë„ í•œ ë²ˆ í¬ê²Œ ì§€ë¥´ê³  ì‹¶ë„¤ìš”! ë‹µë‹µí•œ ë§ˆìŒì´ ì¡°ê¸ˆì€ í’€ë¦¬ê¸¸ ë°”ë¼ìš”.",
                "ë‹¹ì‹ ì˜ í‰í™”ë¥¼ ë°©í•´í•œ ê²ƒë“¤ì´ ë°‰ë„¤ìš”. ì˜¤ëŠ˜ì€ ì¼ì° ì‰¬ë©´ì„œ ë§ˆìŒì„ ë‹¤ìŠ¤ë ¤ë´ìš”.",
                # New Additions
                "ì •ë§ í™”ê°€ ë‚˜ì‹œê² ì–´ìš”. ì–µìš¸í•œ ë§ˆìŒ ì´í•´í•´ìš”.",
                "ê·¸ëŸ° ì¼ì´ ìˆì—ˆë‹¤ë‹ˆ ì €ë„ í™”ê°€ ë‚˜ë„¤ìš”.",
                "ì†ìƒí•œ ë§ˆìŒì„ ì–´ë–»ê²Œ ë‹¬ë˜ë©´ ì¢‹ì„ê¹Œìš”? ì ì‹œ ì‰¬ì–´ê°€ëŠ” ê±´ ì–´ë–¨ê¹Œìš”.",
                "ì°¸ì§€ ë§ê³  í™”ë¥¼ í‘œì¶œí•˜ëŠ” ê²ƒë„ ë°©ë²•ì´ì—ìš”. ê±´ê°•í•˜ê²Œ í•´ì†Œí•´ë´ìš”.",
                "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ì„ ë§Œí•œ ìƒí™©ì´êµ°ìš”. ë„ˆë¬´ ë¬´ë¦¬í•˜ì§€ ë§ˆì„¸ìš”.",
                "ë§ˆìŒì„ ê°€ë¼ì•‰íˆê³  ì²œì²œíˆ ìƒê°í•´ë³´ì„¸ìš”. ë‹¹ì‹ ì€ í•  ìˆ˜ ìˆì–´ìš”.",
                "ë¶€ë‹¹í•œ ì¼ì„ ë‹¹í•˜ì…”ì„œ ë§ì´ ì†ìƒí•˜ì‹œê² ì–´ìš”. í˜ë‚´ì„¸ìš”.",
                "ê·¸ ì‚¬ëŒ ë•Œë¬¸ì— ë‹¹ì‹ ì˜ ì†Œì¤‘í•œ ê¸°ë¶„ì„ ë§ì¹˜ì§€ ë§ˆì„¸ìš”.",
                "í™”ë‚˜ëŠ” ê°ì •ì€ ë‹¹ì—°í•œ ë°˜ì‘ì´ì—ìš”. ìŠ¤ìŠ¤ë¡œë¥¼ ë‹¤ë…ì—¬ì£¼ì„¸ìš”.",
                "ì§€ê¸ˆì€ ë§ˆìŒì„ ì§„ì •ì‹œí‚¤ëŠ” ê²Œ ìš°ì„ ì¸ ê²ƒ ê°™ì•„ìš”. ë”°ëœ»í•œ ì°¨ í•œ ì” ì–´ë•Œìš”?"
            ]
        }
        
        # Extend other categories with new comments
        self.comment_bank["í–‰ë³µí•´"].extend([
            "ì •ë§ í–‰ë³µí•˜ì‹¤ ê²ƒ ê°™ì•„ìš”. ì €ë„ ë©ë‹¬ì•„ ê¸°ë¶„ì´ ì¢‹ì•„ì§€ë„¤ìš”!",
            "ê¸ì •ì ì¸ ë§ˆìŒê°€ì§ì´ ì°¸ ì¢‹ì•„ ë³´ì—¬ìš”. ë©‹ì§€ì‹­ë‹ˆë‹¤.",
            "ë…¸ë ¥ì˜ ê²°ê³¼ê°€ ì¢‹ì•„ì„œ ë‹¤í–‰ì´ì—ìš”. ì¶•í•˜ë“œë ¤ìš”!",
            "ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ë‚´ì…¨êµ°ìš”. ê·¸ ê¸°ë¶„ ì˜¤ë˜ ê°„ì§í•˜ì„¸ìš”.",
            "ê¸°ë¶„ ì¢‹ì€ ì¼ì´ ìˆìœ¼ì…¨ë‚˜ ë´ìš”. í–‰ë³µí•œ ì—ë„ˆì§€ ê°ì‚¬í•©ë‹ˆë‹¤.",
            "ì¶•í•˜í•´ìš”! ì•ìœ¼ë¡œë„ ì¢‹ì€ ì¼ë§Œ ê°€ë“í•˜ê¸¸ ë°”ë¼ìš”.",
            "ì„±ì·¨ê°ì„ ëŠë¼ì…¨ë‹¤ë‹ˆ ë©‹ì ¸ìš”. ë‹¹ì‹ ì´ ìë‘ìŠ¤ëŸ¬ì›Œìš”.",
            "í–‰ë³µí•œ í•˜ë£¨ë¥¼ ë³´ë‚´ì‹  ê²ƒ ê°™ì•„ ì €ë„ ê¸°ì˜ë„¤ìš”.",
            "ì›í•˜ì‹œë˜ ì¼ì´ ì´ë£¨ì–´ì ¸ì„œ ë‹¤í–‰ì´ì—ìš”. ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”.",
            "ì›ƒìŒì´ ëŠì´ì§€ ì•ŠëŠ” í•˜ë£¨ê°€ ë˜ê¸¸ ì‘ì›í• ê²Œìš”!"
        ])
        
        self.comment_bank["í‰ì˜¨í•´"].extend([
            "ë§ˆìŒì´ í¸ì•ˆí•˜ì‹œë‹¤ë‹ˆ ë‹¤í–‰ì´ì—ìš”. ê·¸ í‰ì˜¨í•¨ì´ ì§€ì†ë˜ê¸¸.",
            "ê±±ì • ì—†ì´ í‘¹ ì‰¬ì‹œëŠ” ê²ƒë„ ì¤‘ìš”í•˜ì£ . íë§í•˜ëŠ” ì‹œê°„ ë˜ì„¸ìš”.",
            "ì‚°ì±…ì„ í•˜ë©° ì—¬ìœ ë¥¼ ì¦ê¸°ì…¨êµ°ìš”. ìì—° ì†ì—ì„œ ì¹˜ìœ ë°›ìœ¼ì…¨ê¸¸.",
            "í¸ì•ˆí•œ ì‹œê°„ì„ ë³´ë‚´ê³  ê³„ì‹œë„¤ìš”. ë¶€ëŸ¬ì›Œìš”!",
            "ì•ˆì •ëœ ë§ˆìŒì´ ê³„ì†ë˜ê¸¸ ë°”ë¼ìš”. ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³ í–ˆì–´ìš”.",
            "ì°¨ í•œ ì”í•˜ë©° ì‰¬ëŠ” ì‹œê°„ì€ ì •ë§ ì†Œì¤‘í•˜ì£ . ë”°ëœ»í•œ ì‹œê°„ ë˜ì„¸ìš”.",
            "ì•„ë¬´ëŸ° ê·¼ì‹¬ ì—†ì´ í‰í™”ë¡œìš´ ìƒíƒœì‹œêµ°ìš”. ì°¸ ë³´ê¸° ì¢‹ì•„ìš”.",
            "ê³ ìš”í•œ ì‹œê°„ì„ ì¦ê¸°ëŠ” ê²ƒë„ íë§ì´ ë˜ì£ . ì˜¤ë¡¯ì´ ë‚˜ì—ê²Œ ì§‘ì¤‘í•´ë´ìš”.",
            "ì˜¤ëŠ˜ í•˜ë£¨ê°€ í‰ì˜¨í•˜ê²Œ ë§ˆë¬´ë¦¬ë˜ê¸¸ ë°”ë¼ìš”. êµ¿ë‚˜ì‡!",
            "ë§ˆìŒì˜ ì—¬ìœ ë¥¼ ì°¾ëŠ” ëª¨ìŠµì´ ë³´ê¸° ì¢‹ì•„ìš”. í•­ìƒ ì‘ì›í• ê²Œìš”."
        ])
        
        self.comment_bank["ìš°ìš¸í•´"].extend([
            "ë§ˆìŒì´ ë§ì´ í˜ë“œì‹  ê²ƒ ê°™ë„¤ìš”. ì œê°€ ë“¤ì–´ë“œë¦´ê²Œìš”.",
            "ë¬´ìŠ¨ ì¼ì¸ì§€ ì¢€ ë” ìì„¸íˆ ë“£ê³  ì‹¶ì–´ìš”. ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”.",
            "ë§ì´ ê´´ë¡œìš°ì‹œê² ì–´ìš”. í˜¼ì ë™ë™ ì•“ì§€ ë§ˆì‹œê³  í„¸ì–´ë†“ì•„ ë³´ì„¸ìš”.",
            "í˜¼ìë¼ê³  ìƒê°í•˜ì§€ ë§ˆì‹œê³  ê¸°ìš´ ë‚´ì„¸ìš”. ë‹¹ì‹ ì€ ì†Œì¤‘í•´ìš”.",
            "ìƒì²˜ë°›ì€ ë§ˆìŒì„ ì˜ ì¶”ìŠ¤ë¥´ì‹œê¸¸ ë°”ë¼ìš”. ì‹œê°„ì€ ë‹¹ì‹  í¸ì´ì—ìš”.",
            "ì§€ê¸ˆì€ í˜ë“¤ì§€ë§Œ ë¶„ëª… ë‚˜ì•„ì§ˆ ê±°ì˜ˆìš”. ë¯¿ì–´ ì˜ì‹¬ì¹˜ ì•Šì•„ìš”.",
            "ìš°ìš¸í•œ ë§ˆìŒì´ ë“¤ ë•ŒëŠ” ì ì‹œ ì‰¬ì–´ê°€ë„ ì¢‹ì•„ìš”. ê´œì°®ì•„ìš”.",
            "ë‹¹ì‹ ì˜ ì˜ëª»ì´ ì•„ë‹ˆì—ìš”. ë„ˆë¬´ ìì±…í•˜ì§€ ë§ˆì„¸ìš”.",
            "ëˆˆë¬¼ì„ í˜ë¦¬ëŠ” ê²ƒë„ ê°ì • í•´ì†Œì— ë„ì›€ì´ ë¼ìš”. í‘í‘ ìš°ì…”ë„ ë¼ìš”.",
            "ì œê°€ í•­ìƒ ê³ì—ì„œ ë“¤ì–´ë“œë¦´ê²Œìš”. í˜ë“  í•˜ë£¨ ê³ ìƒ ë§ì•˜ì–´ìš”."
        ])
        
        # Initialize attributes
        self.gpt_model = None
        self.gpt_tokenizer = None
        self.gemini_model = None
        
        # Safe device init (Avoid 'torch' not defined error)
        try:
            import torch
            self.device = torch.device("cpu")
        except ImportError:
            self.device = "cpu" # Fallback string if torch is missing

        # 1. Gemini AI Loading (Priority)
        if hasattr(Config, 'GEMINI_API_KEY') and Config.GEMINI_API_KEY:
            try:
                print("ğŸš€ Initializing Gemini AI for Insight...")
                genai.configure(api_key=Config.GEMINI_API_KEY)
                
                # Dynamically find the best available model to avoid 404
                print("ğŸ” Scanning available Gemini models...")
                models = genai.list_models()
                available_names = [m.name for m in models if 'generateContent' in m.supported_generation_methods]
                print(f" Found Models: {available_names}")
                
                # Priority: Stable 1.5 Flash (First) -> Variants -> Pro -> Experimental
                target_names = [
                    'models/gemini-flash-latest', # This exists in your list! (likely 1.5)
                    'models/gemini-1.5-flash', 
                    'models/gemini-1.5-flash-latest', 
                    'models/gemini-1.5-flash-001',
                    'models/gemini-1.5-flash-002',
                    'models/gemini-1.5-pro',
                    'models/gemini-pro',
                    'models/gemini-2.5-flash' # Last resort
                ]
                
                # Smart Match: Find the first target that partially matches any available model
                chosen = None
                for target in target_names:
                     if target in available_names:
                         chosen = target
                         break
                
                # If exact match fails, try partial match (e.g. 'gemini-1.5-flash' matches 'models/gemini-1.5-flash-001')
                if not chosen:
                    for target in target_names:
                        for available in available_names:
                            if target.replace("models/", "") in available:
                                chosen = available
                                break
                        if chosen: break

                if not chosen and available_names:
                    # Avoid 2.5/2.0 beta models if possible unless they are the ONLY option
                    chosen = available_names[0]
                
                if chosen:
                    self.gemini_model = genai.GenerativeModel(chosen)
                    print(f"âœ… Gemini AI Initialized successfully with model: {chosen}")
                else:
                    print("âŒ No compatible Gemini models found.")
                    
            except Exception as e:
                print(f"âŒ Gemini Initialization Failed: {e}")

        # 2. Local Generative AI Loading (Fallback)
        # Verify torch/transformers is actually available first
        if not FORCE_LOCAL_AI_DISABLE and TRANSFORMERS_AVAILABLE and not self.gemini_model:
            print("Initializing Generative AI (Polyglot-Ko) for Insight (Fallback)...")
            try:
                # Optimized for OCI (CUDA/CPU) and Local (MPS)
                if torch.cuda.is_available():
                    device = torch.device("cuda")
                    torch_dtype = torch.float16
                    print("ğŸš€ Using CUDA for AI acceleration (Cloud/GPU).")
                elif torch.backends.mps.is_available():
                    device = torch.device("mps")
                    torch_dtype = torch.float16
                    print("ğŸš€ Using MPS for AI acceleration (Local Mac).")
                else:
                    device = torch.device("cpu")
                    torch_dtype = torch.float32 
                    print("âš ï¸ Using CPU for AI (Cloud/Standard). Performance may be lower.")
                
                model_name = "EleutherAI/polyglot-ko-1.3b"
                print(f"Loading Polyglot-Ko-1.3B Model (Dtype: {torch_dtype}, Device: {device})...")
                
                self.gpt_tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.gpt_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch_dtype,
                    low_cpu_mem_usage=True
                ).to(device)
                self.device = device 
                print("âœ… Polyglot-Ko-1.3B Loaded successfully.")
            except Exception as e:
                print(f"âŒ Polyglot Load Failed: {e}")
                self.gpt_model = None
                self.gpt_tokenizer = None
        elif self.gemini_model:
            print("â„¹ï¸ Gemini AI is active. Skipping Local Polyglot load to save RAM.")
        else:
            print("âš ï¸ Transformers library not available. Skipping GenAI load.")
               
        # === Tensorflow / LSTM Model Logic ===
        if TENSORFLOW_AVAILABLE:
            self.tokenizer = Tokenizer()
            # Check for saved model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            self.model_path = os.path.join(base_dir, 'emotion_model.h5')
            self.tokenizer_path = os.path.join(base_dir, 'tokenizer.pickle')
            # ... (LSTM loading logic continues implicitly or is handled by load_model later if needed)
               
        # === Tensorflow / LSTM Model Logic ===
        if TENSORFLOW_AVAILABLE:
            # Check Training Condition
            current_count = self._get_keyword_count()
            last_count = self._get_last_trained_count()
            diff = current_count - last_count
            
            print(f"ğŸ“Š Training Check: Current Keywords={current_count}, Last Trained={last_count}, Diff={diff}")
            
            should_train = (diff >= 100)
            model_exists = os.path.exists(self.model_path) and os.path.exists(self.tokenizer_path)

            if should_train:
                try:
                    # In this version of the code, training is handled by a separate script or method.
                    if hasattr(self, '_load_and_train_models'):
                        self._load_and_train_models()
                    else:
                        print("âš ï¸ _load_and_train_models method not found. Skipping auto-training.")
                    self._save_training_state(current_count)
                    print("âœ… Training complete and state saved.")
                except Exception as e:
                    print(f"âŒ Training failed: {e}")
                    # If training failed, try loading existing models if they exist
                    if model_exists:
                        print("ğŸ“¦ Training failed, but models found. Loading existing models...")
                        self._load_existing_models()
                        print("âœ… Emotion Model loaded.")
            elif model_exists:
                print("ğŸ“¦ Models found. Loading existing models...")
                self._load_existing_models()
                print("âœ… Emotion Model loaded.")
            else:
                print("âš ï¸ No models found and new data < 100. Skipping training.")
                print("   The server will run in Basic Mode (Keyword Fallback).")
            
            print("AI Model initialization finished.")

        else: # TENSORFLOW NOT AVAILABLE
             print("Initializing Fallback Emotion Analysis (Keyword based - 5 classes)...")

        # Load Comment Bank (Safety Net) - Always load this
        self.load_comment_bank()
        self.load_emotion_bank()

    def _sanitize_context(self, text):
        """
        Privacy Guard: Removes potential sensitive information before sending to external API.
        - Truncates long text
        - Focuses on sentiment-carrying words
        """
        if not text: return ""
        # Simple privacy protection: Remove common patterns (emails, phone numbers)
        import re
        text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', text)
        text = re.sub(r'\d{2,3}-\d{3,4}-\d{4}', '[PHONE]', text)
        
        # To further protect privacy, we could use the local emotion results 
        # instead of raw text, but for 'insight' we need some context.
        # We limit the length to prevent sending too much detail.
        return text[:100].strip() + "..." if len(text) > 100 else text

    def _get_keyword_count(self):

        """Get total count of emotion keywords from DB (MongoDB)"""
        if self.db is None:
            return 0
        try:
            count = self.db.emotion_keywords.count_documents({})
            return count
        except Exception as e:
            print(f"Error counting keywords: {e}")
            return 0

    def _get_last_trained_count(self):
        """Read last trained count from JSON"""
        if os.path.exists(TRAINING_STATE_FILE):
            try:
                with open(TRAINING_STATE_FILE, 'r') as f:
                    data = json.load(f)
                    return data.get('last_keyword_count', 0)
            except:
                return 0
        return 0

    def _save_training_state(self, count):
        """Save current keyword count to JSON"""
        try:
            with open(TRAINING_STATE_FILE, 'w') as f:
                json.dump({'last_keyword_count': count}, f)
        except Exception as e:
            print(f"Error saving training state: {e}")

    def _load_existing_models(self):
        """Helper to load existing models"""
        try:
            import pickle
            from tensorflow.keras.models import load_model
            
            self.model = load_model(self.model_path)
            
            with open(self.tokenizer_path, 'rb') as handle:
                self.tokenizer = pickle.load(handle)
                
            self.vocab_size = len(self.tokenizer.word_index) + 1
            print("Emotion Model loaded.")
                
        except Exception as e:
            print(f"Error loading models: {e}.")
            # Assuming fallback will take over

    def load_comment_bank(self):
        """Load curated advice from JSON"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.comment_bank = data.get('keywords', {})
                print(f"Loaded {len(self.comment_bank)} keyword categories from Comment Bank.")
            else:
                print("Comment Bank file not found. Skipping.")
        except Exception as e:
            print(f"Error loading comment bank: {e}")

    def generate_keyword_comment(self, user_input):
        """Phase 1: Hybrid Keyword System (Priority 1)"""
        if not self.comment_bank or not user_input:
            return None
            
        # Extract text if input is dict
        if isinstance(user_input, dict):
            text = f"{user_input.get('event', '')} {user_input.get('emotion', '')} {user_input.get('self_talk', '')}"
        else:
            text = str(user_input)
            
        for category, content in self.comment_bank.items():
            # Safety check: content must be a dict
            if not isinstance(content, dict):
                continue
                
            if category in text:
                return content.get('default', "í˜ë‚´ì„¸ìš”.")
                
            keywords = content.get('emotion_keywords', [])
            for k in keywords:
                if k in text:
                    return content.get('default', "í˜ë‚´ì„¸ìš”.")
        return None

    def load_emotion_bank(self):
        """Load 60-class emotion advice"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'emotion_comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.emotion_bank = data.get('keywords', {}) # Structure reuse
                print(f"Loaded {len(self.emotion_bank)} emotion categories.")
            else:
                self.emotion_bank = {}
        except Exception as e:
            print(f"Error loading emotion bank: {e}")
            self.emotion_bank = {}

    def generate_label_comment(self, emotion_label):
        """Phase 1.5: Label-Based Retrieval (Priority 2)"""
        if not self.emotion_bank:
            return None
            
        try:
            if '(' in emotion_label:
                label_key = emotion_label.rsplit(' (', 1)[0]
                pass
            else:
                label_key = emotion_label
        except:
             label_key = emotion_label
        
        # Exact match attempt
        if label_key in self.emotion_bank:
            return self.emotion_bank[label_key].get('default')
            
        # Fuzzy match 
        for key in self.emotion_bank:
            if key in emotion_label:
                return self.emotion_bank[key].get('default')
        
        return None
        
    def generate_polyglot_comment(self, user_input, emotion_label):
        """Phase 2: Polyglot-Ko-1.3B Generation (Priority 2)"""
        if not self.gpt_model or not self.gpt_tokenizer:
            return None
            
        try:
            # Handle user_input (str or dict)
            if isinstance(user_input, dict):
                event = user_input.get('event', '')
                emotion = user_input.get('emotion', '')
                self_talk = user_input.get('self_talk', '')
                
                prompt = (
                    "ì—­í™œ: ë‹¹ì‹ ì€ ë‹¤ì •í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë‚´ë‹´ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ë”°ëœ»í•œ ìœ„ë¡œì™€ ê³µê°ì˜ ë§ì„ ê±´ë„¤ì£¼ì„¸ìš”. ë‹µë³€ì€ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”.\n\n"
                    "ìƒí™©: ì‹œí—˜ì— ë–¨ì–´ì ¸ì„œ ìš¸ì—ˆë‹¤.\n"
                    "ê°ì •: ìŠ¬í”” (ì¢Œì ˆ)\n"
                    "ìƒë‹´ì‚¬: ì •ë§ ì†ìƒí•˜ì‹œê² ì–´ìš”. ì—´ì‹¬íˆ ì¤€ë¹„í–ˆì„ í…ë° ê²°ê³¼ê°€ ì¢‹ì§€ ì•Šì•„ ë§ˆìŒì´ ì•„í”„ì‹œì£ . í•˜ì§€ë§Œ ì´ë²ˆ ì‹¤íŒ¨ê°€ ë‹¹ì‹ ì˜ ëª¨ë“  ê²ƒì„ ê²°ì •í•˜ì§€ëŠ” ì•Šì•„ìš”. ì˜¤ëŠ˜ì€ í‘¹ ì‰¬ë©´ì„œ ìŠ¤ìŠ¤ë¡œë¥¼ ìœ„ë¡œí•´ì£¼ì„¸ìš”.\n\n"
                    f"ìƒí™©: {event} {emotion} {self_talk}\n"
                    f"ê°ì •: {emotion_label}\n"
                    "ìƒë‹´ì‚¬:"
                )
            else:
                text = str(user_input)
                prompt = (
                    "ì—­í™œ: ë‹¹ì‹ ì€ ë‹¤ì •í•˜ê³  ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë‚´ë‹´ìì˜ ì¼ê¸°ë¥¼ ì½ê³  ë”°ëœ»í•œ ìœ„ë¡œì™€ ê³µê°ì˜ ë§ì„ ê±´ë„¤ì£¼ì„¸ìš”. ë‹µë³€ì€ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”.\n\n"
                    "ì¼ê¸°: ì˜¤ëŠ˜ í•˜ë£¨ì¢…ì¼ ë„ˆë¬´ í˜ë“¤ì—ˆë‹¤.\n"
                    "ê°ì •: ìš°ìš¸ (ì§€ì¹¨)\n"
                    "ìƒë‹´ì‚¬: ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”. ì§€ì¹œ ëª¸ê³¼ ë§ˆìŒì„ í¸ì•ˆí•˜ê²Œ ë‚´ë ¤ë†“ê³  íœ´ì‹ì„ ì·¨í•´ë³´ì„¸ìš”. ë‹¹ì‹ ì€ ì¶©ë¶„íˆ ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n"
                    f"ì¼ê¸°: {text}\n"
                    f"ê°ì •: {emotion_label}\n"
                    "ìƒë‹´ì‚¬:"
                )
            # Encode and move to device
            encoded = self.gpt_tokenizer(prompt, return_tensors='pt').to(self.device)
            # Remove token_type_ids if present (GPT models don't use it)
            encoded.pop('token_type_ids', None)
            
            input_ids = encoded['input_ids']
            attention_mask = encoded['attention_mask']
            
            # Ensure pad_token_id is set 
            pad_token_id = self.gpt_tokenizer.pad_token_id
            if pad_token_id is None:
                pad_token_id = self.gpt_tokenizer.eos_token_id

            with torch.no_grad():
                gen_ids = self.gpt_model.generate(
                    input_ids,
                    attention_mask=attention_mask, 
                    max_length=len(input_ids[0]) + 50,
                    do_sample=True,
                    temperature=0.6,
                    top_p=0.90,
                    repetition_penalty=1.2,
                    pad_token_id=pad_token_id,
                    eos_token_id=self.gpt_tokenizer.eos_token_id,
                    bos_token_id=self.gpt_tokenizer.bos_token_id,
                    use_cache=True
                )
                
            generated = self.gpt_tokenizer.decode(gen_ids[0], skip_special_tokens=True)
            
            # Explicitly remove specific tokens just in case
            generated = generated.replace("<|endoftext|>", "").replace("<s>", "").replace("</s>", "")

            # Extract response
            if "ìƒë‹´ì‚¬ ë‹µë³€:" in generated:
                response = generated.split("ìƒë‹´ì‚¬ ë‹µë³€:")[-1].strip()
            elif "ìƒë‹´ì‚¬:" in generated:
                 response = generated.split("ìƒë‹´ì‚¬:")[-1].strip()
            else:
                response = generated
                
            # Post-process: Take first 2 sentences only to avoid hallucination
            sentences = response.split('.')
            # Filter out empty strings
            sentences = [s.strip() for s in sentences if s.strip()]
            
            if len(sentences) > 0:
                clean_response = '. '.join(sentences[:2]).strip()
                if not clean_response.endswith(('!', '?', '.')):
                    clean_response += "."
            else:
                clean_response = ""
            
            return clean_response
            
        except Exception as e:
            print(f"KoGPT Generation Error: {e}")
            return None


    
    def generate_pre_write_insight(self, recent_diaries, weather=None, weather_stats=None):
        """
        Generates a warm insight before user starts writing.
        Priority: Gemini 1.5 Flash (API)
        Fallback: Polyglot-Ko (Local) or Default String
        """
        print(f"ğŸ” [Insight] Request received. Recent diaries count: {len(recent_diaries)}, Weather: {weather}, Stats: {weather_stats}")
        
        if not recent_diaries:
            return "ì˜¤ëŠ˜ì˜ ì²« ê¸°ë¡ì„ ì‹œì‘í•´ë³´ì„¸ìš”! ì†”ì§í•œ ë§ˆìŒì„ ë‹´ìœ¼ë©´ ë©ë‹ˆë‹¤."

        # 1. Gemini AI (Fast & Smart)
        if self.gemini_model:
            try:
                print(f"ğŸš€ [Insight] Attempting {self.gemini_model.model_name} with Privacy Shield...")
                diary_context = ""
                for d in recent_diaries:
                    sanitized_event = self._sanitize_context(d.get('event',''))
                    diary_context += f"- [{d.get('date','')}] ê¸°ë¶„:{d.get('mood','')} / ìƒí™©:{sanitized_event}\n"

                weather_info = f"ì˜¤ëŠ˜ì˜ ë‚ ì”¨: {weather}" if weather else "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ ì •ë³´ ì—†ìŒ"
                stats_info = f" (ê³¼ê±° ì´ ë‚ ì”¨ì— ë‹¹ì‹ ì€ ì£¼ë¡œ {weather_stats} ê°ì •ì„ ëŠë¼ì…¨ë„¤ìš”)" if weather_stats else ""

                prompt = (
                    "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§€ë‚œ ì¼ê¸° ê¸°ë¡ê³¼ ì˜¤ëŠ˜ì˜ ë‚ ì”¨, ê·¸ë¦¬ê³  'ê³¼ê±° ë‚ ì”¨ë³„ ê°ì • íŒ¨í„´'ì„ ë¶„ì„í•˜ì—¬ ë”°ëœ»í•œ í•œ ë¬¸ì¥ì˜ ì¡°ì–¸ì„ ê±´ë„¤ëŠ” ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\n\n"
                    f"### {weather_info}{stats_info}\n"
                    "### ì‚¬ìš©ìì˜ ìµœê·¼ 1ì£¼ì¼ íë¦„\n"
                    f"{diary_context}\n"
                    "### ì§€ì‹œì‚¬í•­\n"
                    "1. ë°˜ë“œì‹œ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
                    "2. ë¬¸ì¥ì˜ ì‹œì‘ì€ ê³¼ê±° íŒ¨í„´ì´ë‚˜ ì˜¤ëŠ˜ ë‚ ì”¨ì— ëŒ€í•œ ê³µê°ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.\n"
                    "   (ì˜ˆ: 'ë¹„ê°€ ì˜¬ ë•Œë©´ í‰ì†Œë³´ë‹¤ ì¡°ê¸ˆ ë” ìš°ìš¸í•´ì§€ê³¤ í•˜ì…¨ì£ . ì˜¤ëŠ˜ì€ ì§€ë‚œì£¼ì˜ í™œê¸°ì°¬ ê¸°ìš´ì„ ë¹Œë ¤ ì¡°ê¸ˆ ë” ì›ƒì–´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?')\n"
                    "3. ìµœê·¼ 1ì£¼ì¼ê°„ì˜ ê°ì • íë¦„ì´ ì¢‹ì€ì§€ ë‚˜ìœì§€ë¥¼ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ í•˜ì„¸ìš”.\n"
                    "4. 'ì˜¤ëŠ˜ í•˜ë£¨ ì‘ì›í•©ë‹ˆë‹¤' ê°™ì€ ë»”í•œ ë§ì€ ê¸ˆì§€ì…ë‹ˆë‹¤.\n"
                    "5. 20ì~50ì ë‚´ì™¸ë¡œ ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§(í•´ìš”ì²´)ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n\n"
                    "ìƒë‹´ì‚¬ ì¡°ì–¸(íŒ¨í„´ê³¼ ìµœê·¼ íë¦„ì´ í†µí•©ëœ í•œ ë¬¸ì¥):"
                )

                # Define a basic generation config if not already defined
                # This is added to ensure 'config' is defined, as per the user's provided 'Code Edit' snippet.
                # If 'config' is meant to be a more complex object, it should be defined elsewhere.
                # For now, a minimal config is provided to prevent NameError.
                try:
                    import google.generativeai as genai
                    config = genai.GenerationConfig(
                        temperature=0.7,
                        top_p=0.95,
                        top_k=40,
                        max_output_tokens=60,
                    )
                except ImportError:
                    print("Warning: google.generativeai not imported, cannot define GenerationConfig.")
                    config = None # Fallback if genai is not available

                print("DEBUG: Sending request to Gemini... WITH TIMEOUT 15s")
                response = self.gemini_model.generate_content(
                prompt,
                generation_config=config,
                request_options={"timeout": 15}  # Force timeout after 15s
            )
                print("DEBUG: Received response from Gemini!")
                # Handling blocked responses or empty candidates
                try:
                    if response and response.text:
                        final_response = response.text.strip()
                        # Basic validation
                        if len(final_response) >= 5:
                            print(f"âœ… [Insight] Gemini Success: {final_response}")
                            return final_response
                        else:
                            print(f"âš ï¸ [Insight] Gemini response too short (<5 chars): '{final_response}'")
                except ValueError:
                    # This happens if response.text is not available (e.g., blocked by safety)
                    print(f"ğŸš« [Insight] Gemini response blocked by safety filters or no candidates.")
                    if hasattr(response, 'prompt_feedback'):
                        print(f"   Feedback: {response.prompt_feedback}")
                    
            except Exception as e:
                print(f"âŒ [Insight] Gemini Inference Failed: {str(e)}")

        # 2. Local Model Fallback (Polyglot)
        if self.gpt_model and self.gpt_tokenizer:
            try:
                print("ğŸƒ [Insight] Falling back to Local Polyglot-Ko...")
                # Simplified fallback logic for robustness
                input_text = f"ìƒë‹´ ê¸°ë¡:\n{recent_diaries[-1].get('event','')}\nì¡°ì–¸:"
                encoded = self.gpt_tokenizer(input_text, return_tensors='pt').to(self.device)
                with torch.no_grad():
                    ids = self.gpt_model.generate(encoded.input_ids, max_length=50)
                decoded = self.gpt_tokenizer.decode(ids[0], skip_special_tokens=True)
                print(f"âœ… [Insight] Local Fallback success.")
                return decoded.split("ì¡°ì–¸:")[-1].strip()
            except Exception as e:
                print(f"âŒ [Insight] Local Fallback Failed: {str(e)}")

        # 3. Final Fallback (Return None to let Frontend handle 30s timeout)
        print("ğŸ’¡ [Insight] All AI models failed/blocked. Returning None to trigger frontend default.")
        return None

    def _rebuild_inference_models(self):
        # Seq2Seq Removed
        pass 

    def train_comment_model(self):
        # Seq2Seq Removed
        pass

    def predict(self, text):
        import time
        start_time = time.time()
        
        if not text: 
            return {"emotion": "ë¶„ì„ ë¶ˆê°€", "comment": "ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."}

        emotion_result = "ë¶„ì„ ë¶ˆê°€"
        
        # 1. Emotion Classification (LSTM or Keyword)
        if TENSORFLOW_AVAILABLE and self.model:
            try:
                tf_start = time.time()
                sequences = self.tokenizer.texts_to_sequences([text])
                padded = pad_sequences(sequences, maxlen=self.max_len)
                prediction = self.model.predict(padded, verbose=0)[0]
                predicted_class_idx = np.argmax(prediction)
                confidence = prediction[predicted_class_idx]
                predicted_label = self.classes[predicted_class_idx]
                emotion_result = f"{predicted_label} ({(confidence * 100):.1f}%)"
                print(f"â±ï¸ [Timer] TensorFlow Prediction took: {time.time() - tf_start:.3f}s")
            except Exception as e:
                print(f"Prediction error: {e}")
                emotion_result = self._fallback_predict(text)
        else:
            emotion_result = self._fallback_predict(text)
            
        # 2. Comment Generation (Priority: Gemini -> Polyglot -> Keyword)
        comment_result = ""
        
        # try Gemini First
        if self.gemini_model:
            try:
                gemini_start = time.time()
                print(f"ğŸš€ [Comment] Generating letter using Gemini...")
                comment_result = self.generate_gemini_comment(text, emotion_result)
                print(f"â±ï¸ [Timer] Gemini Comment took: {time.time() - gemini_start:.3f}s")
            except Exception as e:
                print(f"âŒ [Comment] Gemini failed: {e}")

        # Fallback to Polyglot if Gemini failed
        if not comment_result and self.gpt_model:
            try:
                comment_result = self.generate_polyglot_comment(text, emotion_result)
            except Exception as e:
                print(f"âŒ [Comment] Polyglot failed: {e}")
        
        # Final Fallback
        if not comment_result:
            comment_result = self.generate_keyword_comment(text) or "ì˜¤ëŠ˜ í•˜ë£¨ë„ ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”."
            
        print(f"âœ¨ [Timer] Total AI Analysis took: {time.time() - start_time:.3f}s")
        return {
            "emotion": emotion_result,
            "comment": comment_result
        }

    def _fallback_predict(self, text):
        # Load keywords from MongoDB
        if self.db is None:
             return "ë¶„ì„ ë¶ˆê°€"

        try:
            # Fetch all keywords from Mongo
            # This is inefficient for large datasets but ok for small keywords bank
            keywords = list(self.db.emotion_keywords.find())
            
            scores = [0] * 5
            found_any = False
            
            for kw in keywords:
                if kw['keyword'] in text:
                    scores[kw['emotion_label']] += kw['frequency']
                    found_any = True
            
            if found_any:
                max_score = max(scores)
                total_score = sum(scores)
                confidence = (max_score / total_score * 100) if total_score > 0 else 85.0
                best_idx = scores.index(max_score)
                return f"{self.classes[best_idx]} ({confidence:.1f}%)"
            else:
                return "ê·¸ì €ê·¸ë˜ (40.0%)" 
        except Exception as e:
            print(f"Fallback error: {e}")
            return "ë¶„ì„ ë¶ˆê°€"

    def load_sentiment_corpus(self):
        """
        Load 'Sentiment Dialogue Corpus' (Training & Validation).
        Use Full 60-Class Granularity (E10 ~ E69).
        """
        import json
        
        files = [
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json',
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.json'
        ]
        
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        import glob
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Use glob to avoid Unicode normalization issues (NFC vs NFD) on macOS
        files = glob.glob(os.path.join(base_dir, "*Training.json")) + glob.glob(os.path.join(base_dir, "*Validation.json"))
        
        if not files:
             print("No corpus files found via glob!")
        
        for fpath in files:
            fname = os.path.basename(fpath)
            # fpath is already absolute from glob
             
            print(f"Loading corpus: {fname}...")
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                new_texts = []
                new_labels = []
                
                for entry in data:
                    try:
                        etype = entry.get('profile', {}).get('emotion', {}).get('type', '')
                        
                        # Only use codes defined in our map (E10-E69)
                        if etype in self.code_to_idx:
                            # Extract user text
                            content = entry.get('talk', {}).get('content', {})
                            text = content.get('HS01', '')
                            system_text = content.get('SS01', '')
                            
                            if text:
                                label_idx = self.code_to_idx[etype]
                                new_texts.append(text)
                                new_labels.append(label_idx)
                                
                                # Store for comment training if pair exists
                                if system_text:
                                    self.conversation_pairs.append((text, system_text))
                                    
                    except Exception as e:
                        continue
                        
                # Add to training set
                self.train_texts.extend(new_texts)
                self.train_labels = np.concatenate((self.train_labels, np.array(new_labels)))
                
                print(f"Added {len(new_texts)} samples from {fname}")
                
            except Exception as e:
                print(f"Error loading corpus {fname}: {e}")


    def load_db_data(self):
        """
        Load data from the Diary table to fine-tune the model.
        Returns:
            X (list): Combined text (event + emotion_desc + self_talk)
            y (list): Emotion labels (0-4)
        """
        print("Loading data from Database...")
        session = self.Session()
        X = []
        y = []
        try:
            from models import Diary
            diaries = session.query(Diary).all()
            
            # Mood Level to Label Mapping
            # 5(Happy)->0, 4(Calm)->1, 3(Neutral)->2, 2(Depressed)->3, 1(Angry)->4
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            
            for d in diaries:
                if not d.mood_level: continue
                
                label = mapping.get(d.mood_level)
                if label is None: continue
                
                # Combine text fields for rich context
                text = f"{d.event} {d.emotion_desc} {d.self_talk}"
                X.append(text)
                y.append(label)
                
            print(f"Loaded {len(X)} samples from Database.")
            return X, y
            
        except Exception as e:
            print(f"Error loading DB data: {e}")
            return [], []
        finally:
            session.close()





    def train_comment_model(self):
        """
        Train a Seq2Seq model using ChatbotData.csv AND Sentiment Dialogue Corpus
        """
        if not TENSORFLOW_AVAILABLE: return

        print("Training Comment Generation Model (Seq2Seq)...")
        try:
            import os
            import pickle
            
            # 1. Load ChatbotData.csv
            base_dir = os.path.dirname(os.path.abspath(__file__))
            data_path = os.path.join(base_dir, 'ChatbotData.csv')
            questions = []
            answers = []
            
            if os.path.exists(data_path):
                df = pd.read_csv(data_path)
                # Take a sample to keep training time reasonable, or all? 
                # Let's take mixed sample.
                df = df.sample(frac=1).reset_index(drop=True)
                questions = df['Q'].astype(str).tolist()[:5000] # Cap at 5000 for speed
                # Char-level: Use \t for Start, \n for End
                answers = df['A'].apply(lambda x: '\t' + str(x) + '\n').tolist()[:5000]
            
            # 2. Add Sentiment Dialogue Corpus
            if hasattr(self, 'conversation_pairs'):
                print(f"Integrating {len(self.conversation_pairs)} pairs from Sentiment Corpus...")
                # Sample 5000 from here too to balance
                import random
                pairs = self.conversation_pairs
                if len(pairs) > 5000:
                    pairs = random.sample(pairs, 5000)
                
                for q, a in pairs:
                    questions.append(str(q))
                    answers.append('\t' + str(a) + '\n')
            
            print(f"Total Comment Training Samples: {len(questions)}")

            # Shared Tokenizer (Character Level for better Korean convergence without Mecab)
            # Filters: Don't filter \t and \n as they are our tokens!
            self.comment_tokenizer = Tokenizer(char_level=True, filters='!"#$%&()*+,-./:;<=>?@[\\]^`{|}~') 
            self.comment_tokenizer.fit_on_texts(questions + answers)
            
            vocab_size = len(self.comment_tokenizer.word_index) + 1
            print(f"Comment Vocab Size (Char-level): {vocab_size}")

            # Encoder Data
            tokenized_Q = self.comment_tokenizer.texts_to_sequences(questions)
            encoder_input_data = pad_sequences(tokenized_Q, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Data
            tokenized_A = self.comment_tokenizer.texts_to_sequences(answers)
            decoder_input_data = pad_sequences(tokenized_A, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Target (Shifted-by-one)
            decoder_target_data = np.zeros_like(decoder_input_data, dtype="float32")
            decoder_target_data[:, :-1] = decoder_input_data[:, 1:]
            decoder_target_data = np.expand_dims(decoder_target_data, -1)

            # Model Architecture
            latent_dim = 256
            
            # Encoder
            encoder_inputs = Input(shape=(None,), name='enc_input')
            enc_emb_layer = Embedding(vocab_size, latent_dim, name='enc_embedding')
            enc_emb = enc_emb_layer(encoder_inputs)
            encoder_lstm = LSTM(latent_dim, return_state=True, name='enc_lstm')
            encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
            encoder_states = [state_h, state_c]
            
            # Decoder
            decoder_inputs = Input(shape=(None,), name='dec_input')
            dec_emb_layer = Embedding(vocab_size, latent_dim, name='dec_embedding')
            dec_emb = dec_emb_layer(decoder_inputs)
            decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='dec_lstm')
            decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
            decoder_dense = Dense(vocab_size, activation='softmax', name='dec_dense')
            decoder_outputs = decoder_dense(decoder_outputs)
            
            self.comment_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
            self.comment_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
            print("Fitting Seq2Seq Model (Char-level)...")
            self.comment_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
                                   batch_size=64, epochs=15, validation_split=0.2, verbose=1)
                                   
            print("Comment Model Trained. Saving...")
            
            # Save Main Model
            self.comment_model.save(self.comment_model_path)
            
            # Save Tokenizer
            with open(self.comment_tokenizer_path, 'wb') as handle:
                pickle.dump(self.comment_tokenizer, handle)

            # Construct & Save Inference Models
            self.enc_model = Model(encoder_inputs, encoder_states)
            self.enc_model.save(os.path.join(base_dir, 'comment_enc_model.h5'))
            
            dec_state_input_h = Input(shape=(latent_dim,), name='dec_input_h')
            dec_state_input_c = Input(shape=(latent_dim,), name='dec_input_c')
            dec_states_inputs = [dec_state_input_h, dec_state_input_c]
            
            dec_emb2 = dec_emb_layer(decoder_inputs)
            dec_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states_inputs)
            dec_states2 = [state_h2, state_c2]
            dec_outputs2 = decoder_dense(dec_outputs2)
            
            self.dec_model = Model([decoder_inputs] + dec_states_inputs, [dec_outputs2] + dec_states2)
            self.dec_model.save(os.path.join(base_dir, 'comment_dec_model.h5'))
            
            print("Inference models saved.")
            
        except Exception as e:
            print(f"Error training comment model: {e}")

    def _rebuild_inference_models(self):
        """Rebuild inference models from loaded main model or load separate files"""
        try:
            from tensorflow.keras.models import load_model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            enc_path = os.path.join(base_dir, 'comment_enc_model.h5')
            dec_path = os.path.join(base_dir, 'comment_dec_model.h5')
            
            if os.path.exists(enc_path) and os.path.exists(dec_path):
                self.enc_model = load_model(enc_path)
                self.dec_model = load_model(dec_path)
                print("Inference models loaded successfully.")
            else:
                print("Inference model files missing. Comment generation may fail.")
        except Exception as e:
            print(f"Error loading inference models: {e}")

    # Seq2Seq Generation helpers removed

    def analyze_diary_with_gemini(self, text):
        """
        [Ultra Fast Mode] Uses Gemini for both Classification and Comment Generation.
        Uses Direct HTTP (REST) to prevent SDK hanging issues on OCI.
        """
        import requests
        import json
        
        if not self.gemini_model:
            return None, None
            
        print(f"ğŸš€ [HTTP Analysis] Requesting Gemini for All-in-One analysis...", end=" ", flush=True)

        try:
            # Construct REST API URL manually
            api_key = Config.GEMINI_API_KEY
            # Use 'gemini-1.5-flash-latest' to match available models
            model_name = "gemini-1.5-flash-latest" 
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
            
            headers = {'Content-Type': 'application/json'}
            
            prompt_text = (
                f"ë‹¤ìŒ ì¼ê¸°ë¥¼ ì½ê³  ì‚¬ìš©ìì˜ ê°ì •ì„ ë¶„ì„í•˜ê³ , ì§§ì€ ê³µê° ì½”ë©˜íŠ¸ë¥¼ í•´ì¤˜.\n\n"
                f"ì¼ê¸°:\n{text}\n\n"
                f"ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ JSON ì¤€ìˆ˜):\n"
                f"{{\n"
                f"  \"emotion\": \"happy\" | \"sad\" | \"angry\" | \"neutral\" | \"panic\",\n"
                f"  \"comment\": \"ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” í•œêµ­ì–´ í•œ ë§ˆë”” (ë°˜ë§ ê¸ˆì§€)\"\n"
                f"}}\n"
                f"ì£¼ì˜: ê°ì •ì€ ìœ„ 5ê°œ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ. json ì½”ë“œë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥."
            )
            
            payload = {
                "contents": [{
                    "parts": [{"text": prompt_text}]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 200
                }
            }
            
            # Send Request with STRICT 10s Timeout
            print("Requesting...", end=" ")
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            
            if response.status_code != 200:
                print(f"âŒ API Error {response.status_code}: {response.text}")
                return None, None
                
            # Parse Response
            result = response.json()
            try:
                # Extract text from complex JSON structure
                content_text = result['candidates'][0]['content']['parts'][0]['text']
                
                # Clean up Markdown JSON if present
                clean_json = content_text.strip().replace("```json", "").replace("```", "")
                data = json.loads(clean_json)
                
                emotion_str = data.get('emotion', 'neutral').lower()
                comment = data.get('comment', 'ì˜¤ëŠ˜ í•˜ë£¨ë„ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”.')
                
                # Map emotion string to code
                code_map = {
                    "happy": 1, "joy": 1, 
                    "sad": 2, "depressed": 2,
                    "neutral": 3, "calm": 3, "soso": 3,
                    "angry": 4, "annoyed": 4,
                    "panic": 5, "anxious": 5, "fear": 5
                }
                emotion_code = code_map.get(emotion_str, 3)
                
                print("Done!")
                return str(emotion_code), comment
                
            except (KeyError, IndexError, json.JSONDecodeError) as e:
                print(f"âš ï¸ Parse Failed: {e}")
                return None, None
                
        except Exception as e:
            print(f"âŒ HTTP/Network Error: {e}")
            return None, None

    def generate_comment(self, prediction_text, user_text=None):
        """
        Generate a supportive comment.
        Priority: 1. Keyword Bank (Safety Net) 2. AI Generation (Seq2Seq) 3. Fallback
        """
        # If we have user_text and gemini, try fast path
        if user_text and self.gemini_model:
             _, comment = self.analyze_diary_with_gemini(user_text)
             if comment: return comment

        # 1. Phase 1: Keyword Safety Net (Highest Priority)
        if user_text:
            keyword_comment = self.generate_keyword_comment(user_text)
            if keyword_comment:
                 return keyword_comment

        if not prediction_text or "ë¶„ì„ ë¶ˆê°€" in prediction_text:
            return "ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“¤ë ¤ì£¼ì„¸ìš”. í•­ìƒ ë“£ê³  ìˆì„ê²Œìš”."

        # Extract strict label (remove confidence score)
        # e.g. "ë¶„ë…¸ (ë°°ì‹ ê°) (85.0%)" -> "ë¶„ë…¸ (ë°°ì‹ ê°)"
        try:
             emotion_label_only = prediction_text.rsplit(' (', 1)[0]
        except:
             emotion_label_only = prediction_text.split()[0]

        # 2. Phase 2: KoGPT-2 Generation (High Priority for Context)
        if user_text:
            # Pass the full specific label to KoGPT for context-aware generation
            gpt_comment = self.generate_kogpt2_comment(user_text, emotion_label_only)
            if gpt_comment:
                return f"{gpt_comment}"

        # 3. Phase 1.5: Label-based Specific Advice (Fallback)
        label_comment = self.generate_label_comment(emotion_label_only)
        if label_comment:
             return label_comment

        try:
            label = prediction_text.split()[0] # e.g. "í–‰ë³µí•´"
            
            # 4. Fallback
            return "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”."
            
        except Exception as e:
            print(f"Comment Gen Error: {e}")
            return "ë‹¹ì‹ ì˜ ë§ˆìŒì„ ì´í•´í•´ìš”."


    def update_keywords(self, text, mood_level):
        """
        Learn new keywords from the text based on the user's provided mood_level.
        mood_level: 1-5 (User input)
        Mapping: 5->0(Happy), 4->1(Calm), 3->2(Neutral), 2->3(Depressed), 1->4(Angry)
        """
        if not text: return

        # Map mood_level to label
        # mood_level is expected to be int
        try:
            mood_val = int(mood_level)
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            target_label = mapping.get(mood_val)
            
            if target_label is None:
                return # Invalid mood level
        except:
            return

        session = self.Session()
        try:
            from models import EmotionKeyword
            
            # Simple Tokenization (Space-based)
            # In a real KR app, use Mecab/Konlpy. Here we split by space.
            words = text.split()
            
            for w in words:
                # Basic cleaning
                w = w.strip('.,?!~"\'')
                if len(w) < 2: continue # Skip single chars
                
                # Check if exists
                existing = session.query(EmotionKeyword).filter_by(keyword=w).first()
                
                if existing:
                    # If exists, increment frequency if label matches
                    # If label differs, maybe decrease freq or ignore? 
                    # Let's just track co-occurrence. Complex logic omitted for simplicity.
                    if existing.emotion_label == target_label:
                        existing.frequency += 1
                else:
                    # NEW WORD -> LEARN IT!
                    print(f"Learning new keyword: {w} -> {self.classes[target_label]}")
                    new_kw = EmotionKeyword(
                        keyword=w,
                        emotion_label=target_label,
                        frequency=1
                    )
                    session.add(new_kw)
            
            session.commit()
            
            # If Model is active, we might want to update tokenizer or retrain eventually.
            # Rerunning Tokenizer.fit_on_texts would be needed. 
            # For this session, we won't retrain the LSTM live (too slow), but the Fallback logic will immediately benefit.
            
        except Exception as e:
            print(f"Learning error: {e}")
            session.rollback()
        finally:
            session.close()

