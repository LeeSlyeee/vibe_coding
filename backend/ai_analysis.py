import numpy as np
import os
import random
import random
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from config import Config
import json
TRAINING_STATE_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'training_state.json')
try:
    from models import EmotionKeyword
    from emotion_codes import EMOTION_CODE_MAP
except ImportError:
    # Handle possible circular import dynamically if needed, or rely on caller context
    pass

# Try to import TensorFlow/Keras, but provide fallback if not installed
try:
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
    from tensorflow.keras.utils import to_categorical
    from tensorflow.keras.utils import to_categorical
    import pandas as pd
    TENSORFLOW_AVAILABLE = True
    
    # KoGPT-2 Imports
    from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
    import torch
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"Warning: AI Library not found. Error: {e}")
    print("Using simple keyword-based sentiment analysis.")

class EmotionAnalysis:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.max_len = 50
        self.vocab_size = 0
        
        # 60 Ultra-Fine-Grained Emotion Classes
        # Sort codes to ensure consistent indexing (E10, E11, ... E69)
        self.sorted_codes = sorted(EMOTION_CODE_MAP.keys())
        self.classes = [EMOTION_CODE_MAP[code] for code in self.sorted_codes]
        self.code_to_idx = {code: i for i, code in enumerate(self.sorted_codes)}
        
        # We will load keywords from DB for fallback/learning
        self.db_engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)
        self.Session = sessionmaker(bind=self.db_engine)

        # Initial Training Data
        # Replaced hardcoded 5-class data with empty lists. 
        # We rely 100% on the 60-class Sentiment Dialogue Corpus.
        self.train_texts = []
        self.train_labels = np.array([], dtype=int)

        # Fallback Comment Bank
        self.comment_bank = {
            "ÌñâÎ≥µÌï¥": [
                "Ïò§Îäò ÌïòÎ£® Ï†ïÎßê ÌñâÎ≥µÌïòÏÖ®Íµ∞Ïöî! Ïù¥ Í∏çÏ†ïÏ†ÅÏù∏ ÏóêÎÑàÏßÄÍ∞Ä ÎÇ¥ÏùºÎèÑ Ïù¥Ïñ¥ÏßÄÍ∏∏ Î∞îÎûÑÍ≤åÏöî. üòä",
                "Îì£Í∏∞Îßå Ìï¥ÎèÑ Í∏∞Î∂ÑÏù¥ Ï¢ãÏïÑÏßÄÎäî Ïù¥ÏïºÍ∏∞ÎÑ§Ïöî! ÌñâÎ≥µÌïú ÏàúÍ∞ÑÏùÑ Ïò§Îûò Í∞ÑÏßÅÌïòÏÑ∏Ïöî.",
                "ÏõÉÏùåÏù¥ Í∞ÄÎìùÌïú ÌïòÎ£®ÏòÄÎÑ§Ïöî. ÎÇ¥ÏùºÎèÑ Ïù¥Î†áÍ≤å ÏõÉÏùÑ ÏùºÏù¥ ÎßéÏïòÏúºÎ©¥ Ï¢ãÍ≤†Ïñ¥Ïöî!",
                "Ï†ïÎßê Î©ãÏßÑ ÌïòÎ£®ÏòÄÍµ∞Ïöî! Ïä§Ïä§Î°úÏóêÍ≤å Ïπ≠Ï∞¨ ÌïúÎßàÎîî Ìï¥Ï£ºÏÑ∏Ïöî. üëç",
                "ÌñâÎ≥µÏùÄ Ï†ÑÏóºÎêúÎã§Í≥† ÌïòÏ£†. ÎãπÏã†Ïùò ÌñâÎ≥µÏù¥ Ï£ºÎ≥ÄÍπåÏßÄ Î∞ùÍ≤å ÎπÑÏ∂ú Í±∞ÏòàÏöî.",
                "Í∏∞Î∂Ñ Ï¢ãÏùÄ ÏóêÎÑàÏßÄÍ∞Ä Í∞ÄÎìùÌïòÎÑ§Ïöî! ÎßõÏûàÎäî Í±∞ ÎìúÏãúÎ©¥ÏÑú Ïò§ÎäòÏùÑ Í∏∞ÎÖêÌï¥Î≥¥ÏÑ∏Ïöî.",
                "ÏµúÍ≥†Ïùò ÌïòÎ£®Î•º Î≥¥ÎÇ¥ÏÖ®ÎÑ§Ïöî! Ïû†Îì§Í∏∞ Ï†Ñ ÌñâÎ≥µÌñàÎçò ÏàúÍ∞ÑÏùÑ Îã§Ïãú Îñ†Ïò¨Î†§Î≥¥ÏÑ∏Ïöî.",
                "Ïò§ÎäòÏùò Ï¶êÍ±∞ÏõÄÏù¥ ÎßàÏùåÏÜçÏóê Ïò§ÎûòÏò§Îûò ÎÇ®Í∏∞Î•º Î∞îÎùºÏöî. üíñ",
                "ÏÑ∏ÏÉÅÏù¥ ÎãπÏã†ÏùÑ Ï∂ïÎ≥µÌïòÎäî ÎÇ†Ïù¥ÏóàÎÇò Î¥êÏöî! Ï†ïÎßê Í∏∞ÏÅú ÏÜåÏãùÏù¥ÏóêÏöî.",
                "ÌñâÎ≥µÌïú ÎãπÏã†Ïùò Î™®ÏäµÏùÑ Î≥¥Îãà Ï†ÄÎèÑ Í∏∞Î∂ÑÏù¥ Ï¢ãÏïÑÏßëÎãàÎã§! ÌååÏù¥ÌåÖ!"
            ],
            "ÌèâÏò®Ìï¥": [
                "ÎßàÏùåÏù¥ Ìé∏ÏïàÌïòÎã§Îãà Îã§ÌñâÏù¥ÏóêÏöî. Îî∞ÎúªÌïú Ï∞® Ìïú ÏûîÏúºÎ°ú ÌïòÎ£®Î•º ÎßàÎ¨¥Î¶¨Ìï¥Î≥¥Îäî Í±¥ Ïñ¥Îñ®ÍπåÏöî? üçµ",
                "ÏûîÏûîÌïú Ìò∏Ïàò Í∞ôÏùÄ ÎßàÏùåÏù¥ÎÑ§Ïöî. Ïù¥ ÌèâÌôîÎ°úÏõÄÏù¥ Í≥ÑÏÜçÎêòÍ∏∏ Î∞îÎùºÏöî.",
                "Ïó¨Ïú†Î°úÏö¥ ÌïòÎ£®Î•º Î≥¥ÎÇ¥ÏÖ®Íµ∞Ïöî. Î≥µÏû°Ìïú ÏÉùÍ∞ÅÏùÄ Ïû†Ïãú ÎÇ¥Î†§ÎÜìÍ≥† Ïâ¨Ïñ¥Í∞ÄÏÑ∏Ïöî.",
                "ÌèâÎ≤îÌïòÏßÄÎßå ÏÜåÏ§ëÌïú ÌèâÏò®Ìï®Ïù¥ÎÑ§Ïöî. Ï¢ãÏïÑÌïòÎäî ÏùåÏïÖÏùÑ Îì§ÏúºÎ©∞ ÌûêÎßÅÌï¥Î¥êÏöî. üéµ",
                "ÎßàÏùåÏùò ÏâºÌëúÍ∞Ä ÌïÑÏöîÌïú ÏàúÍ∞Ñ, Îî± Ï†ÅÏ†àÌïú Ìú¥ÏãùÏùÑ Ï∑®ÌïòÏã† Í≤É Í∞ôÏïÑÏöî.",
                "ÌèâÌôîÎ°úÏö¥ ÎßàÏùåÏúºÎ°ú Ïû†ÏûêÎ¶¨Ïóê Îì§ Ïàò ÏûàÍ≤†ÎÑ§Ïöî. Ï¢ãÏùÄ Íøà Íæ∏ÏÑ∏Ïöî. üåô",
                "Ï°∞Ïö©Ìïú ÌñâÎ≥µÏù¥ ÍπÉÎì† ÌïòÎ£®ÏòÄÎÑ§Ïöî. Ïù¥Îü∞ ÎÇ†Îì§Ïù¥ ÏåìÏó¨ ÏÇ∂ÏùÑ Îã®Îã®ÌïòÍ≤å ÎßåÎì§Ïñ¥Ïöî.",
                "ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌùêÎ¶ÑÏóê Î™∏ÏùÑ Îß°Í∏¥ ÎãπÏã†, Ï∞∏ Ìé∏ÏïàÌï¥ Î≥¥Ïó¨Ïöî.",
                "Í∏¥Ïû•Ïù¥ ÌíÄÎ¶¨Í≥† ÎßàÏùåÏù¥ ÎÜìÏù¥Îäî Í∏∞Î∂Ñ, Ï†ïÎßê ÏÜåÏ§ëÌïòÏ£†.",
                "Ïò§ÎäòÏùò ÌèâÏò®Ìï®Ïù¥ ÎÇ¥ÏùºÏùÑ ÏÇ¥ÏïÑÍ∞à ÌûòÏù¥ ÎêòÏñ¥Ï§Ñ Í±∞ÏòàÏöî."
            ],
            "Í∑∏Ï†ÄÍ∑∏Îûò": [
                "ÌèâÎ≤îÌïú ÌïòÎ£®ÏòÄÍµ∞Ïöî. ÎÇ¥ÏùºÏùÄ Ï¢Ä Îçî ÌäπÎ≥ÑÌïú ÏùºÏù¥ ÏÉùÍ∏∏ÏßÄÎèÑ Î™∞ÎùºÏöî! ÌååÏù¥ÌåÖ üí™",
                "Î≥ÑÏùº ÏóÜÎäî ÌïòÎ£®ÎèÑ ÏÜåÏ§ëÌïòÏ£†. Î¨¥ÌÉàÌïòÍ≤å Î≥¥ÎÇ∏ Í≤ÉÏóê Í∞êÏÇ¨Ìï¥Î¥êÏöî.",
                "ÎïåÎ°úÎäî ÏûîÏûîÌïú ÌïòÎ£®Í∞Ä Í∞ÄÏû• ÌÅ∞ Ìú¥ÏãùÏù¥ ÎêòÍ∏∞ÎèÑ ÌïúÎãµÎãàÎã§.",
                "Ïã¨Ïã¨Ìïú ÎÇ†Ïù¥ÏóàÎã§Î©¥, ÎÇ¥ÏùºÏùÄ ÏûëÏùÄ Î™®ÌóòÏùÑ Í≥ÑÌöçÌï¥Î≥¥Îäî Í±¥ Ïñ¥Îñ®ÍπåÏöî?",
                "Í∑∏Ï†Ä Í∑∏Îü∞ ÎÇ†ÎèÑ ÏßÄÎÇòÍ≥† Î≥¥Î©¥ Ï∂îÏñµÏù¥ Îê† Í±∞ÏòàÏöî. Ìé∏ÏïàÌïú Î∞§ Î≥¥ÎÇ¥ÏÑ∏Ïöî.",
                "ÌäπÎ≥ÑÌïú ÏùºÏùÄ ÏóÜÏóàÏßÄÎßå, ÎãπÏã†ÏùÄ Ïò§ÎäòÎèÑ ÎãπÏã†Ïùò ÏûêÎ¶¨Î•º Ïûò ÏßÄÏº∞Ïñ¥Ïöî.",
                "Î¨¥ÎÇúÌïú ÌïòÎ£®ÏòÄÎÑ§Ïöî. ÎÇ¥ÏùºÏùÄ Ï¢ãÏïÑÌïòÎäî Í∞ÑÏãùÏùÑ Î®πÏúºÎ©∞ Í∏∞Î∂ÑÏùÑ Ï†ÑÌôòÌï¥Î≥ºÍπåÏöî?",
                "Ïò§ÎäòÏùÄ Ïû†Ïãú Ïâ¨Ïñ¥Í∞ÄÎäî ÌéòÏù¥ÏßÄÎùºÍ≥† ÏÉùÍ∞ÅÌï¥Ïöî. ÎÇ¥ÏùºÏùÄ Îòê Îã§Î•∏ Ïù¥ÏïºÍ∏∞Í∞Ä Ïì∞Ïùº Í±∞ÏòàÏöî.",
                "Í∞êÏ†ïÏùò Í∏∞Î≥µ ÏóÜÏù¥ ÌèâÌÉÑÌïú ÌïòÎ£®, Í∑∏Í≤ÉÎßåÏúºÎ°úÎèÑ Ï∂©Î∂ÑÌûà Í¥úÏ∞ÆÏïÑÏöî.",
                "ÎÇ¥ÏùºÏùÄ ÏòàÏÉÅÏπò Î™ªÌïú Ï¶êÍ±∞ÏõÄÏù¥ Í∏∞Îã§Î¶¨Í≥† ÏûàÏùÑÏßÄÎèÑ Î™∞ÎùºÏöî!"
            ],
            "Ïö∞Ïö∏Ìï¥": [
                "ÎßéÏù¥ ÌûòÎìúÏÖ®Íµ∞Ïöî. Ïò§Îäò ÌïòÎ£®Îäî Ìëπ Ïâ¨Î©¥ÏÑú ÏûêÏã†ÏùÑ ÌÜ†Îã•Ïó¨Ï£ºÏÑ∏Ïöî. ÎãπÏã†ÏùÄ ÏÜåÏ§ëÌïú ÏÇ¨ÎûåÏûÖÎãàÎã§. üíô",
                "ÎßàÏùåÏù¥ Î¨¥Í±∞Ïö¥ ÎÇ†Ïù¥ÎÑ§Ïöî. Ïö∏Í≥† Ïã∂Îã§Î©¥ Ïã§Ïª∑ Ïö∏Ïñ¥ÎèÑ Í¥úÏ∞ÆÏïÑÏöî. Ï†úÍ∞Ä Í≥ÅÏóê ÏûàÏùÑÍ≤åÏöî.",
                "Í¥úÏ∞ÆÏßÄ ÏïäÏïÑÎèÑ Í¥úÏ∞ÆÏïÑÏöî. Ïò§ÎäòÏùÄ Î¨¥Î¶¨ÌïòÏßÄ ÎßêÍ≥† ÏûêÍ∏∞ ÏûêÏã†Îßå ÏÉùÍ∞ÅÌïòÏÑ∏Ïöî.",
                "ÎãπÏã†Ïùò Ïä¨ÌîîÏù¥ ÍπäÏùÄ ÎßåÌÅº, ÎãπÏã†ÏùÄ Îî∞ÎúªÌïú ÎßàÏùåÏùÑ Í∞ÄÏßÑ ÏÇ¨ÎûåÏùº Í±∞ÏòàÏöî.",
                "Ïñ¥ÎëêÏö¥ Î∞§Ïù¥ ÏßÄÎÇòÎ©¥ Î∞òÎìúÏãú Ìï¥Í∞Ä ÎúπÎãàÎã§. Ïû†Ïãú ÏõÖÌÅ¨Î†§ ÏûàÏñ¥ÎèÑ Í¥úÏ∞ÆÏïÑÏöî.",
                "ÌûòÎì† ÌïòÎ£®Î•º Î≤ÑÌÖ®ÎÇ∏ ÎãπÏã†, Ï†ïÎßê Í≥†ÏÉù ÎßéÏïòÏñ¥Ïöî. Îî∞ÎúªÌïú Ïù¥Î∂à ÏÜçÏóêÏÑú Ìëπ Ï£ºÎ¨¥ÏÑ∏Ïöî.",
                "ÎßàÏùåÏùò ÎπÑÍ∞Ä Í∑∏ÏπòÍ∏∞Î•º Í∏∞Îã§Î¶¥Í≤åÏöî. ÌòºÏûêÎùºÍ≥† ÏÉùÍ∞ÅÌïòÏßÄ ÎßàÏÑ∏Ïöî.",
                "ÏßÄÍ∏à ÎäêÎÅºÎäî Í∞êÏ†ïÎèÑ ÎãπÏã†Ïùò ÏùºÎ∂ÄÏòàÏöî. Î∂ÄÏ†ïÌïòÏßÄ ÎßêÍ≥† Í∞ÄÎßåÌûà ÏïàÏïÑÏ£ºÏÑ∏Ïöî.",
                "ÎßõÏûàÎäî Í±∞ÎùºÎèÑ Î®πÍ≥† Í∏∞Ïö¥ Ï∞®Î¶¨ÏÖ®ÏúºÎ©¥ Ï¢ãÍ≤†Ïñ¥Ïöî. ÎÇ¥ÏùºÏùÄ Ï°∞Í∏à Îçî ÎÇòÏïÑÏßà Í±∞ÏòàÏöî.",
                "ÎãπÏã†ÏùÄ ÌòºÏûêÍ∞Ä ÏïÑÎãàÏóêÏöî. ÌûòÎì† ÏàúÍ∞ÑÏù¥ ÏßÄÎÇòÍ∞ÄÍ∏∏ Ìï®Íªò ÏùëÏõêÌï†Í≤åÏöî."
            ],
            "ÌôîÍ∞ÄÎÇò": [
                "ÏÜçÏÉÅÌïú ÏùºÏù¥ ÏûàÏúºÏÖ®ÎÇò Î¥êÏöî. Ïû†Ïãú Ïã¨Ìò∏Ìù°ÏùÑ ÌïòÎ©∞ ÎßàÏùåÏùÑ Í∞ÄÎùºÏïâÌòÄÎ≥¥Î©¥ Ïñ¥Îñ®ÍπåÏöî? ÌûòÎÇ¥ÏÑ∏Ïöî! üî•",
                "Ï†ïÎßê ÌôîÍ∞Ä ÎÇ† ÎßåÌïú ÏÉÅÌô©Ïù¥ÏóàÍµ∞Ïöî. Í∑∏ Í∞êÏ†ïÏùÑ ÏñµÎàÑÎ•¥ÏßÄ ÎßêÍ≥† Í±¥Ï†ÑÌïòÍ≤å ÌíÄÏñ¥Î≥¥ÏÑ∏Ïöî.",
                "Ïó¥Î∞õÎäî ÌïòÎ£®ÏòÄÎÑ§Ïöî! ÏãúÏõêÌïú Î¨º Ìïú Ïûî ÎßàÏãúÍ≥† ÌÑ∏Ïñ¥Î≤ÑÎ¶¨ÏÑ∏Ïöî.",
                "ÎàÑÍµ¨ÎùºÎèÑ ÌôîÍ∞Ä ÎÇ¨ÏùÑ Í±∞ÏòàÏöî. ÎãπÏã† ÏûòÎ™ªÏù¥ ÏïÑÎãàÎãà ÎÑàÎ¨¥ ÏûêÏ±ÖÌïòÏßÄ ÎßàÏÑ∏Ïöî.",
                "Î∂ÑÎÖ∏Îäî ÏóêÎÑàÏßÄÍ∞Ä Îê† ÏàòÎèÑ ÏûàÏñ¥Ïöî. Ïö¥ÎèôÏù¥ÎÇò Ï∑®ÎØ∏Î°ú Ïä§Ìä∏Î†àÏä§Î•º ÎÇ†Î†§Î≤ÑÎ†§Ïöî! ü•ä",
                "ÎßéÏù¥ ÏñµÏö∏ÌïòÏÖ®Ï£†. ÎãπÏã†Ïùò ÎßàÏùå Îã§ Ïù¥Ìï¥Ìï¥Ïöî.",
                "ÌôîÍ∞Ä ÎÇ† ÎïåÎäî Ïû†Ïãú Í∑∏ ÏÉÅÌô©ÏóêÏÑú Î≤óÏñ¥ÎÇò ÌôòÍ∏∞Î•º ÏãúÌÇ§Îäî Í≤å ÎèÑÏõÄÏù¥ ÎèºÏöî.",
                "Ïò§ÎäòÏùò ÎÇòÏÅú Í∏∞Î∂ÑÏùÄ Ïò§ÎäòÎ°ú ÎÅùÎÇ¥Î≤ÑÎ†§Ïöî. ÎÇ¥ÏùºÏùÄ Í∏∞Î∂Ñ Ï¢ãÏùÄ ÏùºÎßå ÏûàÏùÑ Í±∞ÏòàÏöî.",
                "ÏÜåÎ¶¨ÎùºÎèÑ Ìïú Î≤à ÌÅ¨Í≤å ÏßÄÎ•¥Í≥† Ïã∂ÎÑ§Ïöî! ÎãµÎãµÌïú ÎßàÏùåÏù¥ Ï°∞Í∏àÏùÄ ÌíÄÎ¶¨Í∏∏ Î∞îÎùºÏöî.",
                "ÎãπÏã†Ïùò ÌèâÌôîÎ•º Î∞©Ìï¥Ìïú Í≤ÉÎì§Ïù¥ Î∞âÎÑ§Ïöî. Ïò§ÎäòÏùÄ ÏùºÏ∞ç Ïâ¨Î©¥ÏÑú ÎßàÏùåÏùÑ Îã§Ïä§Î†§Î¥êÏöî.",
                # New Additions
                "Ï†ïÎßê ÌôîÍ∞Ä ÎÇòÏãúÍ≤†Ïñ¥Ïöî. ÏñµÏö∏Ìïú ÎßàÏùå Ïù¥Ìï¥Ìï¥Ïöî.",
                "Í∑∏Îü∞ ÏùºÏù¥ ÏûàÏóàÎã§Îãà Ï†ÄÎèÑ ÌôîÍ∞Ä ÎÇòÎÑ§Ïöî.",
                "ÏÜçÏÉÅÌïú ÎßàÏùåÏùÑ Ïñ¥ÎñªÍ≤å Îã¨ÎûòÎ©¥ Ï¢ãÏùÑÍπåÏöî? Ïû†Ïãú Ïâ¨Ïñ¥Í∞ÄÎäî Í±¥ Ïñ¥Îñ®ÍπåÏöî.",
                "Ï∞∏ÏßÄ ÎßêÍ≥† ÌôîÎ•º ÌëúÏ∂úÌïòÎäî Í≤ÉÎèÑ Î∞©Î≤ïÏù¥ÏóêÏöî. Í±¥Í∞ïÌïòÍ≤å Ìï¥ÏÜåÌï¥Î¥êÏöî.",
                "Ïä§Ìä∏Î†àÏä§Î•º Î∞õÏùÑ ÎßåÌïú ÏÉÅÌô©Ïù¥Íµ∞Ïöî. ÎÑàÎ¨¥ Î¨¥Î¶¨ÌïòÏßÄ ÎßàÏÑ∏Ïöî.",
                "ÎßàÏùåÏùÑ Í∞ÄÎùºÏïâÌûàÍ≥† Ï≤úÏ≤úÌûà ÏÉùÍ∞ÅÌï¥Î≥¥ÏÑ∏Ïöî. ÎãπÏã†ÏùÄ Ìï† Ïàò ÏûàÏñ¥Ïöî.",
                "Î∂ÄÎãπÌïú ÏùºÏùÑ ÎãπÌïòÏÖîÏÑú ÎßéÏù¥ ÏÜçÏÉÅÌïòÏãúÍ≤†Ïñ¥Ïöî. ÌûòÎÇ¥ÏÑ∏Ïöî.",
                "Í∑∏ ÏÇ¨Îûå ÎïåÎ¨∏Ïóê ÎãπÏã†Ïùò ÏÜåÏ§ëÌïú Í∏∞Î∂ÑÏùÑ ÎßùÏπòÏßÄ ÎßàÏÑ∏Ïöî.",
                "ÌôîÎÇòÎäî Í∞êÏ†ïÏùÄ ÎãπÏó∞Ìïú Î∞òÏùëÏù¥ÏóêÏöî. Ïä§Ïä§Î°úÎ•º Îã§ÎèÖÏó¨Ï£ºÏÑ∏Ïöî.",
                "ÏßÄÍ∏àÏùÄ ÎßàÏùåÏùÑ ÏßÑÏ†ïÏãúÌÇ§Îäî Í≤å Ïö∞ÏÑ†Ïù∏ Í≤É Í∞ôÏïÑÏöî. Îî∞ÎúªÌïú Ï∞® Ìïú Ïûî Ïñ¥ÎïåÏöî?"
            ]
        }
        
        # Extend other categories with new comments
        self.comment_bank["ÌñâÎ≥µÌï¥"].extend([
            "Ï†ïÎßê ÌñâÎ≥µÌïòÏã§ Í≤É Í∞ôÏïÑÏöî. Ï†ÄÎèÑ Îç©Îã¨ÏïÑ Í∏∞Î∂ÑÏù¥ Ï¢ãÏïÑÏßÄÎÑ§Ïöî!",
            "Í∏çÏ†ïÏ†ÅÏù∏ ÎßàÏùåÍ∞ÄÏßêÏù¥ Ï∞∏ Ï¢ãÏïÑ Î≥¥Ïó¨Ïöî. Î©ãÏßÄÏã≠ÎãàÎã§.",
            "ÎÖ∏Î†•Ïùò Í≤∞Í≥ºÍ∞Ä Ï¢ãÏïÑÏÑú Îã§ÌñâÏù¥ÏóêÏöî. Ï∂ïÌïòÎìúÎ†§Ïöî!",
            "Ï¶êÍ±∞Ïö¥ ÏãúÍ∞ÑÏùÑ Î≥¥ÎÇ¥ÏÖ®Íµ∞Ïöî. Í∑∏ Í∏∞Î∂Ñ Ïò§Îûò Í∞ÑÏßÅÌïòÏÑ∏Ïöî.",
            "Í∏∞Î∂Ñ Ï¢ãÏùÄ ÏùºÏù¥ ÏûàÏúºÏÖ®ÎÇò Î¥êÏöî. ÌñâÎ≥µÌïú ÏóêÎÑàÏßÄ Í∞êÏÇ¨Ìï©ÎãàÎã§.",
            "Ï∂ïÌïòÌï¥Ïöî! ÏïûÏúºÎ°úÎèÑ Ï¢ãÏùÄ ÏùºÎßå Í∞ÄÎìùÌïòÍ∏∏ Î∞îÎùºÏöî.",
            "ÏÑ±Ï∑®Í∞êÏùÑ ÎäêÎÅºÏÖ®Îã§Îãà Î©ãÏ†∏Ïöî. ÎãπÏã†Ïù¥ ÏûêÎûëÏä§Îü¨ÏõåÏöî.",
            "ÌñâÎ≥µÌïú ÌïòÎ£®Î•º Î≥¥ÎÇ¥Ïã† Í≤É Í∞ôÏïÑ Ï†ÄÎèÑ Í∏∞ÏÅòÎÑ§Ïöî.",
            "ÏõêÌïòÏãúÎçò ÏùºÏù¥ Ïù¥Î£®Ïñ¥Ï†∏ÏÑú Îã§ÌñâÏù¥ÏóêÏöî. Í≥†ÏÉù ÎßéÏúºÏÖ®Ïñ¥Ïöî.",
            "ÏõÉÏùåÏù¥ ÎÅäÏù¥ÏßÄ ÏïäÎäî ÌïòÎ£®Í∞Ä ÎêòÍ∏∏ ÏùëÏõêÌï†Í≤åÏöî!"
        ])
        
        self.comment_bank["ÌèâÏò®Ìï¥"].extend([
            "ÎßàÏùåÏù¥ Ìé∏ÏïàÌïòÏãúÎã§Îãà Îã§ÌñâÏù¥ÏóêÏöî. Í∑∏ ÌèâÏò®Ìï®Ïù¥ ÏßÄÏÜçÎêòÍ∏∏.",
            "Í±±Ï†ï ÏóÜÏù¥ Ìëπ Ïâ¨ÏãúÎäî Í≤ÉÎèÑ Ï§ëÏöîÌïòÏ£†. ÌûêÎßÅÌïòÎäî ÏãúÍ∞Ñ ÎêòÏÑ∏Ïöî.",
            "ÏÇ∞Ï±ÖÏùÑ ÌïòÎ©∞ Ïó¨Ïú†Î•º Ï¶êÍ∏∞ÏÖ®Íµ∞Ïöî. ÏûêÏó∞ ÏÜçÏóêÏÑú ÏπòÏú†Î∞õÏúºÏÖ®Í∏∏.",
            "Ìé∏ÏïàÌïú ÏãúÍ∞ÑÏùÑ Î≥¥ÎÇ¥Í≥† Í≥ÑÏãúÎÑ§Ïöî. Î∂ÄÎü¨ÏõåÏöî!",
            "ÏïàÏ†ïÎêú ÎßàÏùåÏù¥ Í≥ÑÏÜçÎêòÍ∏∏ Î∞îÎùºÏöî. Ïò§Îäò ÌïòÎ£®ÎèÑ ÏàòÍ≥†ÌñàÏñ¥Ïöî.",
            "Ï∞® Ìïú ÏûîÌïòÎ©∞ Ïâ¨Îäî ÏãúÍ∞ÑÏùÄ Ï†ïÎßê ÏÜåÏ§ëÌïòÏ£†. Îî∞ÎúªÌïú ÏãúÍ∞Ñ ÎêòÏÑ∏Ïöî.",
            "ÏïÑÎ¨¥Îü∞ Í∑ºÏã¨ ÏóÜÏù¥ ÌèâÌôîÎ°úÏö¥ ÏÉÅÌÉúÏãúÍµ∞Ïöî. Ï∞∏ Î≥¥Í∏∞ Ï¢ãÏïÑÏöî.",
            "Í≥†ÏöîÌïú ÏãúÍ∞ÑÏùÑ Ï¶êÍ∏∞Îäî Í≤ÉÎèÑ ÌûêÎßÅÏù¥ ÎêòÏ£†. Ïò§Î°ØÏù¥ ÎÇòÏóêÍ≤å ÏßëÏ§ëÌï¥Î¥êÏöî.",
            "Ïò§Îäò ÌïòÎ£®Í∞Ä ÌèâÏò®ÌïòÍ≤å ÎßàÎ¨¥Î¶¨ÎêòÍ∏∏ Î∞îÎùºÏöî. ÍµøÎÇòÏûá!",
            "ÎßàÏùåÏùò Ïó¨Ïú†Î•º Ï∞æÎäî Î™®ÏäµÏù¥ Î≥¥Í∏∞ Ï¢ãÏïÑÏöî. Ìï≠ÏÉÅ ÏùëÏõêÌï†Í≤åÏöî."
        ])
        
        self.comment_bank["Ïö∞Ïö∏Ìï¥"].extend([
            "ÎßàÏùåÏù¥ ÎßéÏù¥ ÌûòÎìúÏã† Í≤É Í∞ôÎÑ§Ïöî. Ï†úÍ∞Ä Îì§Ïñ¥ÎìúÎ¶¥Í≤åÏöî.",
            "Î¨¥Ïä® ÏùºÏù∏ÏßÄ Ï¢Ä Îçî ÏûêÏÑ∏Ìûà Îì£Í≥† Ïã∂Ïñ¥Ïöî. Ïñ∏Ï†úÎì† ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî.",
            "ÎßéÏù¥ Í¥¥Î°úÏö∞ÏãúÍ≤†Ïñ¥Ïöî. ÌòºÏûê ÎÅôÎÅô ÏïìÏßÄ ÎßàÏãúÍ≥† ÌÑ∏Ïñ¥ÎÜìÏïÑ Î≥¥ÏÑ∏Ïöî.",
            "ÌòºÏûêÎùºÍ≥† ÏÉùÍ∞ÅÌïòÏßÄ ÎßàÏãúÍ≥† Í∏∞Ïö¥ ÎÇ¥ÏÑ∏Ïöî. ÎãπÏã†ÏùÄ ÏÜåÏ§ëÌï¥Ïöî.",
            "ÏÉÅÏ≤òÎ∞õÏùÄ ÎßàÏùåÏùÑ Ïûò Ï∂îÏä§Î•¥ÏãúÍ∏∏ Î∞îÎùºÏöî. ÏãúÍ∞ÑÏùÄ ÎãπÏã† Ìé∏Ïù¥ÏóêÏöî.",
            "ÏßÄÍ∏àÏùÄ ÌûòÎì§ÏßÄÎßå Î∂ÑÎ™Ö ÎÇòÏïÑÏßà Í±∞ÏòàÏöî. ÎØøÏñ¥ ÏùòÏã¨Ïπò ÏïäÏïÑÏöî.",
            "Ïö∞Ïö∏Ìïú ÎßàÏùåÏù¥ Îì§ ÎïåÎäî Ïû†Ïãú Ïâ¨Ïñ¥Í∞ÄÎèÑ Ï¢ãÏïÑÏöî. Í¥úÏ∞ÆÏïÑÏöî.",
            "ÎãπÏã†Ïùò ÏûòÎ™ªÏù¥ ÏïÑÎãàÏóêÏöî. ÎÑàÎ¨¥ ÏûêÏ±ÖÌïòÏßÄ ÎßàÏÑ∏Ïöî.",
            "ÎààÎ¨ºÏùÑ ÌùòÎ¶¨Îäî Í≤ÉÎèÑ Í∞êÏ†ï Ìï¥ÏÜåÏóê ÎèÑÏõÄÏù¥ ÎèºÏöî. ÌéëÌéë Ïö∞ÏÖîÎèÑ ÎèºÏöî.",
            "Ï†úÍ∞Ä Ìï≠ÏÉÅ Í≥ÅÏóêÏÑú Îì§Ïñ¥ÎìúÎ¶¥Í≤åÏöî. ÌûòÎì† ÌïòÎ£® Í≥†ÏÉù ÎßéÏïòÏñ¥Ïöî."
        ])
        
        # Initialize attributes
        self.comment_tokenizer = None
        self.comment_model = None
        self.enc_model = None 
        self.dec_model = None
        self.comment_max_len = 20

    
        if TENSORFLOW_AVAILABLE:
            print("Initializing AI Emotion Analysis Model (5-Class LSTM)...")
            self.tokenizer = Tokenizer()
            
            if os.environ.get('SKIP_TRAINING'):
                print("Skipping training logic (SKIP_TRAINING active).")
                return

            # Check for saved model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            self.model_path = os.path.join(base_dir, 'emotion_model.h5')
            self.tokenizer_path = os.path.join(base_dir, 'tokenizer.pickle')
            
            # Comment Model Paths
            self.comment_model_path = os.path.join(base_dir, 'comment_model.h5')
            self.comment_enc_model_path = os.path.join(base_dir, 'comment_enc_model.h5')
            self.comment_tokenizer_path = os.path.join(base_dir, 'comment_tokenizer.pickle')

            # Check Training Condition
            current_count = self._get_keyword_count()
            last_count = self._get_last_trained_count()
            diff = current_count - last_count
            
            print(f"üìä Training Check: Current Keywords={current_count}, Last Trained={last_count}, Diff={diff}")
            
            should_train = (diff >= 100)
            
            # Force train if models are missing?
            # User said "Otherwise just run server". But if models missing, we can't run AI.
            # However, fallback logic exists.
            # We will follow user strictly: Only train if diff >= 100.
            # But we must try to load if we don't train.
            
            model_exists = os.path.exists(self.model_path) and os.path.exists(self.tokenizer_path)

            if should_train:
                print(f"üöÄ New data detected (+{diff} >= 100). Starting Full Training (LSTM + Seq2Seq)...")
                try:
                    self._load_and_train()
                    self._save_training_state(current_count)
                    print("‚úÖ Training complete and state saved.")
                except Exception as e:
                    print(f"‚ùå Training failed: {e}")
                    # Try loading existing if training failed
                    if model_exists:
                        self._load_existing_models()
            
            elif model_exists:
                print("üì¶ Models found. Loading existing models...")
                self._load_existing_models()
                
            else:
                print("‚ö†Ô∏è No models found and new data < 100. Skipping training.")
                print("   The server will run in Basic Mode (Keyword Fallback).")
            
            print("AI Model initialized.")
            
            # Load Comment Bank (Safety Net)
            self.comment_bank = {}
            self.load_comment_bank()
            self.load_emotion_bank()
            
            # Load KoGPT-2 (Phase 2)
            try:
                print("Loading KoGPT-2 Model...")
                self.gpt_tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", 
                    bos_token='</s>', eos_token='</s>', unk_token='<unk>', 
                    pad_token='<pad>', mask_token='<mask>')
                self.gpt_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
                self.gpt_model.eval() # Set to evaluation mode
                print("KoGPT-2 Loaded successfully.")
            except Exception as e:
                print(f"KoGPT-2 Load Failed: {e}")
                self.gpt_model = None

        else:
            print("Initializing Fallback Emotion Analysis (Keyword based - 5 classes)...")

    def _get_keyword_count(self):
        """Get total count of emotion keywords from DB"""
        session = self.Session()
        try:
            # Need to import inside as it might fail if table doesn't exist?
            # Actually models is imported at top level, but table creation happens later.
            # If table doesn't exist, this query will fail.
            from models import EmotionKeyword
            # Check if table exists (raw SQL for safety if ORM fails?)
            # But let's try ORM first.
            count = session.query(EmotionKeyword).count()
            return count
        except Exception as e:
            # Table likely doesn't exist yet
            return 0
        finally:
            session.close()

    def _get_last_trained_count(self):
        """Read last trained count from JSON"""
        if os.path.exists(TRAINING_STATE_FILE):
            try:
                with open(TRAINING_STATE_FILE, 'r') as f:
                    data = json.load(f)
                    return data.get('last_keyword_count', 0)
            except:
                return 0
        return 0

    def _save_training_state(self, count):
        """Save current keyword count to JSON"""
        try:
            with open(TRAINING_STATE_FILE, 'w') as f:
                json.dump({'last_keyword_count': count}, f)
        except Exception as e:
            print(f"Error saving training state: {e}")

    def _load_existing_models(self):
        """Helper to load existing models"""
        try:
            import pickle
            from tensorflow.keras.models import load_model
            
            self.model = load_model(self.model_path)
            
            with open(self.tokenizer_path, 'rb') as handle:
                self.tokenizer = pickle.load(handle)
                
            self.vocab_size = len(self.tokenizer.word_index) + 1
            print("Emotion Model loaded.")
            
            # COMMENT MODEL LOADING
            if os.path.exists(self.comment_model_path) and os.path.exists(self.comment_tokenizer_path):
                print("Loading comment generation model...")
                self.comment_model = load_model(self.comment_model_path)
                with open(self.comment_tokenizer_path, 'rb') as ct:
                    self.comment_tokenizer = pickle.load(ct)
                print("Comment Model loaded.")
                self._rebuild_inference_models()
            else:
                print("Comment model not found (skipping auto-train as per policy).")
                
        except Exception as e:
            print(f"Error loading models: {e}.")
            # Assuming fallback will take over

    def load_comment_bank(self):
        """Load curated advice from JSON"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.comment_bank = data.get('keywords', {})
                print(f"Loaded {len(self.comment_bank)} keyword categories from Comment Bank.")
            else:
                print("Comment Bank file not found. Skipping.")
        except Exception as e:
            print(f"Error loading comment bank: {e}")

    def generate_keyword_comment(self, user_input):
        """Phase 1: Hybrid Keyword System (Priority 1)"""
        if not self.comment_bank or not user_input:
            return None
            
        # Extract text if input is dict
        if isinstance(user_input, dict):
            text = f"{user_input.get('event', '')} {user_input.get('emotion', '')} {user_input.get('self_talk', '')}"
        else:
            text = str(user_input)
            
        for category, content in self.comment_bank.items():
            if category in text:
                return content.get('default', "ÌûòÎÇ¥ÏÑ∏Ïöî.")
                
            keywords = content.get('emotion_keywords', [])
            for k in keywords:
                if k in text:
                    return content.get('default', "ÌûòÎÇ¥ÏÑ∏Ïöî.")
        return None

    def load_emotion_bank(self):
        """Load 60-class emotion advice"""
        try:
            import json
            base_dir = os.path.dirname(os.path.abspath(__file__))
            bank_path = os.path.join(base_dir, 'data', 'emotion_comment_bank.json')
            if os.path.exists(bank_path):
                with open(bank_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.emotion_bank = data.get('keywords', {}) # Structure reuse
                print(f"Loaded {len(self.emotion_bank)} emotion categories.")
            else:
                self.emotion_bank = {}
        except Exception as e:
            print(f"Error loading emotion bank: {e}")
            self.emotion_bank = {}

    def generate_label_comment(self, emotion_label):
        """Phase 1.5: Label-Based Retrieval (Priority 2)"""
        # emotion_label format: "Î∂ÑÎÖ∏ (Î∞∞Ïã†Í∞ê) (85.0%)" or just "Î∂ÑÎÖ∏ (Î∞∞Ïã†Í∞ê)"
        if not self.emotion_bank:
            return None
            
        # Clean label to match key
        # Key in bank: "Î∂ÑÎÖ∏ (ÎÖ∏Ïó¨ÏõÄ/ÏñµÏö∏)"
        # Predicted Label: "Î∂ÑÎÖ∏ (ÎÖ∏Ïó¨ÏõÄ/ÏñµÏö∏)"
        
        # Remove confidence if present (e.g. " (85.0%)")
        # And ensure we match the key format "Name (Sub)"
        try:
            if '(' in emotion_label:
                # Take everything up to the second closing paren? Or just strip the last one (confidence)?
                # Our keys are "Name (Sub)".
                # Predicted might be "Name (Sub) (85%)" or "Name (Sub)"
                
                # Split by ' (' which usually separates label from confidence
                label_key = emotion_label.rsplit(' (', 1)[0]
                # If rsplit didn't find ' (', it returns the string itself if maxsplit used, 
                # but rsplit(' (', 1) might return just the string if not found? 
                # Actually, simple split is risky.
                
                # Safer: Just look for a key in our bank that matches the start of the string
                pass
            else:
                label_key = emotion_label
        except:
             label_key = emotion_label
        
        # Exact match attempt
        if label_key in self.emotion_bank:
            return self.emotion_bank[label_key].get('default')
            
        # Fuzzy match (e.g. just "Î∂ÑÎÖ∏" part?) - No, be precise.
        # Check stripping parens check? 
        for key in self.emotion_bank:
            if key in emotion_label:
                return self.emotion_bank[key].get('default')
        
        return None
        
    def generate_kogpt2_comment(self, user_input, emotion_label):
        """Phase 2: KoGPT-2 Generation (Priority 2)"""
        if not self.gpt_model or not self.gpt_tokenizer:
            return None
            
        try:
            # Handle user_input (str or dict)
            if isinstance(user_input, dict):
                event = user_input.get('event', '')
                emotion = user_input.get('emotion', '')
                self_talk = user_input.get('self_talk', '')
                
                # Few-Shot Prompting to guide the model
                prompt = (
                    "ÏÉÅÌô©: ÏãúÌóòÏóê Îñ®Ïñ¥Ï†∏ÏÑú Ïö∏ÏóàÎã§.\n"
                    "Í∞êÏ†ï: Ïä¨Ìîî (Ï¢åÏ†à)\n"
                    "ÏÉÅÎã¥ÏÇ¨ ÎãµÎ≥Ä: Ï†ïÎßê ÏÜçÏÉÅÌïòÏãúÍ≤†Ïñ¥Ïöî. Ïó¥Ïã¨Ìûà Ï§ÄÎπÑÌñàÏùÑ ÌÖêÎç∞ Í≤∞Í≥ºÍ∞Ä Ï¢ãÏßÄ ÏïäÏïÑ ÎßàÏùåÏù¥ ÏïÑÌîÑÏãúÏ£†. ÌïòÏßÄÎßå Ïù¥Î≤à Ïã§Ìå®Í∞Ä ÎãπÏã†Ïùò Î™®Îì† Í≤ÉÏùÑ Í≤∞Ï†ïÌïòÏßÄÎäî ÏïäÏïÑÏöî. Ïò§ÎäòÏùÄ Ìëπ Ïâ¨Î©¥ÏÑú Ïä§Ïä§Î°úÎ•º ÏúÑÎ°úÌï¥Ï£ºÏÑ∏Ïöî.\n\n"
                    f"ÏÉÅÌô©: {event} {emotion} {self_talk}\n"
                    f"Í∞êÏ†ï: {emotion_label}\n"
                    "ÏÉÅÎã¥ÏÇ¨ ÎãµÎ≥Ä:"
                )
            else:
                text = str(user_input)
                prompt = (
                    "ÏùºÍ∏∞: Ïò§Îäò ÌïòÎ£®Ï¢ÖÏùº ÎÑàÎ¨¥ ÌûòÎì§ÏóàÎã§.\n"
                    "Í∞êÏ†ï: Ïö∞Ïö∏ (ÏßÄÏπ®)\n"
                    "ÏÉÅÎã¥ÏÇ¨ ÎãµÎ≥Ä: Ïò§Îäò ÌïòÎ£® Ï†ïÎßê Í≥†ÏÉù ÎßéÏúºÏÖ®Ïñ¥Ïöî. ÏßÄÏπú Î™∏Í≥º ÎßàÏùåÏùÑ Ìé∏ÏïàÌïòÍ≤å ÎÇ¥Î†§ÎÜìÍ≥† Ìú¥ÏãùÏùÑ Ï∑®Ìï¥Î≥¥ÏÑ∏Ïöî. ÎãπÏã†ÏùÄ Ï∂©Î∂ÑÌûà ÏûòÌïòÍ≥† ÏûàÏäµÎãàÎã§.\n\n"
                    f"ÏùºÍ∏∞: {text}\n"
                    f"Í∞êÏ†ï: {emotion_label}\n"
                    "ÏÉÅÎã¥ÏÇ¨ ÎãµÎ≥Ä:"
                )
            
            input_ids = self.gpt_tokenizer.encode(prompt, return_tensors='pt')
            

            with torch.no_grad():
                gen_ids = self.gpt_model.generate(
                    input_ids,
                    max_length=len(input_ids[0]) + 50, # Shorter limit to reduce drift
                    do_sample=True,
                    temperature=0.6, # Focused
                    top_p=0.90,
                    repetition_penalty=1.2,
                    pad_token_id=self.gpt_tokenizer.pad_token_id,
                    eos_token_id=self.gpt_tokenizer.eos_token_id,
                    bos_token_id=self.gpt_tokenizer.bos_token_id,
                    use_cache=True
                )
                
            generated = self.gpt_tokenizer.decode(gen_ids[0])
            
            # Extract response
            if "ÏÉÅÎã¥ÏÇ¨ ÎãµÎ≥Ä:" in generated:
                response = generated.split("ÏÉÅÎã¥ÏÇ¨ ÎãµÎ≥Ä:")[-1].strip()
            elif "ÏÉÅÎã¥ÏÇ¨:" in generated:
                 response = generated.split("ÏÉÅÎã¥ÏÇ¨:")[-1].strip()
            else:
                response = generated
                
            # Post-process: Take first 2 sentences only to avoid hallucination
            sentences = response.split('.')
            clean_response = '. '.join(sentences[:2]).strip() + "."
            
            return clean_response
            
        except Exception as e:
            print(f"KoGPT Generation Error: {e}")
            return None


    def _rebuild_inference_models(self):
        # Reconstruct Encoder/Decoder from self.comment_model layers
        # This assumes a specific structure: [Input, Input] -> [Embedding, Embedding] -> [LSTM, LSTM] -> Dense
        try:
            from tensorflow.keras.models import Model
            from tensorflow.keras.layers import Input
            
            latent_dim = 256
            
            # Get layers by name or index. 
            # Structure from train_comment_model:
            # Inputs: input_1 (enc), input_2 (dec)
            # Embedding: embedding_1 (shared?) or separate. In code below, distinct.
            
            # Let's rely on layer names if possible, but default names change.
            # Best approach: Retain reference to layers if just trained, but here we loaded from disk.
            # We need to traverse the graph.
            
            # For robustness in this prototype, if rebuild fails, we just don't gen comments.
            # OR, we simply save the inference models separately. That's safer.
            pass # Placeholder, will implement save/load of separate inference models or rebuild logic
        except Exception as e:
            print(f"Rebuild error: {e}")

    def _load_and_train(self):
        # --- Load GoEmotions Data & Map Labels ---
        # DISABLE for 60-class upgrade: GoEmotions labels (0-27) do not map 1:1 to our 60 classes yet.
        # We rely solely on the Sentiment Dialogue Corpus for now.
        # (Removed old hardcoded mapping to 5 classes to prevent poisoning)


        # 2. Initialize Conversation Pairs (for Seq2Seq)
        self.conversation_pairs = []

        # 3. Load Sentiment Dialogue Corpus (Training & Validation)
        # Adds to self.train_texts & self.train_labels AND self.conversation_pairs
        self.load_sentiment_corpus()
        
        # 4. Load Data from Database (Fine-tuning)
        # DISABLE for 60-class upgrade: User Emoji (5-class) is incompatible with 60-class labels.
        # Future work: Map 5-class to "soft labels" or specific sub-classes.
        # db_texts, db_labels = self.load_db_data()
        # if db_texts:
        #     print(f"Integrating {len(db_texts)} samples from Database for Emotion Analysis...")
        #     self.train_texts.extend(db_texts)
        #     self.train_labels = np.concatenate((self.train_labels, np.array(db_labels)))
            
        print(f"Final Training Data Size: {len(self.train_texts)}")

        # 5. Train Initial Model
        self._train_initial_model()
        
        # 6. Train Comment Model (Seq2Seq)
        # Assuming DB data isn't needed for Comment Gen yet (Prompt didn't specify, just Analysis)
        # But wait, User said "AIÎ∂ÑÏÑùÏùÑ ÏßÑÌñâÌï¥ÏÑú... Ï†ÅÏö©Ìï¥Ï§ò". 
        # Typically implies Emotion Analysis. Comment Gen needs (Q, A) pairs. 
        # DB data is Diarh (Monologue), not Dialogue. So it's best suited for Emotion Analysis.
        self.train_comment_model() # Train comment model after main model

    def _train_initial_model(self):
        # Increased Vocab size for larger dataset
        self.tokenizer.fit_on_texts(self.train_texts)
        self.vocab_size = len(self.tokenizer.word_index) + 1
        print(f"Emotion Vocab Size: {self.vocab_size}")
        
        sequences = self.tokenizer.texts_to_sequences(self.train_texts)
        X_train = pad_sequences(sequences, maxlen=self.max_len)
        y_train = self.train_labels
        
        self.model = Sequential()
        self.model.add(Embedding(self.vocab_size, 256, input_length=self.max_len)) 
        self.model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3)) 
        self.model.add(Dense(128, activation='relu')) 
        self.model.add(Dropout(0.4))
        self.model.add(Dense(len(self.classes), activation='softmax')) 
        
        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
        print("Training Emotion Classifier...")
        self.model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=1) 
        
        # Save Model & Tokenizer
        try:
            import pickle
            self.model.save(self.model_path)
            with open(self.tokenizer_path, 'wb') as handle:
                pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"Model saved to {self.model_path}")
        except Exception as e:
            print(f"Error saving model: {e}") 

    def _train_with_data(self, texts, labels):
        """
        Train the model with custom provided data (for retraining script).
        
        Args:
            texts (list): List of text strings
            labels (list): List of label indices (0-59 for 60-class)
        """
        if not TENSORFLOW_AVAILABLE:
            print("TensorFlow not available. Cannot train.")
            return
        
        print(f"Training with {len(texts)} samples...")
        
        # Use existing tokenizer if loaded, or create new one
        if not self.tokenizer:
            self.tokenizer = Tokenizer()
        
        # Fit tokenizer on new data
        self.tokenizer.fit_on_texts(texts)
        self.vocab_size = len(self.tokenizer.word_index) + 1
        print(f"Vocabulary Size: {self.vocab_size}")
        
        # Prepare sequences
        sequences = self.tokenizer.texts_to_sequences(texts)
        X_train = pad_sequences(sequences, maxlen=self.max_len)
        y_train = np.array(labels)
        
        # Build model architecture
        self.model = Sequential()
        self.model.add(Embedding(self.vocab_size, 256, input_length=self.max_len))
        self.model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3))
        self.model.add(Dense(128, activation='relu'))
        self.model.add(Dropout(0.4))
        self.model.add(Dense(len(self.classes), activation='softmax'))
        
        self.model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        # Train
        print("Starting training...")
        history = self.model.fit(
            X_train, y_train,
            epochs=5,
            batch_size=32,
            validation_split=0.1,
            verbose=1
        )
        
        # Save
        try:
            import pickle
            self.model.save(self.model_path)
            with open(self.tokenizer_path, 'wb') as handle:
                pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"‚úÖ Model saved to {self.model_path}")
            print(f"‚úÖ Tokenizer saved to {self.tokenizer_path}")
        except Exception as e:
            print(f"‚ùå Error saving model: {e}")
        
        return history



    def predict(self, text):
        if not text: return "Î∂ÑÏÑù Î∂àÍ∞Ä"

        if TENSORFLOW_AVAILABLE and self.model:
            try:
                sequences = self.tokenizer.texts_to_sequences([text])
                padded = pad_sequences(sequences, maxlen=self.max_len)
                prediction = self.model.predict(padded, verbose=0)[0]
                predicted_class_idx = np.argmax(prediction)
                confidence = prediction[predicted_class_idx]
                predicted_label = self.classes[predicted_class_idx]
                return f"{predicted_label} ({(confidence * 100):.1f}%)"
            except Exception as e:
                print(f"Prediction error: {e}")
                return self._fallback_predict(text)
        else:
            return self._fallback_predict(text)

    def _fallback_predict(self, text):
        # Load keywords from DB dynamically
        session = self.Session()
        try:
            from models import EmotionKeyword # Import here to avoid circular dep
            
            keywords = session.query(EmotionKeyword).all()
            
            scores = [0] * 5
            found_any = False
            
            for kw in keywords:
                if kw.keyword in text:
                    scores[kw.emotion_label] += kw.frequency
                    found_any = True
            
            if found_any:
                max_score = max(scores)
                total_score = sum(scores)
                confidence = (max_score / total_score * 100) if total_score > 0 else 85.0
                best_idx = scores.index(max_score)
                return f"{self.classes[best_idx]} ({confidence:.1f}%)"
            else:
                return "Í∑∏Ï†ÄÍ∑∏Îûò (40.0%)" 
        except Exception as e:
            print(f"Fallback error: {e}")
            return "Î∂ÑÏÑù Î∂àÍ∞Ä"
        finally:
            session.close()

    def load_sentiment_corpus(self):
        """
        Load 'Sentiment Dialogue Corpus' (Training & Validation).
        Use Full 60-Class Granularity (E10 ~ E69).
        """
        import json
        
        files = [
            'Í∞êÏÑ±ÎåÄÌôîÎßêÎ≠âÏπò(ÏµúÏ¢ÖÎç∞Ïù¥ÌÑ∞)_Training.json',
            'Í∞êÏÑ±ÎåÄÌôîÎßêÎ≠âÏπò(ÏµúÏ¢ÖÎç∞Ïù¥ÌÑ∞)_Validation.json'
        ]
        
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        import glob
        base_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Use glob to avoid Unicode normalization issues (NFC vs NFD) on macOS
        files = glob.glob(os.path.join(base_dir, "*Training.json")) + glob.glob(os.path.join(base_dir, "*Validation.json"))
        
        if not files:
             print("No corpus files found via glob!")
        
        for fpath in files:
            fname = os.path.basename(fpath)
            # fpath is already absolute from glob
             
            print(f"Loading corpus: {fname}...")
            try:
                with open(fpath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                new_texts = []
                new_labels = []
                
                for entry in data:
                    try:
                        etype = entry.get('profile', {}).get('emotion', {}).get('type', '')
                        
                        # Only use codes defined in our map (E10-E69)
                        if etype in self.code_to_idx:
                            # Extract user text
                            content = entry.get('talk', {}).get('content', {})
                            text = content.get('HS01', '')
                            system_text = content.get('SS01', '')
                            
                            if text:
                                label_idx = self.code_to_idx[etype]
                                new_texts.append(text)
                                new_labels.append(label_idx)
                                
                                # Store for comment training if pair exists
                                if system_text:
                                    self.conversation_pairs.append((text, system_text))
                                    
                    except Exception as e:
                        continue
                        
                # Add to training set
                self.train_texts.extend(new_texts)
                self.train_labels = np.concatenate((self.train_labels, np.array(new_labels)))
                
                print(f"Added {len(new_texts)} samples from {fname}")
                
            except Exception as e:
                print(f"Error loading corpus {fname}: {e}")


    def load_db_data(self):
        """
        Load data from the Diary table to fine-tune the model.
        Returns:
            X (list): Combined text (event + emotion_desc + self_talk)
            y (list): Emotion labels (0-4)
        """
        print("Loading data from Database...")
        session = self.Session()
        X = []
        y = []
        try:
            from models import Diary
            diaries = session.query(Diary).all()
            
            # Mood Level to Label Mapping
            # 5(Happy)->0, 4(Calm)->1, 3(Neutral)->2, 2(Depressed)->3, 1(Angry)->4
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            
            for d in diaries:
                if not d.mood_level: continue
                
                label = mapping.get(d.mood_level)
                if label is None: continue
                
                # Combine text fields for rich context
                text = f"{d.event} {d.emotion_desc} {d.self_talk}"
                X.append(text)
                y.append(label)
                
            print(f"Loaded {len(X)} samples from Database.")
            return X, y
            
        except Exception as e:
            print(f"Error loading DB data: {e}")
            return [], []
        finally:
            session.close()





    def train_comment_model(self):
        """
        Train a Seq2Seq model using ChatbotData.csv AND Sentiment Dialogue Corpus
        """
        if not TENSORFLOW_AVAILABLE: return

        print("Training Comment Generation Model (Seq2Seq)...")
        try:
            import os
            import pickle
            
            # 1. Load ChatbotData.csv
            base_dir = os.path.dirname(os.path.abspath(__file__))
            data_path = os.path.join(base_dir, 'ChatbotData.csv')
            questions = []
            answers = []
            
            if os.path.exists(data_path):
                df = pd.read_csv(data_path)
                # Take a sample to keep training time reasonable, or all? 
                # Let's take mixed sample.
                df = df.sample(frac=1).reset_index(drop=True)
                questions = df['Q'].astype(str).tolist()[:5000] # Cap at 5000 for speed
                # Char-level: Use \t for Start, \n for End
                answers = df['A'].apply(lambda x: '\t' + str(x) + '\n').tolist()[:5000]
            
            # 2. Add Sentiment Dialogue Corpus
            if hasattr(self, 'conversation_pairs'):
                print(f"Integrating {len(self.conversation_pairs)} pairs from Sentiment Corpus...")
                # Sample 5000 from here too to balance
                import random
                pairs = self.conversation_pairs
                if len(pairs) > 5000:
                    pairs = random.sample(pairs, 5000)
                
                for q, a in pairs:
                    questions.append(str(q))
                    answers.append('\t' + str(a) + '\n')
            
            print(f"Total Comment Training Samples: {len(questions)}")

            # Shared Tokenizer (Character Level for better Korean convergence without Mecab)
            # Filters: Don't filter \t and \n as they are our tokens!
            self.comment_tokenizer = Tokenizer(char_level=True, filters='!"#$%&()*+,-./:;<=>?@[\\]^`{|}~') 
            self.comment_tokenizer.fit_on_texts(questions + answers)
            
            vocab_size = len(self.comment_tokenizer.word_index) + 1
            print(f"Comment Vocab Size (Char-level): {vocab_size}")

            # Encoder Data
            tokenized_Q = self.comment_tokenizer.texts_to_sequences(questions)
            encoder_input_data = pad_sequences(tokenized_Q, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Data
            tokenized_A = self.comment_tokenizer.texts_to_sequences(answers)
            decoder_input_data = pad_sequences(tokenized_A, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Target (Shifted-by-one)
            decoder_target_data = np.zeros_like(decoder_input_data, dtype="float32")
            decoder_target_data[:, :-1] = decoder_input_data[:, 1:]
            decoder_target_data = np.expand_dims(decoder_target_data, -1)

            # Model Architecture
            latent_dim = 256
            
            # Encoder
            encoder_inputs = Input(shape=(None,), name='enc_input')
            enc_emb_layer = Embedding(vocab_size, latent_dim, name='enc_embedding')
            enc_emb = enc_emb_layer(encoder_inputs)
            encoder_lstm = LSTM(latent_dim, return_state=True, name='enc_lstm')
            encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
            encoder_states = [state_h, state_c]
            
            # Decoder
            decoder_inputs = Input(shape=(None,), name='dec_input')
            dec_emb_layer = Embedding(vocab_size, latent_dim, name='dec_embedding')
            dec_emb = dec_emb_layer(decoder_inputs)
            decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='dec_lstm')
            decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
            decoder_dense = Dense(vocab_size, activation='softmax', name='dec_dense')
            decoder_outputs = decoder_dense(decoder_outputs)
            
            self.comment_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
            self.comment_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
            print("Fitting Seq2Seq Model (Char-level)...")
            self.comment_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
                                   batch_size=64, epochs=15, validation_split=0.2, verbose=1)
                                   
            print("Comment Model Trained. Saving...")
            
            # Save Main Model
            self.comment_model.save(self.comment_model_path)
            
            # Save Tokenizer
            with open(self.comment_tokenizer_path, 'wb') as handle:
                pickle.dump(self.comment_tokenizer, handle)

            # Construct & Save Inference Models
            self.enc_model = Model(encoder_inputs, encoder_states)
            self.enc_model.save(os.path.join(base_dir, 'comment_enc_model.h5'))
            
            dec_state_input_h = Input(shape=(latent_dim,), name='dec_input_h')
            dec_state_input_c = Input(shape=(latent_dim,), name='dec_input_c')
            dec_states_inputs = [dec_state_input_h, dec_state_input_c]
            
            dec_emb2 = dec_emb_layer(decoder_inputs)
            dec_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states_inputs)
            dec_states2 = [state_h2, state_c2]
            dec_outputs2 = decoder_dense(dec_outputs2)
            
            self.dec_model = Model([decoder_inputs] + dec_states_inputs, [dec_outputs2] + dec_states2)
            self.dec_model.save(os.path.join(base_dir, 'comment_dec_model.h5'))
            
            print("Inference models saved.")
            
        except Exception as e:
            print(f"Error training comment model: {e}")

    def _rebuild_inference_models(self):
        """Rebuild inference models from loaded main model or load separate files"""
        try:
            from tensorflow.keras.models import load_model
            base_dir = os.path.dirname(os.path.abspath(__file__))
            enc_path = os.path.join(base_dir, 'comment_enc_model.h5')
            dec_path = os.path.join(base_dir, 'comment_dec_model.h5')
            
            if os.path.exists(enc_path) and os.path.exists(dec_path):
                self.enc_model = load_model(enc_path)
                self.dec_model = load_model(dec_path)
                print("Inference models loaded successfully.")
            else:
                print("Inference model files missing. Comment generation may fail.")
        except Exception as e:
            print(f"Error loading inference models: {e}")

    def generate_ai_comment(self, text):
        if not self.enc_model or not self.comment_tokenizer:
            return None
            
        try:
            # Preprocess
            seq = self.comment_tokenizer.texts_to_sequences([text])
            if not seq or not seq[0]: return None # Unknown words
            
            input_seq = pad_sequences(seq, maxlen=self.comment_max_len, padding='post')
            
            # Encode
            states_value = self.enc_model.predict(input_seq, verbose=0)
            
            # Generate
            target_seq = np.zeros((1,1))
            # Use \t Key
            target_seq[0, 0] = self.comment_tokenizer.word_index['\t']
            
            decoded_sentence = ''
            stop_condition = False
            
            while not stop_condition:
                output_tokens, h, c = self.dec_model.predict([target_seq] + states_value, verbose=0)
                
                # Sample with Temperature
                sampled_token_index = self.sample(output_tokens[0, -1, :], temperature=0.6)
                sampled_word = self.comment_tokenizer.index_word.get(sampled_token_index, '')
                
                if sampled_word == '\n' or len(decoded_sentence) > 50:
                    stop_condition = True
                else:
                    decoded_sentence += sampled_word
                
                # Update target seq
                target_seq = np.zeros((1, 1))
                target_seq[0, 0] = sampled_token_index
                
                # Update states
                states_value = [h, c]
                
        except Exception as e:
            print(f"Gen Error: {e}")
            return None
            
        return decoded_sentence.strip()

    def sample(self, preds, temperature=1.0):
        # Helper function to sample an index from a probability array
        preds = np.asarray(preds).astype('float64')
        preds = np.log(preds + 1e-10) / temperature
        exp_preds = np.exp(preds)
        preds = exp_preds / np.sum(exp_preds)
        probas = np.random.multinomial(1, preds, 1)
        return np.argmax(probas)


    def generate_comment(self, prediction_text, user_text=None):
        """
        Generate a supportive comment.
        Priority: 1. Keyword Bank (Safety Net) 2. AI Generation (Seq2Seq) 3. Fallback
        """
        # 1. Phase 1: Keyword Safety Net (Highest Priority)
        if user_text:
            keyword_comment = self.generate_keyword_comment(user_text)
            if keyword_comment:
                 return keyword_comment

        if not prediction_text or "Î∂ÑÏÑù Î∂àÍ∞Ä" in prediction_text:
            return "ÎãπÏã†Ïùò Ïù¥ÏïºÍ∏∞Î•º Îçî Îì§Î†§Ï£ºÏÑ∏Ïöî. Ìï≠ÏÉÅ Îì£Í≥† ÏûàÏùÑÍ≤åÏöî."

        # Extract strict label (remove confidence score)
        # e.g. "Î∂ÑÎÖ∏ (Î∞∞Ïã†Í∞ê) (85.0%)" -> "Î∂ÑÎÖ∏ (Î∞∞Ïã†Í∞ê)"
        try:
             emotion_label_only = prediction_text.rsplit(' (', 1)[0]
        except:
             emotion_label_only = prediction_text.split()[0]

        # 2. Phase 2: KoGPT-2 Generation (High Priority for Context)
        if user_text:
            # Pass the full specific label to KoGPT for context-aware generation
            gpt_comment = self.generate_kogpt2_comment(user_text, emotion_label_only)
            if gpt_comment:
                return f"{gpt_comment}"

        # 3. Phase 1.5: Label-based Specific Advice (Fallback)
        label_comment = self.generate_label_comment(emotion_label_only)
        if label_comment:
             return label_comment

        try:
            label = prediction_text.split()[0] # e.g. "ÌñâÎ≥µÌï¥"
            
            # 3. Try Legacy AI Generation (Seq2Seq) - Deprecated but kept as backup
            ai_generated = None
            if self.enc_model:
                try:
                    # Pass the label as context since we don't use full text for Seq2Seq yet
                    ai_generated = self.generate_ai_comment(label)
                except:
                    pass
            
            if ai_generated:
                return f"{ai_generated}"
            
            # 4. Fallback
            return "Ïò§Îäò ÌïòÎ£®ÎèÑ ÏàòÍ≥† ÎßéÏúºÏÖ®Ïñ¥Ïöî."
            
        except Exception as e:
            print(f"Comment Gen Error: {e}")
            return "ÎãπÏã†Ïùò ÎßàÏùåÏùÑ Ïù¥Ìï¥Ìï¥Ïöî."


    def update_keywords(self, text, mood_level):
        """
        Learn new keywords from the text based on the user's provided mood_level.
        mood_level: 1-5 (User input)
        Mapping: 5->0(Happy), 4->1(Calm), 3->2(Neutral), 2->3(Depressed), 1->4(Angry)
        """
        if not text: return

        # Map mood_level to label
        # mood_level is expected to be int
        try:
            mood_val = int(mood_level)
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            target_label = mapping.get(mood_val)
            
            if target_label is None:
                return # Invalid mood level
        except:
            return

        session = self.Session()
        try:
            from models import EmotionKeyword
            
            # Simple Tokenization (Space-based)
            # In a real KR app, use Mecab/Konlpy. Here we split by space.
            words = text.split()
            
            for w in words:
                # Basic cleaning
                w = w.strip('.,?!~"\'')
                if len(w) < 2: continue # Skip single chars
                
                # Check if exists
                existing = session.query(EmotionKeyword).filter_by(keyword=w).first()
                
                if existing:
                    # If exists, increment frequency if label matches
                    # If label differs, maybe decrease freq or ignore? 
                    # Let's just track co-occurrence. Complex logic omitted for simplicity.
                    if existing.emotion_label == target_label:
                        existing.frequency += 1
                else:
                    # NEW WORD -> LEARN IT!
                    print(f"Learning new keyword: {w} -> {self.classes[target_label]}")
                    new_kw = EmotionKeyword(
                        keyword=w,
                        emotion_label=target_label,
                        frequency=1
                    )
                    session.add(new_kw)
            
            session.commit()
            
            # If Model is active, we might want to update tokenizer or retrain eventually.
            # Rerunning Tokenizer.fit_on_texts would be needed. 
            # For this session, we won't retrain the LSTM live (too slow), but the Fallback logic will immediately benefit.
            
        except Exception as e:
            print(f"Learning error: {e}")
            session.rollback()
        finally:
            session.close()

# Singleton instance
ai_analyzer = EmotionAnalysis()
