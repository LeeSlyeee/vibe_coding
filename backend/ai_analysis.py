import numpy as np
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from config import Config
try:
    from models import EmotionKeyword
except ImportError:
    # Handle possible circular import dynamically if needed, or rely on caller context
    pass

# Try to import TensorFlow/Keras, but provide fallback if not installed
try:
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("Warning: TensorFlow not found. Using simple keyword-based sentiment analysis.")

class EmotionAnalysis:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.max_len = 50
        self.vocab_size = 0
        
        # 5 Emotion Classes
        self.classes = ["í–‰ë³µí•´", "í‰ì˜¨í•´", "ê·¸ì €ê·¸ëž˜", "ìš°ìš¸í•´", "í™”ê°€ë‚˜"]
        
        # We will load keywords from DB for fallback/learning
        self.db_engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)
        self.Session = sessionmaker(bind=self.db_engine)

        # Initial Training Data (Hardcoded for LSTM initialization)
        #Ideally this should also come from DB, but keeping it simple for now
        self.train_texts = [
            # 0. í–‰ë³µí•´
            "ë„ˆë¬´ ìž¬ë°Œë„¤ìš”", "ìµœê³ ì˜ˆìš”", "ì˜¤ëŠ˜ ì •ë§ í–‰ë³µí–ˆë‹¤", "ê¸°ë¶„ì´ ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤", "ì¦ê±°ìš´ í•˜ë£¨",
            "ë³´ëžŒì°¬ í•˜ë£¨", "ì„±ì·¨ê°ì„ ëŠê¼ˆë‹¤", "ë¿Œë“¯í•˜ë‹¤", "ì‚¬ëž‘í•©ë‹ˆë‹¤", "ì¹œêµ¬ë“¤ê³¼ ì¦ê±°ìš´ ì‹œê°„",
            "ë§›ìžˆëŠ”ê±° ë¨¹ì–´ì„œ ì‹ ë‚œë‹¤", "í•©ê²©í•´ì„œ ê¸°ì˜ë‹¤", "ì„ ë¬¼ì„ ë°›ì•„ì„œ ì¢‹ë‹¤", "ì›ƒìŒì´ ë©ˆì¶”ì§€ ì•ŠëŠ”ë‹¤", "ì‹ ë‚˜ëŠ” ìŒì•…",
            
            # 1. í‰ì˜¨í•´
            "ë§ˆìŒì´ íŽ¸ì•ˆí•˜ë‹¤", "ì¡°ìš©í•œ í•˜ë£¨", "ì—¬ìœ ë¡œìš´ ì €ë…", "ì‚°ì±…ì„ í•˜ë‹ˆ ìƒì¾Œí•˜ë‹¤", "ì°¨ í•œìž”ì˜ ì—¬ìœ ",
            "ì•„ë¬´ ê±±ì • ì—†ì´ ì‰¬ì—ˆë‹¤", "ëª…ìƒì„ í–ˆë‹¤", "ìž ì„ í‘¹ ìž¤ë‹¤", "ìž”ìž”í•œ ìŒì•…ì„ ë“¤ì—ˆë‹¤", "í‰í™”ë¡­ë‹¤",
            "ë”°ëœ»í•œ í–‡ì‚´", "ë°”ëžŒì´ ì‹œì›í•˜ë‹¤", "ì±…ì„ ì½ìœ¼ë©° ížë§", "ì°¨ë¶„í•´ì§€ëŠ” ê¸°ë¶„", "ì•ˆì •ëœ ëŠë‚Œ",

            # 2. ê·¸ì €ê·¸ëž˜
            "ê·¸ì € ê·¸ëŸ° í•˜ë£¨", "í‰ë²”í•œ ì¼ìƒ", "íŠ¹ë³„í•œ ì¼ì€ ì—†ì—ˆë‹¤", "ê·¸ëƒ¥ ê·¸ëž¬ë‹¤", "ë¬´ë‚œí•œ í•˜ë£¨",
            "ë³„ì¼ ì—†ì—ˆë‹¤", "ë˜‘ê°™ì€ í•˜ë£¨", "ì§€ë£¨í•˜ì§€ë„ ìž¬ë¯¸ìžˆì§€ë„ ì•Šë‹¤", "ì˜ì˜", "ë³´í†µì´ë‹¤",
            "ê¸€ìŽ„ ìž˜ ëª¨ë¥´ê² ë‹¤", "ë‚˜ì˜ì§€ ì•Šë‹¤", "ì ë‹¹í•˜ë‹¤", "ê·¸ëŸ­ì €ëŸ­", "í• ë§Œ í–ˆë‹¤",

            # 3. ìš°ìš¸í•´
            "ì˜¤ëŠ˜ ë„ˆë¬´ ìŠ¬íŽë‹¤", "ìš°ìš¸í•´ìš”", "ë§ˆìŒì´ ì•„í”„ë‹¤", "ì™¸ë¡­ë‹¤", "ì§€ì¹œë‹¤",
            "ëˆˆë¬¼ì´ ë‚œë‹¤", "íž˜ë“  í•˜ë£¨ì˜€ì–´ìš”", "ì‹¤ìˆ˜í•´ì„œ ì†ìƒí•˜ë‹¤", "í›„íšŒëœë‹¤", "ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ë‹¤",
            "ì¢Œì ˆê°ì„ ëŠê¼ˆë‹¤", "ìƒì²˜ë°›ì•˜ë‹¤", "ê·¸ë¦¬ì›Œìš”", "ë¬´ê¸°ë ¥í•˜ë‹¤", "ê°€ìŠ´ì´ ë‹µë‹µí•˜ë‹¤",

            # 4. í™”ê°€ë‚˜
            "í™”ê°€ ë‚œë‹¤", "ì§œì¦ë‚˜ìš”", "ì •ë§ ì—´ë°›ëŠ”ë‹¤", "ê¸°ë¶„ì´ ë‚˜ì˜ë‹¤", "ì‹¸ì› ë‹¤",
            "ì–µìš¸í•˜ë‹¤", "ë¯¸ì›Œ ì£½ê² ë‹¤", "ë¶„ë…¸ê°€ ì¹˜ë¯¼ë‹¤", "ì–´ì´ì—†ë‹¤", "ìš©ì„œí•  ìˆ˜ ì—†ë‹¤",
            "ë‹µë‹µí•´ì„œ ë¯¸ì¹˜ê² ë‹¤", "ì‹ ê²½ì§ˆì´ ë‚œë‹¤", "í­ë°œí•  ê²ƒ ê°™ë‹¤", "ê¸°ë¶„ ìž¡ì³¤ë‹¤", "ë§ë„ ì•ˆ ëœë‹¤"
        ]
        labels_list = [0]*15 + [1]*15 + [2]*15 + [3]*15 + [4]*15
        self.train_labels = np.array(labels_list)

        if TENSORFLOW_AVAILABLE:
            print("Initializing AI Emotion Analysis Model (5-Class LSTM)...")
            self.tokenizer = Tokenizer()
            self._train_initial_model()
            print("AI Model initialized.")
        else:
            print("Initializing Fallback Emotion Analysis (Keyword based - 5 classes)...")

    def _train_initial_model(self):
        self.tokenizer.fit_on_texts(self.train_texts)
        self.vocab_size = len(self.tokenizer.word_index) + 1
        sequences = self.tokenizer.texts_to_sequences(self.train_texts)
        X_train = pad_sequences(sequences, maxlen=self.max_len)
        y_train = self.train_labels
        
        self.model = Sequential()
        self.model.add(Embedding(self.vocab_size, 64, input_length=self.max_len)) 
        self.model.add(LSTM(64)) 
        self.model.add(Dense(32, activation='relu')) 
        self.model.add(Dense(5, activation='softmax')) 
        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        self.model.fit(X_train, y_train, epochs=50, verbose=0) 

    def predict(self, text):
        if not text: return "ë¶„ì„ ë¶ˆê°€"

        if TENSORFLOW_AVAILABLE and self.model:
            try:
                sequences = self.tokenizer.texts_to_sequences([text])
                padded = pad_sequences(sequences, maxlen=self.max_len)
                prediction = self.model.predict(padded)[0]
                predicted_class_idx = np.argmax(prediction)
                confidence = prediction[predicted_class_idx]
                predicted_label = self.classes[predicted_class_idx]
                return f"{predicted_label} ({(confidence * 100):.1f}%)"
            except Exception as e:
                print(f"Prediction error: {e}")
                return self._fallback_predict(text)
        else:
            return self._fallback_predict(text)

    def _fallback_predict(self, text):
        # Load keywords from DB dynamically
        session = self.Session()
        try:
            # Fetch all keywords
            # Optimized: Could cache this and update periodically, but for now fetch on specific calls if not heavy
            # Actually, fetching all keywords every time is slow. Let's do a simple query or cache.
            # Since this is a prototype, fetching is fine for small DB.
            from models import EmotionKeyword # Import here to avoid circular dep
            
            keywords = session.query(EmotionKeyword).all()
            
            scores = [0] * 5
            found_any = False
            
            for kw in keywords:
                if kw.keyword in text:
                    scores[kw.emotion_label] += kw.frequency
                    found_any = True
            
            if found_any:
                max_score = max(scores)
                total_score = sum(scores)
                confidence = (max_score / total_score * 100) if total_score > 0 else 85.0
                best_idx = scores.index(max_score)
                return f"{self.classes[best_idx]} ({confidence:.1f}%)"
            else:
                return "ê·¸ì €ê·¸ëž˜ (40.0%)" 
        except Exception as e:
            print(f"Fallback error: {e}")
            return "ë¶„ì„ ë¶ˆê°€"
        finally:
            session.close()

    def generate_comment(self, prediction_text):
        """
        Generate a supportive comment based on the prediction label.
        prediction_text format: "Label (Confidence%)" e.g., "í–‰ë³µí•´ (90.0%)"
        """
        if not prediction_text or "ë¶„ì„ ë¶ˆê°€" in prediction_text:
            return "ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“¤ë ¤ì£¼ì„¸ìš”. í•­ìƒ ë“£ê³  ìžˆì„ê²Œìš”."

        try:
            # Extract label (split by space)
            label = prediction_text.split()[0]
            
            comments = {
                "í–‰ë³µí•´": "ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ í–‰ë³µí•˜ì…¨êµ°ìš”! ì´ ê¸ì •ì ì¸ ì—ë„ˆì§€ê°€ ë‚´ì¼ë„ ì´ì–´ì§€ê¸¸ ë°”ëž„ê²Œìš”. ðŸ˜Š",
                "í‰ì˜¨í•´": "ë§ˆìŒì´ íŽ¸ì•ˆí•˜ë‹¤ë‹ˆ ë‹¤í–‰ì´ì—ìš”. ë”°ëœ»í•œ ì°¨ í•œ ìž”ìœ¼ë¡œ í•˜ë£¨ë¥¼ ë§ˆë¬´ë¦¬í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ðŸµ",
                "ê·¸ì €ê·¸ëž˜": "í‰ë²”í•œ í•˜ë£¨ì˜€êµ°ìš”. ë‚´ì¼ì€ ì¢€ ë” íŠ¹ë³„í•œ ì¼ì´ ìƒê¸¸ì§€ë„ ëª°ë¼ìš”! íŒŒì´íŒ… ðŸ’ª",
                "ìš°ìš¸í•´": "ë§Žì´ íž˜ë“œì…¨êµ°ìš”. ì˜¤ëŠ˜ í•˜ë£¨ëŠ” í‘¹ ì‰¬ë©´ì„œ ìžì‹ ì„ í† ë‹¥ì—¬ì£¼ì„¸ìš”. ë‹¹ì‹ ì€ ì†Œì¤‘í•œ ì‚¬ëžŒìž…ë‹ˆë‹¤. ðŸ’™",
                "í™”ê°€ë‚˜": "ì†ìƒí•œ ì¼ì´ ìžˆìœ¼ì…¨ë‚˜ ë´ìš”. ìž ì‹œ ì‹¬í˜¸í¡ì„ í•˜ë©° ë§ˆìŒì„ ê°€ë¼ì•‰í˜€ë³´ë©´ ì–´ë–¨ê¹Œìš”? íž˜ë‚´ì„¸ìš”! ðŸ”¥"
            }
            
            return comments.get(label, "ë‹¹ì‹ ì˜ ê°ì •ì„ ì†Œì¤‘ížˆ ê°„ì§í•˜ì„¸ìš”.")
        except:
            return "ë‹¹ì‹ ì˜ ê°ì •ì„ ì†Œì¤‘ížˆ ê°„ì§í•˜ì„¸ìš”."

    def update_keywords(self, text, mood_level):
        """
        Learn new keywords from the text based on the user's provided mood_level.
        mood_level: 1-5 (User input)
        Mapping: 5->0(Happy), 4->1(Calm), 3->2(Neutral), 2->3(Depressed), 1->4(Angry)
        """
        if not text: return

        # Map mood_level to label
        # mood_level is expected to be int
        try:
            mood_val = int(mood_level)
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            target_label = mapping.get(mood_val)
            
            if target_label is None:
                return # Invalid mood level
        except:
            return

        session = self.Session()
        try:
            from models import EmotionKeyword
            
            # Simple Tokenization (Space-based)
            # In a real KR app, use Mecab/Konlpy. Here we split by space.
            words = text.split()
            
            for w in words:
                # Basic cleaning
                w = w.strip('.,?!~"\'')
                if len(w) < 2: continue # Skip single chars
                
                # Check if exists
                existing = session.query(EmotionKeyword).filter_by(keyword=w).first()
                
                if existing:
                    # If exists, increment frequency if label matches
                    # If label differs, maybe decrease freq or ignore? 
                    # Let's just track co-occurrence. Complex logic omitted for simplicity.
                    if existing.emotion_label == target_label:
                        existing.frequency += 1
                else:
                    # NEW WORD -> LEARN IT!
                    print(f"Learning new keyword: {w} -> {self.classes[target_label]}")
                    new_kw = EmotionKeyword(
                        keyword=w,
                        emotion_label=target_label,
                        frequency=1
                    )
                    session.add(new_kw)
            
            session.commit()
            
            # If Model is active, we might want to update tokenizer or retrain eventually.
            # Rerunning Tokenizer.fit_on_texts would be needed. 
            # For this session, we won't retrain the LSTM live (too slow), but the Fallback logic will immediately benefit.
            
        except Exception as e:
            print(f"Learning error: {e}")
            session.rollback()
        finally:
            session.close()

# Singleton instance
ai_analyzer = EmotionAnalysis()
