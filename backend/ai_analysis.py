import numpy as np
import os
import random
import random
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from config import Config
try:
    from models import EmotionKeyword
except ImportError:
    # Handle possible circular import dynamically if needed, or rely on caller context
    pass

# Try to import TensorFlow/Keras, but provide fallback if not installed
try:
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input
    from tensorflow.keras.utils import to_categorical
    import pandas as pd
    TENSORFLOW_AVAILABLE = True
except ImportError as e:
    TENSORFLOW_AVAILABLE = False
    print(f"Warning: TensorFlow not found. Error: {e}")
    print("Using simple keyword-based sentiment analysis.")

class EmotionAnalysis:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.max_len = 50
        self.vocab_size = 0
        
        # 5 Emotion Classes
        self.classes = ["í–‰ë³µí•´", "í‰ì˜¨í•´", "ê·¸ì €ê·¸ëž˜", "ìš°ìš¸í•´", "í™”ê°€ë‚˜"]
        
        # We will load keywords from DB for fallback/learning
        self.db_engine = create_engine(Config.SQLALCHEMY_DATABASE_URI)
        self.Session = sessionmaker(bind=self.db_engine)

        # Initial Training Data (Hybrid: Hardcoded + GoEmotions)
        self.train_texts = [
            # 0. í–‰ë³µí•´
            "ë„ˆë¬´ ìž¬ë°Œë„¤ìš”", "ìµœê³ ì˜ˆìš”", "ì˜¤ëŠ˜ ì •ë§ í–‰ë³µí–ˆë‹¤", "ê¸°ë¶„ì´ ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤", "ì¦ê±°ìš´ í•˜ë£¨",
            "ë³´ëžŒì°¬ í•˜ë£¨", "ì„±ì·¨ê°ì„ ëŠê¼ˆë‹¤", "ë¿Œë“¯í•˜ë‹¤", "ì‚¬ëž‘í•©ë‹ˆë‹¤", "ì¹œêµ¬ë“¤ê³¼ ì¦ê±°ìš´ ì‹œê°„",
            "ë§›ìžˆëŠ”ê±° ë¨¹ì–´ì„œ ì‹ ë‚œë‹¤", "í•©ê²©í•´ì„œ ê¸°ì˜ë‹¤", "ì„ ë¬¼ì„ ë°›ì•„ì„œ ì¢‹ë‹¤", "ì›ƒìŒì´ ë©ˆì¶”ì§€ ì•ŠëŠ”ë‹¤", "ì‹ ë‚˜ëŠ” ìŒì•…",
            
            # 1. í‰ì˜¨í•´
            "ë§ˆìŒì´ íŽ¸ì•ˆí•˜ë‹¤", "ì¡°ìš©í•œ í•˜ë£¨", "ì—¬ìœ ë¡œìš´ ì €ë…", "ì‚°ì±…ì„ í•˜ë‹ˆ ìƒì¾Œí•˜ë‹¤", "ì°¨ í•œìž”ì˜ ì—¬ìœ ",
            "ì•„ë¬´ ê±±ì • ì—†ì´ ì‰¬ì—ˆë‹¤", "ëª…ìƒì„ í–ˆë‹¤", "ìž ì„ í‘¹ ìž¤ë‹¤", "ìž”ìž”í•œ ìŒì•…ì„ ë“¤ì—ˆë‹¤", "í‰í™”ë¡­ë‹¤",
            "ë”°ëœ»í•œ í–‡ì‚´", "ë°”ëžŒì´ ì‹œì›í•˜ë‹¤", "ì±…ì„ ì½ìœ¼ë©° ížë§", "ì°¨ë¶„í•´ì§€ëŠ” ê¸°ë¶„", "ì•ˆì •ëœ ëŠë‚Œ",

            # 2. ê·¸ì €ê·¸ëž˜
            "ê·¸ì € ê·¸ëŸ° í•˜ë£¨", "í‰ë²”í•œ ì¼ìƒ", "íŠ¹ë³„í•œ ì¼ì€ ì—†ì—ˆë‹¤", "ê·¸ëƒ¥ ê·¸ëž¬ë‹¤", "ë¬´ë‚œí•œ í•˜ë£¨",
            "ë³„ì¼ ì—†ì—ˆë‹¤", "ë˜‘ê°™ì€ í•˜ë£¨", "ì§€ë£¨í•˜ì§€ë„ ìž¬ë¯¸ìžˆì§€ë„ ì•Šë‹¤", "ì˜ì˜", "ë³´í†µì´ë‹¤",
            "ê¸€ìŽ„ ìž˜ ëª¨ë¥´ê² ë‹¤", "ë‚˜ì˜ì§€ ì•Šë‹¤", "ì ë‹¹í•˜ë‹¤", "ê·¸ëŸ­ì €ëŸ­", "í• ë§Œ í–ˆë‹¤",

            # 3. ìš°ìš¸í•´
            "ì˜¤ëŠ˜ ë„ˆë¬´ ìŠ¬íŽë‹¤", "ìš°ìš¸í•´ìš”", "ë§ˆìŒì´ ì•„í”„ë‹¤", "ì™¸ë¡­ë‹¤", "ì§€ì¹œë‹¤",
            "ëˆˆë¬¼ì´ ë‚œë‹¤", "íž˜ë“  í•˜ë£¨ì˜€ì–´ìš”", "ì‹¤ìˆ˜í•´ì„œ ì†ìƒí•˜ë‹¤", "í›„íšŒëœë‹¤", "ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ë‹¤",
            "ì¢Œì ˆê°ì„ ëŠê¼ˆë‹¤", "ìƒì²˜ë°›ì•˜ë‹¤", "ê·¸ë¦¬ì›Œìš”", "ë¬´ê¸°ë ¥í•˜ë‹¤", "ê°€ìŠ´ì´ ë‹µë‹µí•˜ë‹¤",

            # 4. í™”ê°€ë‚˜
            "í™”ê°€ ë‚œë‹¤", "ì§œì¦ë‚˜ìš”", "ì •ë§ ì—´ë°›ëŠ”ë‹¤", "ê¸°ë¶„ì´ ë‚˜ì˜ë‹¤", "ì‹¸ì› ë‹¤",
            "ì–µìš¸í•˜ë‹¤", "ë¯¸ì›Œ ì£½ê² ë‹¤", "ë¶„ë…¸ê°€ ì¹˜ë¯¼ë‹¤", "ì–´ì´ì—†ë‹¤", "ìš©ì„œí•  ìˆ˜ ì—†ë‹¤",
            "ë‹µë‹µí•´ì„œ ë¯¸ì¹˜ê² ë‹¤", "ì‹ ê²½ì§ˆì´ ë‚œë‹¤", "í­ë°œí•  ê²ƒ ê°™ë‹¤", "ê¸°ë¶„ ìž¡ì³¤ë‹¤", "ë§ë„ ì•ˆ ëœë‹¤"
        ]
        
        # Hardcoded Labels
        labels_list = [0]*15 + [1]*15 + [2]*15 + [3]*15 + [4]*15
        self.train_labels = np.array(labels_list)

        # Fallback Comment Bank
        self.comment_bank = {
            "í–‰ë³µí•´": [
                "ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ í–‰ë³µí•˜ì…¨êµ°ìš”! ì´ ê¸ì •ì ì¸ ì—ë„ˆì§€ê°€ ë‚´ì¼ë„ ì´ì–´ì§€ê¸¸ ë°”ëž„ê²Œìš”. ðŸ˜Š",
                "ë“£ê¸°ë§Œ í•´ë„ ê¸°ë¶„ì´ ì¢‹ì•„ì§€ëŠ” ì´ì•¼ê¸°ë„¤ìš”! í–‰ë³µí•œ ìˆœê°„ì„ ì˜¤ëž˜ ê°„ì§í•˜ì„¸ìš”.",
                "ì›ƒìŒì´ ê°€ë“í•œ í•˜ë£¨ì˜€ë„¤ìš”. ë‚´ì¼ë„ ì´ë ‡ê²Œ ì›ƒì„ ì¼ì´ ë§Žì•˜ìœ¼ë©´ ì¢‹ê² ì–´ìš”!",
                "ì •ë§ ë©‹ì§„ í•˜ë£¨ì˜€êµ°ìš”! ìŠ¤ìŠ¤ë¡œì—ê²Œ ì¹­ì°¬ í•œë§ˆë”” í•´ì£¼ì„¸ìš”. ðŸ‘",
                "í–‰ë³µì€ ì „ì—¼ëœë‹¤ê³  í•˜ì£ . ë‹¹ì‹ ì˜ í–‰ë³µì´ ì£¼ë³€ê¹Œì§€ ë°ê²Œ ë¹„ì¶œ ê±°ì˜ˆìš”.",
                "ê¸°ë¶„ ì¢‹ì€ ì—ë„ˆì§€ê°€ ê°€ë“í•˜ë„¤ìš”! ë§›ìžˆëŠ” ê±° ë“œì‹œë©´ì„œ ì˜¤ëŠ˜ì„ ê¸°ë…í•´ë³´ì„¸ìš”.",
                "ìµœê³ ì˜ í•˜ë£¨ë¥¼ ë³´ë‚´ì…¨ë„¤ìš”! ìž ë“¤ê¸° ì „ í–‰ë³µí–ˆë˜ ìˆœê°„ì„ ë‹¤ì‹œ ë– ì˜¬ë ¤ë³´ì„¸ìš”.",
                "ì˜¤ëŠ˜ì˜ ì¦ê±°ì›€ì´ ë§ˆìŒì†ì— ì˜¤ëž˜ì˜¤ëž˜ ë‚¨ê¸°ë¥¼ ë°”ë¼ìš”. ðŸ’–",
                "ì„¸ìƒì´ ë‹¹ì‹ ì„ ì¶•ë³µí•˜ëŠ” ë‚ ì´ì—ˆë‚˜ ë´ìš”! ì •ë§ ê¸°ìœ ì†Œì‹ì´ì—ìš”.",
                "í–‰ë³µí•œ ë‹¹ì‹ ì˜ ëª¨ìŠµì„ ë³´ë‹ˆ ì €ë„ ê¸°ë¶„ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤! íŒŒì´íŒ…!"
            ],
            "í‰ì˜¨í•´": [
                "ë§ˆìŒì´ íŽ¸ì•ˆí•˜ë‹¤ë‹ˆ ë‹¤í–‰ì´ì—ìš”. ë”°ëœ»í•œ ì°¨ í•œ ìž”ìœ¼ë¡œ í•˜ë£¨ë¥¼ ë§ˆë¬´ë¦¬í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ðŸµ",
                "ìž”ìž”í•œ í˜¸ìˆ˜ ê°™ì€ ë§ˆìŒì´ë„¤ìš”. ì´ í‰í™”ë¡œì›€ì´ ê³„ì†ë˜ê¸¸ ë°”ë¼ìš”.",
                "ì—¬ìœ ë¡œìš´ í•˜ë£¨ë¥¼ ë³´ë‚´ì…¨êµ°ìš”. ë³µìž¡í•œ ìƒê°ì€ ìž ì‹œ ë‚´ë ¤ë†“ê³  ì‰¬ì–´ê°€ì„¸ìš”.",
                "í‰ë²”í•˜ì§€ë§Œ ì†Œì¤‘í•œ í‰ì˜¨í•¨ì´ë„¤ìš”. ì¢‹ì•„í•˜ëŠ” ìŒì•…ì„ ë“¤ìœ¼ë©° ížë§í•´ë´ìš”. ðŸŽµ",
                "ë§ˆìŒì˜ ì‰¼í‘œê°€ í•„ìš”í•œ ìˆœê°„, ë”± ì ì ˆí•œ íœ´ì‹ì„ ì·¨í•˜ì‹  ê²ƒ ê°™ì•„ìš”.",
                "í‰í™”ë¡œìš´ ë§ˆìŒìœ¼ë¡œ ìž ìžë¦¬ì— ë“¤ ìˆ˜ ìžˆê² ë„¤ìš”. ì¢‹ì€ ê¿ˆ ê¾¸ì„¸ìš”. ðŸŒ™",
                "ì¡°ìš©í•œ í–‰ë³µì´ ê¹ƒë“  í•˜ë£¨ì˜€ë„¤ìš”. ì´ëŸ° ë‚ ë“¤ì´ ìŒ“ì—¬ ì‚¶ì„ ë‹¨ë‹¨í•˜ê²Œ ë§Œë“¤ì–´ìš”.",
                "ìžì—°ìŠ¤ëŸ¬ìš´ íë¦„ì— ëª¸ì„ ë§¡ê¸´ ë‹¹ì‹ , ì°¸ íŽ¸ì•ˆí•´ ë³´ì—¬ìš”.",
                "ê¸´ìž¥ì´ í’€ë¦¬ê³  ë§ˆìŒì´ ë†“ì´ëŠ” ê¸°ë¶„, ì •ë§ ì†Œì¤‘í•˜ì£ .",
                "ì˜¤ëŠ˜ì˜ í‰ì˜¨í•¨ì´ ë‚´ì¼ì„ ì‚´ì•„ê°ˆ íž˜ì´ ë˜ì–´ì¤„ ê±°ì˜ˆìš”."
            ],
            "ê·¸ì €ê·¸ëž˜": [
                "í‰ë²”í•œ í•˜ë£¨ì˜€êµ°ìš”. ë‚´ì¼ì€ ì¢€ ë” íŠ¹ë³„í•œ ì¼ì´ ìƒê¸¸ì§€ë„ ëª°ë¼ìš”! íŒŒì´íŒ… ðŸ’ª",
                "ë³„ì¼ ì—†ëŠ” í•˜ë£¨ë„ ì†Œì¤‘í•˜ì£ . ë¬´íƒˆí•˜ê²Œ ë³´ë‚¸ ê²ƒì— ê°ì‚¬í•´ë´ìš”.",
                "ë•Œë¡œëŠ” ìž”ìž”í•œ í•˜ë£¨ê°€ ê°€ìž¥ í° íœ´ì‹ì´ ë˜ê¸°ë„ í•œë‹µë‹ˆë‹¤.",
                "ì‹¬ì‹¬í•œ ë‚ ì´ì—ˆë‹¤ë©´, ë‚´ì¼ì€ ìž‘ì€ ëª¨í—˜ì„ ê³„íší•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ê·¸ì € ê·¸ëŸ° ë‚ ë„ ì§€ë‚˜ê³  ë³´ë©´ ì¶”ì–µì´ ë  ê±°ì˜ˆìš”. íŽ¸ì•ˆí•œ ë°¤ ë³´ë‚´ì„¸ìš”.",
                "íŠ¹ë³„í•œ ì¼ì€ ì—†ì—ˆì§€ë§Œ, ë‹¹ì‹ ì€ ì˜¤ëŠ˜ë„ ë‹¹ì‹ ì˜ ìžë¦¬ë¥¼ ìž˜ ì§€ì¼°ì–´ìš”.",
                "ë¬´ë‚œí•œ í•˜ë£¨ì˜€ë„¤ìš”. ë‚´ì¼ì€ ì¢‹ì•„í•˜ëŠ” ê°„ì‹ì„ ë¨¹ìœ¼ë©° ê¸°ë¶„ì„ ì „í™˜í•´ë³¼ê¹Œìš”?",
                "ì˜¤ëŠ˜ì€ ìž ì‹œ ì‰¬ì–´ê°€ëŠ” íŽ˜ì´ì§€ë¼ê³  ìƒê°í•´ìš”. ë‚´ì¼ì€ ë˜ ë‹¤ë¥¸ ì´ì•¼ê¸°ê°€ ì“°ì¼ ê±°ì˜ˆìš”.",
                "ê°ì •ì˜ ê¸°ë³µ ì—†ì´ í‰íƒ„í•œ í•˜ë£¨, ê·¸ê²ƒë§Œìœ¼ë¡œë„ ì¶©ë¶„ížˆ ê´œì°®ì•„ìš”.",
                "ë‚´ì¼ì€ ì˜ˆìƒì¹˜ ëª»í•œ ì¦ê±°ì›€ì´ ê¸°ë‹¤ë¦¬ê³  ìžˆì„ì§€ë„ ëª°ë¼ìš”!"
            ],
            "ìš°ìš¸í•´": [
                "ë§Žì´ íž˜ë“œì…¨êµ°ìš”. ì˜¤ëŠ˜ í•˜ë£¨ëŠ” í‘¹ ì‰¬ë©´ì„œ ìžì‹ ì„ í† ë‹¥ì—¬ì£¼ì„¸ìš”. ë‹¹ì‹ ì€ ì†Œì¤‘í•œ ì‚¬ëžŒìž…ë‹ˆë‹¤. ðŸ’™",
                "ë§ˆìŒì´ ë¬´ê±°ìš´ ë‚ ì´ë„¤ìš”. ìš¸ê³  ì‹¶ë‹¤ë©´ ì‹¤ì»· ìš¸ì–´ë„ ê´œì°®ì•„ìš”. ì œê°€ ê³ì— ìžˆì„ê²Œìš”.",
                "ê´œì°®ì§€ ì•Šì•„ë„ ê´œì°®ì•„ìš”. ì˜¤ëŠ˜ì€ ë¬´ë¦¬í•˜ì§€ ë§ê³  ìžê¸° ìžì‹ ë§Œ ìƒê°í•˜ì„¸ìš”.",
                "ë‹¹ì‹ ì˜ ìŠ¬í””ì´ ê¹Šì€ ë§Œí¼, ë‹¹ì‹ ì€ ë”°ëœ»í•œ ë§ˆìŒì„ ê°€ì§„ ì‚¬ëžŒì¼ ê±°ì˜ˆìš”.",
                "ì–´ë‘ìš´ ë°¤ì´ ì§€ë‚˜ë©´ ë°˜ë“œì‹œ í•´ê°€ ëœ¹ë‹ˆë‹¤. ìž ì‹œ ì›…í¬ë ¤ ìžˆì–´ë„ ê´œì°®ì•„ìš”.",
                "íž˜ë“  í•˜ë£¨ë¥¼ ë²„í…¨ë‚¸ ë‹¹ì‹ , ì •ë§ ê³ ìƒ ë§Žì•˜ì–´ìš”. ë”°ëœ»í•œ ì´ë¶ˆ ì†ì—ì„œ í‘¹ ì£¼ë¬´ì„¸ìš”.",
                "ë§ˆìŒì˜ ë¹„ê°€ ê·¸ì¹˜ê¸°ë¥¼ ê¸°ë‹¤ë¦´ê²Œìš”. í˜¼ìžë¼ê³  ìƒê°í•˜ì§€ ë§ˆì„¸ìš”.",
                "ì§€ê¸ˆ ëŠë¼ëŠ” ê°ì •ë„ ë‹¹ì‹ ì˜ ì¼ë¶€ì˜ˆìš”. ë¶€ì •í•˜ì§€ ë§ê³  ê°€ë§Œížˆ ì•ˆì•„ì£¼ì„¸ìš”.",
                "ë§›ìžˆëŠ” ê±°ë¼ë„ ë¨¹ê³  ê¸°ìš´ ì°¨ë¦¬ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”. ë‚´ì¼ì€ ì¡°ê¸ˆ ë” ë‚˜ì•„ì§ˆ ê±°ì˜ˆìš”.",
                "ë‹¹ì‹ ì€ í˜¼ìžê°€ ì•„ë‹ˆì—ìš”. íž˜ë“  ìˆœê°„ì´ ì§€ë‚˜ê°€ê¸¸ í•¨ê»˜ ì‘ì›í• ê²Œìš”."
            ],
            "í™”ê°€ë‚˜": [
                "ì†ìƒí•œ ì¼ì´ ìžˆìœ¼ì…¨ë‚˜ ë´ìš”. ìž ì‹œ ì‹¬í˜¸í¡ì„ í•˜ë©° ë§ˆìŒì„ ê°€ë¼ì•‰í˜€ë³´ë©´ ì–´ë–¨ê¹Œìš”? íž˜ë‚´ì„¸ìš”! ðŸ”¥",
                "ì •ë§ í™”ê°€ ë‚  ë§Œí•œ ìƒí™©ì´ì—ˆêµ°ìš”. ê·¸ ê°ì •ì„ ì–µëˆ„ë¥´ì§€ ë§ê³  ê±´ì „í•˜ê²Œ í’€ì–´ë³´ì„¸ìš”.",
                "ì—´ë°›ëŠ” í•˜ë£¨ì˜€ë„¤ìš”! ì‹œì›í•œ ë¬¼ í•œ ìž” ë§ˆì‹œê³  í„¸ì–´ë²„ë¦¬ì„¸ìš”.",
                "ëˆ„êµ¬ë¼ë„ í™”ê°€ ë‚¬ì„ ê±°ì˜ˆìš”. ë‹¹ì‹  ìž˜ëª»ì´ ì•„ë‹ˆë‹ˆ ë„ˆë¬´ ìžì±…í•˜ì§€ ë§ˆì„¸ìš”.",
                "ë¶„ë…¸ëŠ” ì—ë„ˆì§€ê°€ ë  ìˆ˜ë„ ìžˆì–´ìš”. ìš´ë™ì´ë‚˜ ì·¨ë¯¸ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë‚ ë ¤ë²„ë ¤ìš”! ðŸ¥Š",
                "ë§Žì´ ì–µìš¸í•˜ì…¨ì£ . ë‹¹ì‹ ì˜ ë§ˆìŒ ë‹¤ ì´í•´í•´ìš”.",
                "í™”ê°€ ë‚  ë•ŒëŠ” ìž ì‹œ ê·¸ ìƒí™©ì—ì„œ ë²—ì–´ë‚˜ í™˜ê¸°ë¥¼ ì‹œí‚¤ëŠ” ê²Œ ë„ì›€ì´ ë¼ìš”.",
                "ì˜¤ëŠ˜ì˜ ë‚˜ìœ ê¸°ë¶„ì€ ì˜¤ëŠ˜ë¡œ ëë‚´ë²„ë ¤ìš”. ë‚´ì¼ì€ ê¸°ë¶„ ì¢‹ì€ ì¼ë§Œ ìžˆì„ ê±°ì˜ˆìš”.",
                "ì†Œë¦¬ë¼ë„ í•œ ë²ˆ í¬ê²Œ ì§€ë¥´ê³  ì‹¶ë„¤ìš”! ë‹µë‹µí•œ ë§ˆìŒì´ ì¡°ê¸ˆì€ í’€ë¦¬ê¸¸ ë°”ë¼ìš”.",
                "ë‹¹ì‹ ì˜ í‰í™”ë¥¼ ë°©í•´í•œ ê²ƒë“¤ì´ ë°‰ë„¤ìš”. ì˜¤ëŠ˜ì€ ì¼ì° ì‰¬ë©´ì„œ ë§ˆìŒì„ ë‹¤ìŠ¤ë ¤ë´ìš”."
            ]
        }
        
        # Initialize attributes
        self.comment_tokenizer = None
        self.comment_model = None
        self.enc_model = None 
        self.dec_model = None
        self.comment_max_len = 20

        if TENSORFLOW_AVAILABLE:
            print("Initializing AI Emotion Analysis Model (5-Class LSTM)...")
            self.tokenizer = Tokenizer()
            
            if os.environ.get('SKIP_TRAINING'):
                print("Skipping training logic (SKIP_TRAINING active).")
                return
            
            # --- Load GoEmotions Data & Map Labels ---
            try:
                base_dir = os.path.dirname(os.path.abspath(__file__))
                csv_path = os.path.join(base_dir, 'goemotions_korean_train.csv')
                
                if os.path.exists(csv_path):
                    print(f"Loading extended dataset from {csv_path}...")
                    df = pd.read_csv(csv_path)
                    
                    # Ensure strings
                    df['text'] = df['text'].astype(str)
                    
                    # Mapping Dictionary (28 -> 5)
                    # 0: í–‰ë³µí•´, 1: í‰ì˜¨í•´, 2: ê·¸ì €ê·¸ëž˜, 3: ìš°ìš¸í•´, 4: í™”ê°€ë‚˜
                    # GoEmotions: 0-27
                    label_map = {
                        0: 0, 1: 0, 5: 0, 13: 0, 15: 0, 17: 0, 18: 0, 20: 0, 21: 0, 23: 0, # Happpy...
                        4: 1, 8: 1, 22: 1, # Calm/Positive
                        6: 2, 7: 2, 26: 2, 27: 2, # Neutral/Ambiguous
                        9: 3, 12: 3, 16: 3, 19: 3, 24: 3, 25: 3, # Depressed/Sad
                        2: 4, 3: 4, 10: 4, 11: 4, 14: 4 # Angry/Fees/Disgust
                    }
                    
                    # Function to map labels
                    def map_emotion(label_str):
                        try:
                            # Labels in CSV might be "0", or "2,15" (multilabel)
                            # We take the first label for simplicity in this single-label project
                            first_label = int(str(label_str).split(',')[0])
                            return label_map.get(first_label, 2) # Default to Neutral if map fails
                        except:
                            return 2

                    df['target'] = df['labels'].apply(map_emotion)
                    
                    # Merge with hardcoded data
                    new_texts = df['text'].tolist()
                    new_labels = df['target'].tolist()
                    
                    self.train_texts.extend(new_texts)
                    # Combine numpy arrays
                    self.train_labels = np.concatenate((self.train_labels, np.array(new_labels)))
                    
                    print(f"Total training samples: {len(self.train_texts)}")
                else:
                    print("GoEmotions CSV not found. Using small hardcoded dataset.")
            except Exception as e:
                print(f"Error loading GoEmotions: {e}. Using small hardcoded dataset.")

            self._train_initial_model()
            
            # Setup for Comment Generation (Lazy load or init)
            # ... (Existing code)
            
            print("AI Model initialized.")
        else:
            print("Initializing Fallback Emotion Analysis (Keyword based - 5 classes)...")

    def _train_initial_model(self):
        # Increased Vocab size for larger dataset
        self.tokenizer.fit_on_texts(self.train_texts)
        self.vocab_size = len(self.tokenizer.word_index) + 1
        print(f"Emotion Vocab Size: {self.vocab_size}")
        
        sequences = self.tokenizer.texts_to_sequences(self.train_texts)
        X_train = pad_sequences(sequences, maxlen=self.max_len)
        y_train = self.train_labels
        
        self.model = Sequential()
        self.model.add(Embedding(self.vocab_size, 128, input_length=self.max_len)) 
        self.model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) # Increased capacity & dropout
        self.model.add(Dense(64, activation='relu')) 
        self.model.add(Dropout(0.3))
        self.model.add(Dense(5, activation='softmax')) 
        
        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
        # Train for fewer epochs if dataset is huge, or more? 
        # 40k samples -> 5 epochs is enough for a prototype to see convergence without waiting too long
        print("Training Emotion Classifier...")
        self.model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=1) 


    def predict(self, text):
        if not text: return "ë¶„ì„ ë¶ˆê°€"

        if TENSORFLOW_AVAILABLE and self.model:
            try:
                sequences = self.tokenizer.texts_to_sequences([text])
                padded = pad_sequences(sequences, maxlen=self.max_len)
                prediction = self.model.predict(padded)[0]
                predicted_class_idx = np.argmax(prediction)
                confidence = prediction[predicted_class_idx]
                predicted_label = self.classes[predicted_class_idx]
                return f"{predicted_label} ({(confidence * 100):.1f}%)"
            except Exception as e:
                print(f"Prediction error: {e}")
                return self._fallback_predict(text)
        else:
            return self._fallback_predict(text)

    def _fallback_predict(self, text):
        # Load keywords from DB dynamically
        session = self.Session()
        try:
            # Fetch all keywords
            # Optimized: Could cache this and update periodically, but for now fetch on specific calls if not heavy
            # Actually, fetching all keywords every time is slow. Let's do a simple query or cache.
            # Since this is a prototype, fetching is fine for small DB.
            from models import EmotionKeyword # Import here to avoid circular dep
            
            keywords = session.query(EmotionKeyword).all()
            
            scores = [0] * 5
            found_any = False
            
            for kw in keywords:
                if kw.keyword in text:
                    scores[kw.emotion_label] += kw.frequency
                    found_any = True
            
            if found_any:
                max_score = max(scores)
                total_score = sum(scores)
                confidence = (max_score / total_score * 100) if total_score > 0 else 85.0
                best_idx = scores.index(max_score)
                return f"{self.classes[best_idx]} ({confidence:.1f}%)"
            else:
                return "ê·¸ì €ê·¸ëž˜ (40.0%)" 
        except Exception as e:
            print(f"Fallback error: {e}")
            return "ë¶„ì„ ë¶ˆê°€"
        finally:
            session.close()

    def train_comment_model(self):
        """
        Train a Seq2Seq model using ChatbotData.csv
        """
        if not TENSORFLOW_AVAILABLE: return

        print("Training Comment Generation Model (Seq2Seq)...")
        try:
            import os
            base_dir = os.path.dirname(os.path.abspath(__file__))
            data_path = os.path.join(base_dir, 'ChatbotData.csv')
            df = pd.read_csv(data_path)
            # Limit data for speed in prototype? Or use all. 12k is fine.
            df = df.sample(frac=1).reset_index(drop=True) # Shuffle
            
            questions = df['Q'].astype(str).tolist()
            answers = df['A'].apply(lambda x: 'sos ' + str(x) + ' eos').tolist()
            
            # Shared Tokenizer
            self.comment_tokenizer = Tokenizer()
            self.comment_tokenizer.fit_on_texts(questions + answers)
            
            vocab_size = len(self.comment_tokenizer.word_index) + 1
            print(f"Vocab Size: {vocab_size}")

            # Encoder Data
            tokenized_Q = self.comment_tokenizer.texts_to_sequences(questions)
            encoder_input_data = pad_sequences(tokenized_Q, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Data
            tokenized_A = self.comment_tokenizer.texts_to_sequences(answers)
            decoder_input_data = pad_sequences(tokenized_A, maxlen=self.comment_max_len, padding='post')
            
            # Decoder Target (Shifted-by-one)
            # Use the padded decoder_input_data to ensure consistent length
            decoder_target_data = np.zeros_like(decoder_input_data, dtype="float32")
            decoder_target_data[:, :-1] = decoder_input_data[:, 1:]
            
            decoder_target_data = np.expand_dims(decoder_target_data, -1)

            # Model Architecture
            latent_dim = 256
            
            # Encoder
            encoder_inputs = Input(shape=(None,))
            enc_emb = Embedding(vocab_size, latent_dim)(encoder_inputs)
            encoder_lstm = LSTM(latent_dim, return_state=True)
            encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)
            encoder_states = [state_h, state_c]
            
            # Decoder
            decoder_inputs = Input(shape=(None,))
            dec_emb_layer = Embedding(vocab_size, latent_dim)
            dec_emb = dec_emb_layer(decoder_inputs)
            decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
            decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
            decoder_dense = Dense(vocab_size, activation='softmax')
            decoder_outputs = decoder_dense(decoder_outputs)
            
            self.comment_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
            self.comment_model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
            print("Fitting Seq2Seq Model ( This may take time )...")
            # Epochs=20 for better results, but 5 for speed in strict mode plan
            self.comment_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
                                   batch_size=64, epochs=5, validation_split=0.2)
                                   
            print("Comment Model Trained.")
            
            # Store Inference Models
            self.enc_model = Model(encoder_inputs, encoder_states)
            
            dec_state_input_h = Input(shape=(latent_dim,))
            dec_state_input_c = Input(shape=(latent_dim,))
            dec_states_inputs = [dec_state_input_h, dec_state_input_c]
            
            dec_emb2 = dec_emb_layer(decoder_inputs)
            dec_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=dec_states_inputs)
            dec_states2 = [state_h2, state_c2]
            dec_outputs2 = decoder_dense(dec_outputs2)
            
            self.dec_model = Model([decoder_inputs] + dec_states_inputs, [dec_outputs2] + dec_states2)
            
        except Exception as e:
            print(f"Error training comment model: {e}")

    def generate_ai_comment(self, text):
        if not self.enc_model or not self.comment_tokenizer:
            return None
            
        try:
            # Preprocess
            seq = self.comment_tokenizer.texts_to_sequences([text])
            input_seq = pad_sequences(seq, maxlen=self.comment_max_len, padding='post')
            
            # Encode
            states_value = self.enc_model.predict(input_seq)
            
            # Generate
            target_seq = np.zeros((1,1))
            target_seq[0, 0] = self.comment_tokenizer.word_index['sos']
            
            decoded_sentence = ''
            stop_condition = False
            
            while not stop_condition:
                output_tokens, h, c = self.dec_model.predict([target_seq] + states_value)
                
                # Sample a token
                sampled_token_index = np.argmax(output_tokens[0, -1, :])
                sampled_word = self.comment_tokenizer.index_word.get(sampled_token_index, '')
                
                if sampled_word == 'eos' or len(decoded_sentence) > 50:
                    stop_condition = True
                else:
                    decoded_sentence += ' ' + sampled_word
                
                # Update target seq
                target_seq = np.zeros((1, 1))
                target_seq[0, 0] = sampled_token_index
                
                # Update states
                states_value = [h, c]
                
            return decoded_sentence.strip()
            
        except Exception as e:
            print(f"Gen Error: {e}")
            return None

    def generate_comment(self, prediction_text):
        """
        Generate a supportive comment.
        Priority: 1. AI Generation (Seq2Seq) 2. Random Selection (Fallback)
        """
        if not prediction_text or "ë¶„ì„ ë¶ˆê°€" in prediction_text:
            return "ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“¤ë ¤ì£¼ì„¸ìš”. í•­ìƒ ë“£ê³  ìžˆì„ê²Œìš”."

        try:
            label = prediction_text.split()[0] # e.g. "í–‰ë³µí•´"
            
            # 1. Try AI Generation
            # We want to feed the *label* or the *original text*?
            # Ideally the original diary text. But here we only have prediction_text.
            # However, `prediction_text` is just "Label score".
            # The prompt implies generating comment based on "prediction" or "diary"?
            # Function signature is `generate_comment(self, prediction_text)`.
            # We will use the 'Label' as the input prompt to the chatbot model (simple approach)
            # OR we can assume we might pass the full text later.
            # For now, let's use the Label as the input. e.g. "ë‚˜ ì§€ê¸ˆ í–‰ë³µí•´" (simulated)
            
            ai_generated = None
            if self.enc_model:
                # Synthesize a prompt from label
                prompt = f"ë‚˜ ì§€ê¸ˆ {label}" 
                ai_generated = self.generate_ai_comment(prompt)
                
            if ai_generated and len(ai_generated) > 2:
                return f"{ai_generated} (AI)"
            
            # 2. Fallback (Random Selection)
            if label in self.comment_bank:
                return random.choice(self.comment_bank[label])
            else:
                return "ë‹¹ì‹ ì˜ ê°ì •ì„ ì†Œì¤‘ížˆ ê°„ì§í•˜ì„¸ìš”."
        except Exception as e:
            print(f"Comment Gen Error: {e}")
            return "ë‹¹ì‹ ì˜ ê°ì •ì„ ì†Œì¤‘ížˆ ê°„ì§í•˜ì„¸ìš”."

    def update_keywords(self, text, mood_level):
        """
        Learn new keywords from the text based on the user's provided mood_level.
        mood_level: 1-5 (User input)
        Mapping: 5->0(Happy), 4->1(Calm), 3->2(Neutral), 2->3(Depressed), 1->4(Angry)
        """
        if not text: return

        # Map mood_level to label
        # mood_level is expected to be int
        try:
            mood_val = int(mood_level)
            mapping = {5: 0, 4: 1, 3: 2, 2: 3, 1: 4}
            target_label = mapping.get(mood_val)
            
            if target_label is None:
                return # Invalid mood level
        except:
            return

        session = self.Session()
        try:
            from models import EmotionKeyword
            
            # Simple Tokenization (Space-based)
            # In a real KR app, use Mecab/Konlpy. Here we split by space.
            words = text.split()
            
            for w in words:
                # Basic cleaning
                w = w.strip('.,?!~"\'')
                if len(w) < 2: continue # Skip single chars
                
                # Check if exists
                existing = session.query(EmotionKeyword).filter_by(keyword=w).first()
                
                if existing:
                    # If exists, increment frequency if label matches
                    # If label differs, maybe decrease freq or ignore? 
                    # Let's just track co-occurrence. Complex logic omitted for simplicity.
                    if existing.emotion_label == target_label:
                        existing.frequency += 1
                else:
                    # NEW WORD -> LEARN IT!
                    print(f"Learning new keyword: {w} -> {self.classes[target_label]}")
                    new_kw = EmotionKeyword(
                        keyword=w,
                        emotion_label=target_label,
                        frequency=1
                    )
                    session.add(new_kw)
            
            session.commit()
            
            # If Model is active, we might want to update tokenizer or retrain eventually.
            # Rerunning Tokenizer.fit_on_texts would be needed. 
            # For this session, we won't retrain the LSTM live (too slow), but the Fallback logic will immediately benefit.
            
        except Exception as e:
            print(f"Learning error: {e}")
            session.rollback()
        finally:
            session.close()

# Singleton instance
ai_analyzer = EmotionAnalysis()
