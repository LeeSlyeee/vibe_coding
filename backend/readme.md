# Mood Diary AI 백엔드

무드 다이어리(Mood Diary) 애플리케이션을 위한 Flask 기반 백엔드로, 고급 감정 분석 및 공감형 AI 코멘트 생성 기능을 특징으로 합니다.

## 🚀 주요 기능

### 1. 감정 분석 (LSTM)

- **모델**: 128 유닛을 가진 양방향 LSTM (Bidirectional LSTM).
- **클래스**: 5가지 감정 (행복해, 평온해, 그저그래, 우울해, 화가나).
- **훈련 데이터**: 약 47,000개의 결합된 데이터셋.
  - `GoEmotions-Korean`: 50,000개 이상의 라벨링된 문장 (Google).
  - `감성대화말뭉치`: 36,000개 이상의 대화 쌍 (AI Hub).
  - `User Data`: 10,000개 이상의 실제 일기 데이터 (파인 튜닝).
- **성능**: 저장된 모델(`emotion_model.h5`)을 통해 높은 정확도 제공.

### 2. 생성형 AI 코멘트 (고급 하이브리드 시스템)

- **시스템**: 3단계 우선순위 아키텍처.
  1.  **안전장치 (규칙 기반)**: "시험", "이별"과 같은 중요 키워드에 대해 즉각적이고 높은 품질의 조언 제공.
  2.  **KoGPT-2 (LLM)**: SKT의 `kogpt2-base-v2` 모델을 사용하여 창의적이고 시적인 답변 생성.
  3.  **Seq2Seq (대체 모델)**: 경량 백업 모델.
- **기능**: 사용자 일기를 바탕으로 공감하고 맥락을 파악하여 조언 생성.
- **최적화**: 효율적인 한국어 토큰화를 위해 `PreTrainedTokenizerFast` 사용.
- **파일**: `comment_bank.json` (규칙), `comment_model.h5` (레거시).

### 3. 능동형 학습 및 키워드 추출

- **키워드 추출**: 사용자 일기에서 감정별 키워드를 자동으로 학습.
- **지식 베이스**: `EmotionKeyword` 테이블에 현재 **30,000개 이상의 학습된 키워드** 보유.
- **지속적 학습**: 일기 데이터가 쌓일수록 시스템의 성능이 향상됨.

---

## 🛠 설정 및 실행

### A. 환경 설정

```bash
cd backend
source venv/bin/activate
```

### B. 서버 실행

```bash
python app.py
```

- 서버는 `http://127.0.0.1:5000`에서 시작됩니다.
- API 문서: `app.py`의 `/api/` 엔드포인트를 참조하세요.

### C. 스크립트를 통한 AI 관리

- **전체 모델 재훈련**: `python retrain_model.py` (강제 재훈련)
- **기존 데이터 일괄 업데이트**: `python batch_update_ai.py` (기존 일기에 새로운 AI 분석 적용)
- **코멘트 검증**: `python verify_comments.py`

## 📁 프로젝트 구조

- `ai_analysis.py`: 핵심 AI 로직 (훈련, 추론, 지속성 관리).
- `app.py`: Flask REST API 라우트.
- `models.py`: SQLAlchemy 데이터베이스 모델 (`User`, `Diary`, `EmotionKeyword`).
- `*.h5 / *.pickle`: 최적화된 AI 모델 파일.

## 🔄 최근 업데이트 (2026.01.15)

### 1. AI 모델 업그레이드 (Polyglot-Ko 1.3B)

- 기존 **KoGPT-2** (1.2억 파라미터)에서 **Polyglot-Ko 1.3B** (13억 파라미터)로 교체했습니다.
- **향상된 점**: 문맥 이해력 대폭 증가, 더 자연스럽고 풍부한 한국어 표현 가능.

### 2. 프롬프트 엔지니어링 강화 (Persona 적용)

- AI에게 **"다정하고 공감 능력이 뛰어난 심리 상담사"** 페르소나를 부여했습니다.
- 단순한 응답을 넘어, 사용자의 일기 내용에 깊이 공감하고 위로하는 2~3문장의 답변을 생성하도록 최적화되었습니다.

### 3. 클라우드 배포 최적화 (Oracle Cloud)

- **메모리 최적화**: 대형 언어 모델(LLM) 구동을 위한 메모리 관리 로직 적용.
- **안전성**: `SKIP_TRAINING=true` 환경에서도 필수 AI 데이터(감정 은행 등)는 로딩되도록 수정.
- **타임아웃 설정**: LLM 추론 시간을 고려하여 Gunicorn 타임아웃을 **120초**로 확장.

### 4. Gemini API & OCI 성능 최적화 (2026.01.17 ~ 2026.01.18)

- **통합 분석 시스템 (Ultra Fast Mode)**:
  - 기존 로컬 LSTM 분석 대신 **Gemini 1.5 Flash**를 이용한 통합 분석(감정 분류 + 코멘트 생성)을 도입했습니다.
  - **성능 향상**: 분석 대기 시간을 약 **15초에서 3초 미만**으로 대폭 단축했습니다.
  - **OCI 최적화 (RAM 다이어트)**: Gemini 모드 활성화 시 메모리를 많이 점유하는 로컬 딥러닝 라이브러리(TensorFlow, PyTorch) 로딩을 차단하여 100명 이상의 동시 접속 환경에서도 안정적인 운영이 가능하도록 개선했습니다.
- **데이터 기반 통계 리포트 (Insight v2)**:
  - **AI 우선 감정 시스템**: 통계 차트에서 사용자 선택보다 AI가 객관적으로 분석한 감정 점수를 최우선으로 반영하여 더 정교한 분석 리포트를 제공합니다.
  - **KST 타임존 보정**: 서버 시간(UTC)과 상관없이 한국 표준시(+9시간) 기준으로 정확한 날짜별 통계를 산출합니다.
- **히스토리 패턴 기반 날씨 조언**:
  - 단순히 현재 날씨만 언급하는 것이 아니라, 사용자의 **과거 데이터(동일 날씨에서의 감정 패턴)**를 분석하여 "지난번 흐린 날에는 이런 감정을 느끼셨는데, 오늘은 어떠세요?"와 같은 초개인화된 인사이트를 제공합니다.
- **프라이버시 실드(Privacy Shield)**:
  - 외부 API 전송 전 이메일, 전화번호 등 민감 정보를 자동으로 마스킹하고, 최소한의 맥락 정보만 추출하여 보안을 강화했습니다.
- **설정 및 실행**:
  - `.env` 파일에 `GEMINI_API_KEY` 설정 시 최적화 모드가 자동 활성화됩니다.
  - 추천 실행 방식 (OCI): `nohup ./venv/bin/python app.py > app.log 2>&1 &`

### 5. 로컬 AI & OCI 성능 최적화 (2026.01.18)

- **로컬 LLM 도입 (Gemma 2 - 2B)**:
  - 구글의 최신 경량화 모델인 **Gemma 2 (2B)**를 로컬 서버(Ollama)에 탑재했습니다.
  - **장점**: 외부 API 의존성 완전 제거, 데이터 프라이버시 100% 보장.
  - **비용**: 구글 API 과금 및 Quota 제한 없이 **평생 무료/무제한** 사용 가능.
  - **속도**: 9B 모델 대비 4배 빠른 추론 속도 (OCI ARM 서버 기준 1분 내외).

- **배치 처리 시스템 (Resumable Batch)**:
  - 대량의 일기 데이터를 안정적으로 분석하기 위한 **'이어하기(Rescue)'** 기능 구현.
  - 서버 중단/재시작 시에도 이미 분석된 데이터는 건너뛰고 남은 데이터만 효율적으로 처리.
  - **작동 방식**: 백그라운드(`nohup`)에서 실행하며, 처리된 항목은 `task_id`로 추적.

- **설정 및 실행**:
  - **필수**: 서버에 `Ollama` 설치 및 `ollama pull gemma2:2b` 필요.
  - **실행**: `./venv/bin/python batch_gemini_update.py` (배치 모드)
  - **운영**: API 키 불필요, 오직 서버의 로컬 컴퓨팅 파워만 사용.
