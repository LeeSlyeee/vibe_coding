#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mood Diary AI - Model Retraining Script

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ìŒ“ì¸ ì‚¬ìš©ì ì¼ê¸° ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬
ê°ì • ë¶„ì„ LSTM ëª¨ë¸ì„ ì¬í›ˆë ¨í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    cd backend
    source venv/bin/activate
    python retrain_model.py

ê¸°ëŠ¥:
    1. DBì—ì„œ ëª¨ë“  ì¼ê¸° ë°ì´í„° ë¡œë“œ (event + emotion_desc + self_talk)
    2. ê¸°ì¡´ ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ì™€ ê²°í•©
    3. LSTM ëª¨ë¸ ì¬í›ˆë ¨
    4. ëª¨ë¸ ì €ì¥ (emotion_model.h5, tokenizer.pickle)
"""

import os
import sys
import numpy as np

# Flask app contextë¥¼ ìœ„í•œ ì„¤ì •
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app import app, db
from models import Diary
from ai_analysis import EmotionAnalysis

def main():
    print("=" * 60)
    print("ğŸ”„ Mood Diary AI - ëª¨ë¸ ì¬í›ˆë ¨ ì‹œì‘")
    print("=" * 60)
    
    with app.app_context():
        # 1. DBì—ì„œ ì¼ê¸° ë°ì´í„° í™•ì¸
        total_diaries = db.session.query(Diary).count()
        print(f"\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ:")
        print(f"   - ì´ ì¼ê¸° ê°œìˆ˜: {total_diaries}ê°œ")
        
        if total_diaries < 10:
            print("\nâš ï¸  ê²½ê³ : ì¼ê¸° ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ (10ê°œ ë¯¸ë§Œ).")
            print("   ëª¨ë¸ ì¬í›ˆë ¨ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 100ê°œ ì´ìƒì˜ ì¼ê¸°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            response = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() != 'y':
                print("ì¬í›ˆë ¨ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
        
        # 2. AI ë¶„ì„ê¸° ì´ˆê¸°í™” (ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ)
        print("\nğŸ¤– AI ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        # Prevent auto-training in __init__ since we are doing manual retraining
        os.environ['SKIP_TRAINING'] = '1'
        ai = EmotionAnalysis()
        del os.environ['SKIP_TRAINING'] # Clean up
        
        # 3. ê¸°ì¡´ ëª¨ë¸ ë°±ì—…
        import shutil
        from datetime import datetime
        
        backup_dir = os.path.join(os.path.dirname(__file__), 'model_backups')
        os.makedirs(backup_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        model_path = ai.model_path
        tokenizer_path = ai.tokenizer_path
        
        if os.path.exists(model_path):
            backup_model = os.path.join(backup_dir, f'emotion_model_{timestamp}.h5')
            shutil.copy2(model_path, backup_model)
            print(f"âœ… ê¸°ì¡´ ëª¨ë¸ ë°±ì—…: {backup_model}")
        
        if os.path.exists(tokenizer_path):
            backup_tokenizer = os.path.join(backup_dir, f'tokenizer_{timestamp}.pickle')
            shutil.copy2(tokenizer_path, backup_tokenizer)
            print(f"âœ… ê¸°ì¡´ í† í¬ë‚˜ì´ì € ë°±ì—…: {backup_tokenizer}")
        
        # 4. DB ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¼ê¸° ë¡œë“œ ì¤‘...")
        db_texts, db_labels = ai.load_db_data()
        
        if not db_texts:
            print("âŒ DBì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(db_texts)}ê°œì˜ ì¼ê¸° ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # 5. ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ ë¡œë“œ
        print("\nğŸ“š ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ ë¡œë“œ ì¤‘...")
        # Reset internal buffers to ensure clean load (in case they were used)
        ai.train_texts = []
        ai.train_labels = np.array([])
        
        ai.load_sentiment_corpus()
        
        corpus_texts = ai.train_texts
        # Convert numpy array to list for concatenation
        corpus_labels = ai.train_labels.tolist() if hasattr(ai.train_labels, 'tolist') else list(ai.train_labels)
        
        if corpus_texts:
            print(f"âœ… {len(corpus_texts)}ê°œì˜ ì½”í¼ìŠ¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        else:
            print("âš ï¸  ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DB ë°ì´í„°ë§Œìœ¼ë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.")
        
        # 6. ë°ì´í„° ê²°í•©
        all_texts = corpus_texts + db_texts
        all_labels = corpus_labels + db_labels
        
        print(f"\nğŸ“Š ìµœì¢… í›ˆë ¨ ë°ì´í„°:")
        print(f"   - ì½”í¼ìŠ¤: {len(corpus_texts)}ê°œ")
        print(f"   - ì‚¬ìš©ì ì¼ê¸°: {len(db_texts)}ê°œ")
        print(f"   - ì´í•©: {len(all_texts)}ê°œ")
        
        # 7. ëª¨ë¸ ì¬í›ˆë ¨
        print("\nğŸ”¥ ëª¨ë¸ ì¬í›ˆë ¨ ì‹œì‘...")
        print("   (ì´ ê³¼ì •ì€ 5-10ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        print("-" * 60)
        
        try:
            # ê¸°ì¡´ _train_initial_modelì„ ì§ì ‘ í˜¸ì¶œí•˜ë˜, ë°ì´í„°ë¥¼ ì „ë‹¬
            ai._train_with_data(all_texts, all_labels)
            
            print("-" * 60)
            print("âœ… ëª¨ë¸ ì¬í›ˆë ¨ ì™„ë£Œ!")
            print(f"   - ëª¨ë¸ íŒŒì¼: {model_path}")
            print(f"   - í† í¬ë‚˜ì´ì €: {tokenizer_path}")
            
            # Update state for app.py to prevent redundant auto-training
            try:
                current_kw = ai._get_keyword_count()
                ai._save_training_state(current_kw)
                print(f"âœ… Training state updated (Count: {current_kw})")
            except Exception as e:
                print(f"âš ï¸ Warning: perform state update failed: {e}")
            
        except Exception as e:
            print(f"\nâŒ ì¬í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ê¸°ì¡´ ë°±ì—… íŒŒì¼ì„ ë³µì›í•˜ì‹œë ¤ë©´ model_backups í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        # 8. ëª¨ë¸ ê²€ì¦
        print("\nğŸ§ª ëª¨ë¸ ê²€ì¦ ì¤‘...")
        test_texts = [
            "ì˜¤ëŠ˜ ì‹œí—˜ì— ë–¨ì–´ì ¸ì„œ ë„ˆë¬´ ìŠ¬í”„ë‹¤",
            "ì¹œêµ¬ë“¤ê³¼ ë†€ëŸ¬ê°€ì„œ ì •ë§ í–‰ë³µí–ˆì–´",
            "ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ê³  ë¬´ê¸°ë ¥í•´"
        ]
        
        for text in test_texts:
            result = ai.predict(text)
            print(f"   ì…ë ¥: {text}")
            print(f"   â†’ ì˜ˆì¸¡: {result}\n")
        
        print("=" * 60)
        print("ğŸ‰ ì¬í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("=" * 60)
        print("\nğŸ’¡ Tip: ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ë©´ ìƒˆ ëª¨ë¸ì´ ì ìš©ë©ë‹ˆë‹¤.")
        print("   $ pkill -f app.py && python app.py")


if __name__ == "__main__":
    main()
