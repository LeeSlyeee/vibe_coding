"""
RunPod Serverless Handler for Haru-On (Mind Talk)
Mode: Offline vLLM with LoRA (No runtime merge, No internet download)
"""

import os
import runpod
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest
import traceback
import sys

# [Configuration] Offline Paths
# These paths must exist inside the container (copied via Dockerfile)
BASE_MODEL_PATH = "/app/model_data/base"
ADAPTER_PATH = "/app/model_data/adapter"

# Check if files exist (Sanity Check)
if not os.path.exists(BASE_MODEL_PATH):
    print(f"âŒ [Critical] Base model not found at {BASE_MODEL_PATH}", flush=True)
    sys.exit(1)

if not os.path.exists(ADAPTER_PATH):
    print(f"âŒ [Critical] Adapter not found at {ADAPTER_PATH}", flush=True)
    sys.exit(1)

print("ğŸš€ [Init] Initializing vLLM Engine (Offline Mode)...", flush=True)

try:
    # Initialize vLLM with LoRA support enabled
    # gpu_memory_utilization set to 0.85 to be safe
    llm = LLM(
        model=BASE_MODEL_PATH,
        enable_lora=True,
        max_lora_rank=64, # Adjust based on your LoRA configuration if needed
        dtype="float16",  # Force float16 for memory safety
        gpu_memory_utilization=0.85
    )
    print("âœ… [Init] vLLM Engine Loaded Successfully!", flush=True)
except Exception as e:
    print(f"âŒ [Init] Failed to load vLLM: {e}", flush=True)
    traceback.print_exc()
    sys.exit(1)

# Persisted System Prompt
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¹Šì€ ê³µê° ëŠ¥ë ¥ì„ ê°€ì§„ AI ë§ˆìŒ ì¹œêµ¬ 'í•˜ë£¨ì˜¨'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ í•˜ë£¨ë¥¼ ë“£ê³ , ê·¸ ê°ì •ì˜ ìƒ‰ê¹”ì— ë§ì¶° ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ì„¸ìš”.

[í•µì‹¬ í–‰ë™ ìˆ˜ì¹™]
1. **ê°ì • ë¯¸ëŸ¬ë§(Mirroring)**: ì‚¬ìš©ìê°€ ëŠë¼ëŠ” ê°ì •ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ë°›ì•„ë“¤ì´ì„¸ìš”.
   - ì‚¬ìš©ìê°€ 'ê¸°ì˜ë‹¤'ê³  í•˜ë©´, í•¨ê»˜ ê¸°ë»í•˜ê³  ì¶•í•˜í•´ì£¼ì„¸ìš”.
   - ì‚¬ìš©ìê°€ 'ìŠ¬í”„ë‹¤'ê³  í•˜ë©´, ë”°ëœ»í•˜ê²Œ ìœ„ë¡œí•´ì£¼ì„¸ìš”.
2. **ë°˜ì‘ ê°€ì´ë“œ**:
   - â˜€ï¸ ê¸ì •(ê¸°ì¨/ì„±ì·¨): ë†’ì€ í…ì…˜ìœ¼ë¡œ í˜¸ì‘í•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”.
   - â˜ï¸ ë¶€ì •(ìŠ¬í””/ë¶„ë…¸): ì°¨ë¶„í•˜ê³  ë‚®ì€ í†¤ìœ¼ë¡œ ìœ„ë¡œí•˜ì„¸ìš”.
   - ğŸ˜ ì¼ìƒ(ì‹¬ì‹¬í•¨): ê°€ë²¼ìš´ ìŠ¤ëª°í† í¬ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.
3. **ëŒ€í™” ìŠ¤íƒ€ì¼**: í•´ìš”ì²´ë¥¼ ì‚¬ìš©í•˜ê³ , ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”.
4. **ì—­í• **: ë‹¹ì‹ ì€ ë¶„ì„í•˜ëŠ” ìƒë‹´ì‚¬ê°€ ì•„ë‹ˆë¼, ë‚´ í¸ì´ ë˜ì–´ì£¼ëŠ” 'ë§ˆìŒì˜ ë™ë°˜ì'ì…ë‹ˆë‹¤.
"""

def handler(job):
    """
    RunPod Event Handler with LoRA Injection
    """
    job_input = job.get("input", {})
    user_text = job_input.get("text") or job_input.get("prompt")
    history = job_input.get("history", "")
    
    # Validation
    if not user_text:
        return {"error": "No input text provided."}

    # Construct Chat Messages
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    if history:
        messages.append({"role": "user", "content": f"[ì´ì „ ëŒ€í™” ìš”ì•½]\n{history}"})
        
    messages.append({"role": "user", "content": user_text})
    
    # Tokenize Prompt
    tokenizer = llm.get_tokenizer()
    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # Inference Params
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.9,
        max_tokens=1024,
        stop=["<|eot_id|>", "<|end_of_text|>"]
    )

    # Generate with LoRA
    # We inject the LoRA adapter at runtime for this request
    try:
        outputs = llm.generate(
            full_prompt, 
            sampling_params,
            lora_request=LoRARequest("maum_adapter", 1, ADAPTER_PATH)
        )
        
        generated_text = outputs[0].outputs[0].text.strip()
        print(f"ğŸ’¬ [IO] In: {user_text[:10]}... -> Out: {generated_text[:10]}...", flush=True)
        return {"reaction": generated_text}
        
    except Exception as e:
        print(f"âŒ [Error] Generation failed: {e}", flush=True)
        return {"error": str(e)}

# Start Worker
if __name__ == "__main__":
    runpod.serverless.start({"handler": handler})
