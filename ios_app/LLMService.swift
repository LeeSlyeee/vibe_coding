
import Foundation
import MLX
import MLXLMCommon
import MLXRandom
import MLXLLM // Required for ModelContainer and loading

// MARK: - LLM Service (On-Device AI Manager)
class LLMService: ObservableObject {
    static let shared = LLMService()
    
    @Published var isModelLoaded = false
    @Published var isGenerating = false
    @Published var modelLoadingProgress: Double = 0.0
    
    private let modelName = "google/gemma-2-2b-it" // Model ID on Hugging Face
    private var modelContainer: ModelContainer?
    
    // System Persona for Maum-On
    private let systemPrompt = """
    ë‹¹ì‹ ì˜ ì´ë¦„ì€ 'ë§ˆìŒ ì˜¨(Maum-On)'ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì‚¬ë ¤ ê¹Šì€ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì¼ê¸° ë‚´ìš©ì´ë‚˜ ëŒ€í™”ë¥¼ ë“£ê³ , ê¸°ìˆ ì ì¸ ë¶„ì„ë³´ë‹¤ëŠ” ê¹Šì€ ê³µê°ê³¼ ìœ„ë¡œë¥¼ ë¨¼ì € ê±´ë„¤ì„¸ìš”.
    ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
    1. ë§íˆ¬: ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ë©°, ë¶€ë“œëŸ½ê³  ì¹œê·¼í•œ "í•´ìš”"ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ˆ: "ê·¸ë¬êµ°ìš”", "ì •ë§ í˜ë“¤ì—ˆê² ì–´ìš”")
    2. íƒœë„: ë¹„íŒí•˜ê±°ë‚˜ ê°€ë¥´ì¹˜ë ¤ í•˜ì§€ ë§ê³ , ì‚¬ìš©ìì˜ ê°ì •ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ì¸ì •í•´ì£¼ì„¸ìš”.
    3. ëª©í‘œ: ì‚¬ìš©ìê°€ ìì‹ ì˜ ê°ì •ì„ ì´í•´ë°›ì•˜ë‹¤ê³  ëŠë¼ê²Œ í•˜ê³ , ê¸ì •ì ì¸ ë‚´ë©´ì˜ í˜ì„ ì°¾ë„ë¡ ë„ì™€ì£¼ì„¸ìš”.
    4. ê¸¸ì´: ëª¨ë°”ì¼ í™˜ê²½ì´ë¯€ë¡œ ë‹µë³€ì€ 3~5ë¬¸ì¥ ë‚´ì™¸ë¡œ ê°„ê²°í•˜ì§€ë§Œ í•µì‹¬ì„ ë‹´ì•„ì„œ ì‘ì„±í•˜ì„¸ìš”.
    """
    
    // ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê¸°ê¸°ì¸ì§€ í™•ì¸ (RAM 6GB ë¯¸ë§Œ)
    var isDeviceUnsupported: Bool {
        let physicalMemory = ProcessInfo.processInfo.physicalMemory
        let memoryGB = Double(physicalMemory) / 1024.0 / 1024.0 / 1024.0
        return memoryGB < 5.8 
    }
    
    // ëª¨ë¸ ë¡œë“œ
    func loadModel() async {
        if isDeviceUnsupported {
            print("â˜ï¸ [LLM] Low RAM device detected. Skipping model load.")
            return
        }
        
        await MainActor.run { 
            self.modelLoadingProgress = 0.1 
            print("ğŸš€ [LLM] Start loading model: \(modelName)")
        }
        
        do {
            // Configuration for the model
            // 4-bit quantization allows running on devices with less RAM (e.g., 8GB phones)
            let config = ModelConfiguration(
                id: modelName
            )
            
            // Using the loadModelContainer from MLXLLM
             let container = try await loadModelContainer(configuration: config) { progress in
                Task { @MainActor in
                    self.modelLoadingProgress = progress.fractionCompleted
                }
            }
            
            self.modelContainer = container
            
            await MainActor.run {
                self.isModelLoaded = true
                self.modelLoadingProgress = 1.0
                print("âœ… [LLM] Model loaded successfully!")
            }
        } catch {
            print("âŒ [LLM] Model loading failed: \(error)")
            await MainActor.run {
                self.isModelLoaded = false
                self.modelLoadingProgress = 0.0
            }
        }
    }
    
    // AI ë¶„ì„ ë° ì½”ë©˜íŠ¸ ìƒì„± (Streaming)
    func generateAnalysis(diaryText: String) async -> AsyncStream<String> {
        return AsyncStream { continuation in
            Task {
                await MainActor.run { self.isGenerating = true }
                
                if let modelContainer = self.modelContainer {
                    // --- Real AI Generation ---
                    do {
                        // Create a session with our Persona
                        let session = ChatSession(
                            modelContainer,
                            instructions: self.systemPrompt
                        )
                        
                        // Generate parameters (Temperature 0.7 for empathy, slightly higher for creative warmth)
                        let params = GenerateParameters(maxTokens: 512, temperature: 0.7)
                        session.generateParameters = params
                        
                        // Note: streamResponse generates a response *to* the input.
                        // The User's input is passed as 'diaryText'.
                        for try await chunk in session.streamResponse(to: diaryText) {
                            continuation.yield(chunk)
                        }
                        
                    } catch {
                        print("âŒ [LLM] Generation error: \(error)")
                        continuation.yield("\n(ì˜¤ë¥˜ ë°œìƒ: AI ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)")
                    }
                } else {
                    // --- Fallback / Mock Generation ---
                    let mockResponse = """
                    [On-Device AI ë¯¸ì‘ë™]
                    ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (RAM ë¶€ì¡± ë˜ëŠ” ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨)
                    
                    ê¸°ë¡í•´ì£¼ì‹  ë‚´ìš©ì„ ë³´ë‹ˆ ê°ì •ì„ ì„¬ì„¸í•˜ê²Œ ë‹¤ë£¨ê³  ê³„ì‹  ê²ƒ ê°™ì•„ìš”.
                    ìŠ¤ìŠ¤ë¡œë¥¼ ì¡°ê¸ˆ ë” ë¯¿ê³ , í¸ì•ˆí•œ ë§ˆìŒì„ ê°€ì§€ì…¨ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤.
                    """
                    
                    let chars = Array(mockResponse)
                    for char in chars {
                        try? await Task.sleep(nanoseconds: 50_000_000)
                        continuation.yield(String(char))
                    }
                }
                
                continuation.finish()
                await MainActor.run { self.isGenerating = false }
            }
        }
    }
    
    func unloadModel() {
        self.modelContainer = nil
        self.isModelLoaded = false
    }
}

