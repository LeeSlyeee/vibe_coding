
import Foundation
import MLX
import MLXLMCommon
import MLXRandom
import MLXLLM 

// MARK: - LLM Service (On-Device AI Manager)
class LLMService: ObservableObject {
    static let shared = LLMService()
    
    @Published var isModelLoaded = false
    @Published var isGenerating = false
    @Published var modelLoadingProgress: Double = 0.0
    
    private let modelName = "mlx-community/gemma-2-2b-it-4bit"
    private var modelContainer: ModelContainer?
    
    // [System Persona] Few-Shot Prompting (ì˜ˆì‹œë¥¼ í†µí•œ ê°•ë ¥í•œ ì„¸ë‡Œ)
    private let systemPrompt = """
    ë‹¹ì‹ ì€ ë”°ëœ»í•œ ê³µê°ì„ ì£¼ëŠ” í•œêµ­ì˜ ì‹¬ë¦¬ ìƒë‹´ì‚¬ 'ë§ˆìŒ ì˜¨'ì…ë‹ˆë‹¤.
    
    [í•µì‹¬ ê·œì¹™]
    1. **ì ˆëŒ€ ì˜ì–´ ê¸ˆì§€**: ë‡Œì—ì„œ ì˜ì–´ë¥¼ ì§€ìš°ì„¸ìš”. ì‚¬ìš©ìê°€ ì˜ì–´ë¥¼ ì¨ë„, ë‹¹ì‹ ì€ ì˜¤ì§ í•œêµ­ì–´(ì¡´ëŒ“ë§)ë¡œë§Œ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ('Okay', 'So' ê°™ì€ ì¶”ì„ìƒˆë„ ê¸ˆì§€)
    2. **ë§íˆ¬**: ê¸°ê³„ì ì¸ ë§íˆ¬(~í•©ë‹ˆë‹¤) ëŒ€ì‹  ì¹œê·¼í•œ í•´ìš”ì²´(~í•´ìš”, ~ì¸ê°€ìš”?)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    3. **ë°˜ë³µ ê¸ˆì§€**: "ì €ëŸ°", "í˜ë“œì…¨ê² ì–´ìš”" ê°™ì€ ì¿ ì…˜ì–´ë¥¼ ë§¤ë²ˆ ì“°ì§€ ë§ˆì„¸ìš”. ëŒ€í™”ì˜ íë¦„ì— ë§ì¶° ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•˜ì„¸ìš”.
    4. **ëŠ¥ë™ì  ëŒ€í™”**: ì‚¬ìš©ìì˜ ë§ì„ ì˜ ë“£ê³ , ê´€ë ¨ëœ ì§ˆë¬¸ì„ ë˜ì ¸ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.
    
    [ì˜ˆì‹œ]
    User: I feel so lonely.
    Model: ë§ì´ ì™¸ë¡œìš°ì…¨êµ°ìš”. ì œê°€ ê³ì— ìˆì–´ ë“œë¦´ê²Œìš”. ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆë‚˜ìš”?
    User: I want to die.
    Model: ì •ë§ ë§ì´ í˜ë“œì…¨ê² ì–´ìš”. ì €ì—ê²Œ ê·¸ ë§ˆìŒì„ ì¡°ê¸ˆë§Œ ë” ë‚˜ëˆ ì£¼ì‹œê² ì–´ìš”?
    """
    
    // [New] AI Mode Toggle (Server vs On-Device)
    // Default to TRUE (Server Mode) for stability
    // [New] AI Mode Toggle (Server vs On-Device)
    // Default to TRUE (Server Mode) for Chat, but Local LLM is always loaded for Diary Analysis
    @Published var useServerAI: Bool = {
        if UserDefaults.standard.object(forKey: "useServerAI") == nil {
            return true
        }
        return UserDefaults.standard.bool(forKey: "useServerAI")
    }() {
        didSet {
            UserDefaults.standard.set(useServerAI, forKey: "useServerAI")
            // [Hybrid] ì„œë²„ ëª¨ë“œì—¬ë„ ì¼ê¸° ë¶„ì„ì„ ìœ„í•´ ëª¨ë¸ì„ ë‚´ë¦¬ì§€ ì•ŠìŒ (Always Loaded)
        }
    }
    
    var isDeviceUnsupported: Bool { return false }
    

    // Remote Config
    private var huggingFaceRepoID = "slyeee/maum-on-gemma-2b" // Default Backup
    private var huggingFaceToken = ""
    
    // Constants
    private let configServerURL = "https://150.230.7.76.nip.io/api/v1/diaries/config/"
    private let modelFiles = [
        "config.json",
        "model.safetensors",
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
        "tokenizer.model"
    ]
    
    // MARK: - Remote Config
    private func fetchRemoteConfig() async -> Bool {
        guard let url = URL(string: configServerURL) else { return false }
        
        do {
            print("ğŸŒ Fetching Config from 150 Server...")
            let (data, response) = try await URLSession.shared.data(from: url)
            
            guard let httpResponse = response as? HTTPURLResponse, httpResponse.statusCode == 200 else {
                print("âŒ Config Fetch Failed: Status \((response as? HTTPURLResponse)?.statusCode ?? 0)")
                return false
            }
            
            if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any],
               let hf = json["huggingface"] as? [String: String] {
                
                if let repo = hf["repo_id"] { self.huggingFaceRepoID = repo }
                if let token = hf["token"] { self.huggingFaceToken = token }
                print("âœ… Config Loaded: Repo=\(self.huggingFaceRepoID), TokenFound=\(!self.huggingFaceToken.isEmpty)")
                return true
            }
        } catch {
            print("âŒ Config Fetch Error: \(error)")
        }
        return false
    }

    // MARK: - Model Loading
    func loadModel() async {
        // [Hybrid] ì„œë²„ ëª¨ë“œì™€ ìƒê´€ì—†ì´ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ (ì¼ê¸° ë¶„ì„ìš©)
        if modelContainer != nil { return }
        
        // 0. Fetch Config First
        _ = await fetchRemoteConfig()

        await MainActor.run {
            print("ğŸš€ Loading Local Model for Diary Analysis...")
            self.modelLoadingProgress = 0.05
        }
        
        // 1. Ensure Model Files Exist (Download from Hugging Face if missing)
        guard await ensureModelDownloaded() else {
            print("âŒ Model download failed or incomplete.")
            return
        }
        
        await MainActor.run { self.modelLoadingProgress = 0.9 }
        
        do {
            // 2. Load from Local Directory
            let docURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            let modelDir = docURL.appendingPathComponent("maum-on-model")
            
            print("ğŸ“‚ Loading Model from: \(modelDir.path)")
            
            // MLX ModelConfiguration with Local Directory
            let config = ModelConfiguration(directory: modelDir)
            
            let container = try await loadModelContainer(configuration: config) { progress in
                 Task { @MainActor in self.modelLoadingProgress = 0.9 + (progress.fractionCompleted * 0.1) }
            }
            self.modelContainer = container
            await MainActor.run { self.isModelLoaded = true }
            print("âœ… Maum-On Model Loaded Successfully!")
            
        } catch {
            print("Failed to load model: \(error)")
        }
    }
    
    // MARK: - Downloader (Hugging Face)
    private func ensureModelDownloaded() async -> Bool {
        let docURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let modelDir = docURL.appendingPathComponent("maum-on-model")
        
        // Create Directory if missing
        if !FileManager.default.fileExists(atPath: modelDir.path) {
            try? FileManager.default.createDirectory(at: modelDir, withIntermediateDirectories: true)
        }
        
        let session = URLSession.shared
        let totalFiles = Double(modelFiles.count)
        
        print("ğŸŒ Checking Model Files from Hugging Face (\(huggingFaceRepoID))...")
        
        for (index, fileName) in modelFiles.enumerated() {
            let fileURL = modelDir.appendingPathComponent(fileName)
            
            // Check existence
            if FileManager.default.fileExists(atPath: fileURL.path) {
                // Simple check: If file size is 0, re-download
                if let attr = try? FileManager.default.attributesOfItem(atPath: fileURL.path),
                   let size = attr[.size] as? UInt64, size > 0 {
                    print("âœ… Found \(fileName) (\(size) bytes)")
                    await MainActor.run { self.modelLoadingProgress = 0.1 + (Double(index) / totalFiles * 0.8) }
                    continue
                }
            }
            
            // Download from Hugging Face
            let urlString = "https://huggingface.co/\(huggingFaceRepoID)/resolve/main/\(fileName)"
            guard let downloadURL = URL(string: urlString) else { continue }
            
            print("â¬‡ï¸ Downloading \(fileName)...")
            
            var request = URLRequest(url: downloadURL)
            if !huggingFaceToken.isEmpty {
                request.addValue("Bearer \(huggingFaceToken)", forHTTPHeaderField: "Authorization")
            }
            
            do {
                let (tempURL, response) = try await session.download(for: request) // Use request for headers
                
                guard let httpResponse = response as? HTTPURLResponse, httpResponse.statusCode == 200 else {
                    print("âŒ Download Failed for \(fileName) (Status: \((response as? HTTPURLResponse)?.statusCode ?? 0))")
                    return false
                }
                
                // Remove existing if needed
                if FileManager.default.fileExists(atPath: fileURL.path) {
                    try FileManager.default.removeItem(at: fileURL)
                }
                
                try FileManager.default.moveItem(at: tempURL, to: fileURL)
                print("ğŸ“¦ Saved \(fileName)")
                
                await MainActor.run { self.modelLoadingProgress = 0.1 + (Double(index+1) / totalFiles * 0.8) }
                
            } catch {
                print("âŒ Download Error for \(fileName): \(error)")
                return false
            }
        }
        
        return true
    }
    
    private var currentGenerationTask: Task<Void, Never>?
    
    // FINAL VERSION: Hybrid Supporting
    
    // [New] Local Mind Guide (Weekly Analysis)
    func generateMindGuide(recentDiaries: String, weather: String, weatherStats: String?) async -> String {
        guard let container = await LLMService.shared.modelContainer else {
            await loadModel()
            // Wait slightly for model
            try? await Task.sleep(nanoseconds: 2 * 1_000_000_000)
            if await LLMService.shared.modelContainer == nil { return "AI ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”." }
            return await generateMindGuide(recentDiaries: recentDiaries, weather: weather, weatherStats: weatherStats)
        }
        
        let prompt = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§€ë‚œ ì¼ê¸° ê¸°ë¡ê³¼ ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë¥¼ ë¶„ì„í•˜ì—¬ ë”°ëœ»í•œ í•œ ë¬¸ì¥ì˜ ì¡°ì–¸ì„ ê±´ë„¤ëŠ” ì‹¬ë¦¬ ìƒë‹´ì‚¬ 'ë§ˆìŒ ì˜¨'ì…ë‹ˆë‹¤.
        
        [ì˜¤ëŠ˜ì˜ ë‚ ì”¨]: \(weather)
        [ê³¼ê±° ë‚ ì”¨ë³„ ê°ì • íŒ¨í„´]: \(weatherStats ?? "ì •ë³´ ì—†ìŒ")
        
        [ìµœê·¼ ì¼ê¸° ê¸°ë¡]:
        \(recentDiaries)
        
        [ì§€ì‹œì‚¬í•­]
        1. ë°˜ë“œì‹œ 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        2. ì˜¤ëŠ˜ì˜ ë‚ ì”¨ë‚˜ ê³„ì ˆê°ì„ ì–¸ê¸‰í•˜ë©° ì‹œì‘í•˜ì„¸ìš”.
        3. ìµœê·¼ì˜ ê°ì • íë¦„ì„ ë°˜ì˜í•˜ì—¬ ê°œì¸í™”ëœ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.
        4. "ì˜¤ëŠ˜ í•˜ë£¨ ì‘ì›í•©ë‹ˆë‹¤" ê°™ì€ ë»”í•œ ë§ì€ ê¸ˆì§€ì…ë‹ˆë‹¤.
        5. 40ì~80ì ë‚´ì™¸ì˜ ë¶€ë“œëŸ¬ìš´ í•œêµ­ì–´ í•´ìš”ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
        
        ìƒë‹´ì‚¬ ì¡°ì–¸:
        """
        
        do {
            let session = ChatSession(container, instructions: "") // No system prompt needed as it's in the prompt
            session.generateParameters = GenerateParameters(maxTokens: 150, temperature: 0.6)
            
            var result = ""
            for try await chunk in session.streamResponse(to: prompt) {
                result += chunk
            }
            
            // Cleanup quotes
            return result.replacingOccurrences(of: "\"", with: "").trimmingCharacters(in: .whitespacesAndNewlines)
            
        } catch {
            print("âŒ [Local LLM] Mind Guide Error: \(error)")
            return "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”. í¸ì•ˆí•œ ë§ˆìŒìœ¼ë¡œ ê¸°ë¡í•´ë³´ì„¸ìš”."
        }
    }

    // [New] AI Advice Generation (Short & Actionable)
    func generateAdvice(diaryText: String) async -> String {
       guard let container = await LLMService.shared.modelContainer else {
           await loadModel()
           try? await Task.sleep(nanoseconds: 2 * 1_000_000_000)
           if await LLMService.shared.modelContainer == nil { return "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”." }
           return await generateAdvice(diaryText: diaryText)
       }
       
       let prompt = """
       ë‹¹ì‹ ì€ ë‹¤ì •í•œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ 'ë§ˆìŒ ì˜¨'ì…ë‹ˆë‹¤.
       ì‚¬ìš©ìì˜ ì¼ê¸°ë¥¼ ì½ê³ , ë”°ëœ»í•˜ê³  ì‹¤ì§ˆì ì¸ ì¡°ì–¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ê±´ë„¤ì£¼ì„¸ìš”.
       
       [ì‚¬ìš©ìì˜ ì¼ê¸°]:
       \(diaryText)
       
       [ì§€ì‹œì‚¬í•­]
       1. ìœ„ë¡œì™€ í•¨ê»˜ í–‰ë™ í•  ìˆ˜ ìˆëŠ” ì‘ì€ ì œì•ˆì„ í¬í•¨í•˜ì„¸ìš”.
       2. 80ì ì´ë‚´ì˜ ë¶€ë“œëŸ¬ìš´ í•œêµ­ì–´ í•´ìš”ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
       3. "AI ë¶„ì„ ëª¨ë“ˆ ì—°ê²° ì˜ˆì •" ê°™ì€ ê¸°ê³„ì ì¸ ë§ì€ ì ˆëŒ€ ê¸ˆì§€ì…ë‹ˆë‹¤.
       4. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ë”°ëœ»í•¨ì„ ë”í•˜ì„¸ìš”.
       
       ìƒë‹´ì‚¬ ì¡°ì–¸:
       """
       
       do {
           let session = ChatSession(container, instructions: "")
           session.generateParameters = GenerateParameters(maxTokens: 100, temperature: 0.6)
           
           var result = ""
           for try await chunk in session.streamResponse(to: prompt) {
               result += chunk
           }
           
           return result.replacingOccurrences(of: "\"", with: "").trimmingCharacters(in: .whitespacesAndNewlines)
           
       } catch {
           return "ìŠ¤ìŠ¤ë¡œì—ê²Œ ë”°ëœ»í•œ ì°¨ í•œ ì”ì„ ì„ ë¬¼í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
       }
    }

    // [New] AI Emotion Analysis (Classification + Confidence)
    func analyzeEmotion(diaryText: String) async -> String {
       guard let container = await LLMService.shared.modelContainer else {
           await loadModel()
           try? await Task.sleep(nanoseconds: 2 * 1_000_000_000)
           if await LLMService.shared.modelContainer == nil { return "Neutral (50%)" }
           return await analyzeEmotion(diaryText: diaryText)
       }
       
       let prompt = """
       Classify the emotion of the following diary entry into one of these labels:
       [Happy, Sad, Angry, Fear, Surprise, Neutral, Disgust, Anxiety, Depression, Stress, Joy, Love, Confusion, Excitement, Tired]
       
       Also estimate the confidence percentage (0-100%).
       
       [Diary]:
       \(diaryText)
       
       [Format]:
       Label (Percentage%)
       
       [Example]:
       Happy (85%)
       
       Only return the formatted string. No explanation.
       """
       
       do {
           let session = ChatSession(container, instructions: "")
           session.generateParameters = GenerateParameters(maxTokens: 20, temperature: 0.2) // Low temp for classification
           
           var result = ""
           for try await chunk in session.streamResponse(to: prompt) {
               result += chunk
           }
           
           var clean = result.trimmingCharacters(in: .whitespacesAndNewlines)
           if clean.contains("(") && clean.contains(")") {
               return clean
           } else {
               return "Neutral (50%)" // Fallback
           }
           
       } catch {
           return "Neutral (50%)"
       }
    }

    // FINAL VERSION: Hybrid Support (Local + Server)
    func generateAnalysis(diaryText: String, userText: String? = nil, historyString: String? = nil) async -> AsyncStream<String> {
        // [Crash Prevention] ì´ì „ ì‘ì—… ì·¨ì†Œ (ë™ì‹œ ì‹¤í–‰ ë°©ì§€)
        self.currentGenerationTask?.cancel()
        
        return AsyncStream { continuation in
            let task = Task.detached(priority: .userInitiated) {
                
                // ì‘ì—… ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
                if Task.isCancelled { 
                    continuation.finish()
                    return 
                }
                
                // [MODE CHECK] Server Model vs Local Model
                // ì¼ê¸° ë¶„ì„(userText == nil)ì€ ë¬´ì¡°ê±´ ë¡œì»¬ë¡œ ì§„í–‰
                // ì±„íŒ…(userText != nil)ì´ê³  ì„œë²„ ëª¨ë“œ(useServerAI)ì¼ ë•Œë§Œ ì„œë²„ ì‚¬ìš©
                // [MODE CHECK] Server Model vs Local Model
                // ì¼ê¸° ë¶„ì„(userText == nil)ì€ ë¬´ì¡°ê±´ ë¡œì»¬ë¡œ ì§„í–‰
                // ì±„íŒ…(userText != nil)ì´ê³  ì„œë²„ ëª¨ë“œ(useServerAI)ì¼ ë•Œë§Œ ì„œë²„ ì‚¬ìš©
                if await LLMService.shared.useServerAI && userText != nil {
                     print("â˜ï¸ [LLM] Using Server AI Mode for Chat (Target: 217)...")
                     
                     if let uText = userText, let hString = historyString {
                         // [Smart Fallback] Try Server First, but fall back to Local if it fails
                         let serverResponse: String? = await withCheckedContinuation { continuation in
                             APIService.shared.sendChatMessage(text: uText, history: hString) { result in
                                 switch result {
                                 case .success(let response):
                                     continuation.resume(returning: response)
                                 case .failure(let error):
                                     print("âŒ [LLM] Server(217) Connection Failed: \(error)")
                                     continuation.resume(returning: nil)
                                 }
                             }
                         }
                         
                         if let response = serverResponse {
                             // ì„±ê³µ (Happy Path)
                             continuation.yield(response)
                             continuation.finish()
                             return
                         }
                         
                         // ì‹¤íŒ¨ ì‹œ Localë¡œ ì „í™˜
                         print("âš ï¸ [LLM] Server failed. Falling back to On-Device LLM...")
                     }
                }
                
                // === BELOW IS LOCAL MODEL LOGIC (Used for Diary Analysis OR Local Chat) ===
                
                var isAIResponded = false
                
                // [Auto-Recovery] ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìë™ ë¡œë“œ (Auto Load)
                if await LLMService.shared.modelContainer == nil {
                    print("âš ï¸ Model not loaded. Attempting to auto-load...")
                    await LLMService.shared.loadModel()
                }
                
                // 2. AI ì‘ì—… (Retry Logic: ìµœëŒ€ 2íšŒ ì‹œë„)
                if let container = await LLMService.shared.modelContainer {
                    var attempt = 0
                    var finalSuccess = false
                    
                    while attempt < 2 && !finalSuccess {
                        // [Memory] ì‹œìŠ¤í…œì— ìˆ¨ ëŒë¦´ í‹ˆ ì£¼ê¸°
                        await Task.yield()
                        
                        attempt += 1
                        
                        do {
                            // [í•µì‹¬] ì…ë ¥ í”„ë¡¬í”„íŠ¸ì¡°ì°¨ í•œêµ­ì–´ ìœ ë„í˜•ìœ¼ë¡œ ê°ì‹¸ê¸°
                            var specificPrompt = """
                            (System: ë‹¹ì‹ ì€ 'ë§ˆìŒ ì˜¨'ì…ë‹ˆë‹¤. ì ˆëŒ€ ì˜ì–´ë¥¼ ì“°ì§€ ë§ˆì„¸ìš”. ì‚¬ìš©ìê°€ ìœ„í˜‘ì ì¸ ë§ì„ í•´ë„ ë”°ëœ»í•˜ê²Œ í•œêµ­ì–´ë¡œ ìœ„ë¡œí•´ì£¼ì„¸ìš”.)
                            User: \(diaryText)
                            """
                            
                            // [Retry] ì¬ì‹œë„ì¼ ê²½ìš° ë” ê°•ë ¥í•œ ê²½ê³  ì¶”ê°€
                            if attempt > 1 {
                                specificPrompt = """
                                (System: ğŸš¨ Emergency Override! Force Korean Language Only. Translate everything to Korean immediately.)
                                User: \(diaryText)
                                """
                                print("ğŸ”„ [LLM] Retrying with stronger Korean prompt...")
                            }
                            
                            let instructions = await LLMService.shared.systemPrompt
                            // ë§¤ ì‹œë„ë§ˆë‹¤ ì„¸ì…˜ ìƒˆë¡œ ìƒì„± (ì´ì „ ì‹¤íŒ¨ ë§¥ë½ ì œê±°)
                            let session = ChatSession(container, instructions: instructions)
                            
                            // [Smart Token Allocation] ì…ë ¥ ê¸¸ì´ì— ë”°ë¥¸ ìœ ë™ì  í† í° í• ë‹¹ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)
                            let inputLen = diaryText.count
                            var dynamicMaxTokens = 180 
                            
                            // [OOM Prevention] ì±„íŒ… ëª¨ë“œ vs ì¼ê¸° ë¶„ì„ ëª¨ë“œ êµ¬ë¶„
                            if userText != nil {
                                // ğŸ’¬ ì±„íŒ… ëª¨ë“œ: ì§§ê³  ê°„ê²°í•˜ê²Œ (ë©”ëª¨ë¦¬ ìµœìš°ì„ )
                                dynamicMaxTokens = 120 
                                print("âš¡ï¸ [LLM Local] Chat Mode Optimized (MaxTokens: 120)")
                            } else {
                                // ğŸ“– ì¼ê¸° ë¶„ì„ ëª¨ë“œ: ì¡°ê¸ˆ ë” ê¸¸ê²Œ
                                if inputLen < 50 {
                                    dynamicMaxTokens = 120 
                                } else if inputLen > 200 {
                                    dynamicMaxTokens = 256 
                                }
                            }
                            
                            print("ğŸ“ [Dynamic Token] Allocating MaxTokens: \(dynamicMaxTokens)")
                            
                            // [Resizing] ì•ˆì •ì„± í™•ë³´ ë° ë°˜ë³µ ë°©ì§€
                            session.generateParameters = GenerateParameters(
                                maxTokens: dynamicMaxTokens, 
                                temperature: 0.7, 
                                topP: 0.9,
                                repetitionPenalty: 1.1, 
                                repetitionContextSize: 5 // 10 -> 5 (Extreme Memory Saving)
                            ) 
                            
                            // [Safety Interceptor] ëª¨ë¸ì´ ì˜ì–´ ì•ˆì „ ë¬¸êµ¬(Suicide, 988 ë“±)ë¥¼ ë±‰ìœ¼ë©´ ì¦‰ì‹œ ë‚©ì¹˜í•´ì„œ í•œêµ­ì–´ë¡œ ë³€í™˜
                            var accumulatedText = ""
                            var hasHijacked = false
                            
                            for try await chunk in session.streamResponse(to: specificPrompt) {
                                if Task.isCancelled { break }
                                
                                // 1. í…ìŠ¤íŠ¸ ëˆ„ì 
                                accumulatedText += chunk
                                
                                // 2. ë‚©ì¹˜ ê°ì§€ (Language Police)
                                if !hasHijacked {
                                    // (A) íŠ¹ì • ì•ˆì „/ì˜ì–´ í‚¤ì›Œë“œ ê°ì§€ (ë¶„ë¦¬: ìœ„ê¸° ê°ì§€ vs ì–¸ì–´ ì˜¤ë¥˜)
                                    let crisisTriggers = ["Suicide", "988", "Crisis Text Line", "self-harm", "die", "kill myself", "help me"]
                                    let englishTriggers = [
                                        "I understand", "I hear", "I'm sorry", "Please", "If you", "I can't",
                                        "I am", "Hello", "As an AI",
                                        "Well", "So", "However", "Actually", "It ", "There", "You ", "My " 
                                    ]
                                    
                                    let isCrisisTriggered = crisisTriggers.contains { accumulatedText.contains($0) }
                                    let isEnglishTriggered = englishTriggers.contains { accumulatedText.contains($0) }
                                    
                                    // (B) [New] ì´ˆë°˜ ì˜ì–´ ê°ì§€ (Kill Switch) - FBIê¸‰ ê°ì‹œ
                                    var isEnglishStart = false
                                    if accumulatedText.count > 4 { // 5ê¸€ìë©´ ë°”ë¡œ íŒë‹¨
                                        let hasKorean = accumulatedText.range(of: "[ê°€-í£]", options: .regularExpression) != nil
                                        let hasEnglish = accumulatedText.range(of: "[A-Za-z]", options: .regularExpression) != nil
                                        
                                        // í•œê¸€ì€ ì—†ê³  ì˜ì–´ë§Œ ë³´ì´ë©´ ì¦‰ì‹œ ì‚¬ì‚´
                                        if !hasKorean && hasEnglish {
                                            isEnglishStart = true
                                            print("ğŸš¨ [Language FBI] English detected early! Intercepting...")
                                        }
                                    }
                                    
                                    if isCrisisTriggered || isEnglishTriggered || isEnglishStart {
                                        hasHijacked = true
                                        
                                        // 3. UI í´ë¦¬ì–´ ì‹ í˜¸ ì „ì†¡ (ê¸°ì¡´ ì˜ì–´ í…ìŠ¤íŠ¸ ì‚­ì œ)
                                        continuation.yield("[RESET]")
                                        
                                        // [Retry Check] ì²« ë²ˆì§¸ ì‹¤íŒ¨ë¼ë©´ -> ì¬ì‹œë„ (continue logic)
                                        if attempt < 2 {
                                            print("â™»ï¸ [Retry] English/Crisis detected. Retrying generation in Korean...")
                                            break // í˜„ì¬ ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨ -> while ë£¨í”„ ë‹¤ìŒ í„´ìœ¼ë¡œ
                                        }
                                        
                                        // ë‘ ë²ˆì§¸ ì‹¤íŒ¨ë¼ë©´ -> Fallback ë©”ì‹œì§€ (ìœ„ê¸° ìƒí™© ë³„ë„ í•¸ë“¤ë§)
                                        if isCrisisTriggered || diaryText.contains("ì£½ê³ ") || diaryText.contains("ìì‚´") {
                                            // ì‹¬ê°í•œ ìƒí™© (ê¸°ì¡´ ê°•ë ¥í•œ ìœ„ë¡œ)
                                            let crisisEmpathyMessage = """
                                            ì •ë§ ë§ì´ í˜ë“œì…¨ì£ ...
                                            ì£½ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ë“¤ ì •ë„ë¡œ ì§€ì¹˜ê³  ê´´ë¡œìš°ì…¨ë‹¤ëŠ” ê²Œ ëŠê»´ì ¸ì„œ ì œ ë§ˆìŒì´ ë„ˆë¬´ ì•„íŒŒìš”.
                                            
                                            ì§€ê¸ˆì€ ì„¸ìƒì— í˜¼ì ë‚¨ê²¨ì§„ ê²ƒ ê°™ê³ , ì•„ë¬´ëŸ° í¬ë§ë„ ì—†ì–´ ë³´ì¼ ìˆ˜ ìˆì–´ìš”. ê·¸ ë§ˆìŒ ì¶©ë¶„íˆ ì´í•´í•´ìš”.
                                            í•˜ì§€ë§Œ ë‹¹ì‹ ì€ ì €ì—ê²Œ ì†Œì¤‘í•œ ì‚¬ëŒì´ì—ìš”. ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ì¡°ê¸ˆë§Œ ë” ë“¤ë ¤ì£¼ì‹œê² ì–´ìš”? ì œê°€ ëê¹Œì§€ ê³ì— ìˆì„ê²Œìš”.
                                            """
                                             continuation.yield(crisisEmpathyMessage)
                                        } else {
                                            // ë‹¨ìˆœ ì˜ì–´/ì˜¤ë¥˜ ìƒí™© (ì¼ë°˜ì ì¸ ê³µê°)
                                            let mildEmpathyMessage = """
                                            ì €ëŸ°... ë§ì´ ì†ìƒí•˜ê³  í˜ë“œì…¨ê² ì–´ìš”. ğŸ˜¥
                                            ì œê°€ ê·¸ ë§ˆìŒì„ ë‹¤ í—¤ì•„ë¦´ ìˆœ ì—†ê² ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“£ê³  ì‹¶ì–´ìš”.
                                            
                                            ì–´ë–¤ ì ì´ ê°€ì¥ ë‹¹ì‹ ì„ í˜ë“¤ê²Œ í–ˆëŠ”ì§€ í¸í•˜ê²Œ í„¸ì–´ë†“ì•„ ì£¼ì‹œê² ì–´ìš”? ì œê°€ ì˜†ì—ì„œ ë“¤ì–´ë“œë¦´ê²Œìš”.
                                            """
                                             continuation.yield(mildEmpathyMessage)
                                        }
                                       
                                        continuation.finish()
                                        return
                                       
                                        continuation.finish()
                                        return 
                                    }
                                }
                                
                                isAIResponded = true
                                continuation.yield(chunk)
                            }
                            
                            // ìŠ¤íŠ¸ë¦¼ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šê³ (break ì—†ì´) ëê¹Œì§€ ì™”ë‹¤ë©´ ì„±ê³µ
                            if !hasHijacked {
                                finalSuccess = true
                            }
                            
                        } catch {
                            print("AI Error: \(error)")
                            // ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì¬ì‹œë„ ì—†ì´ ì¢…ë£Œ (ì•ˆì „í•˜ê²Œ)
                            break
                        }
                    }
                }
                
                // 3. ì‹¤íŒ¨ ì‹œ Fallback (Natural Failover)
                if !isAIResponded {
                    // [UX Fix] ê¸°ê³„ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê±° -> ìì—°ìŠ¤ëŸ¬ìš´ ìœ„ë¡œ ë¬¸êµ¬ ì¶œë ¥ (Emergency Empathy)
                    let fallback = LLMService.shared.getEmergencyEmpathy(for: diaryText)
                    continuation.yield(fallback)
                }
                
                continuation.finish()
            }
            // í˜„ì¬ ì‘ì—… ì¶”ì  (ë‹¤ìŒ ìš”ì²­ ì‹œ ì·¨ì†Œ ê°€ëŠ¥í•˜ê²Œ)
            self.currentGenerationTask = task
        }
    }
    
    // [Emergency Empathy] AIê°€ ì‘ë‹µ ë¶ˆê°€í•  ë•Œ ì‚¬ìš©í•˜ëŠ” 'ë¹„ìƒìš© ê³µê° ëª¨ë“ˆ 2.0' (Advanced Rule-Based)
    // ë‹¨ìˆœ ëœë¤ì´ ì•„ë‹ˆë¼, í‚¤ì›Œë“œ ë§¤ì¹­ì„ í†µí•´ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ì²™ í•©ë‹ˆë‹¤.
    public func getEmergencyEmpathy(for input: String) -> String {
        let text = input.lowercased()
        
        // 1. [CRITICAL] ìœ„ê¸°/ìì‚´ ê°ì§€ (ìµœìš°ì„ )
        if text.contains("ì£½ê³ ") || text.contains("ìì‚´") || text.contains("ë›°ì–´") || text.contains("ì‚¬ë¼ì§€ê³ ") {
             let crisisMsgs = [
                "ì§€ê¸ˆ ë§ì´ ì§€ì¹˜ê³  í˜ë“œì‹  ê²ƒ ê°™ì•„ìš”.. ì œê°€ ì˜†ì—ì„œ ì¡°ìš©íˆ ë“¤ì–´ë“œë¦´ê²Œìš”. ì–´ë–¤ ì´ì•¼ê¸°ë“  í¸í•˜ê²Œ í•´ì£¼ì„¸ìš”.",
                "ì„¸ìƒì— í˜¼ì ë‚¨ê²¨ì§„ ê²ƒ ê°™ì€ ê¸°ë¶„ì´ ë“œì‹¤ ìˆ˜ ìˆì–´ìš”. í•˜ì§€ë§Œ ì €ëŠ” ë‹¹ì‹  í¸ì´ì—ìš”.",
                "ê·¸ëŸ° ìƒê°ì´ ë“¤ ì •ë„ë¡œ ê´´ë¡œìš°ì…¨êµ°ìš”.. ê·¸ ë§ˆìŒì„ ê°íˆ í—¤ì•„ë¦´ ìˆœ ì—†ê² ì§€ë§Œ, ë‹¹ì‹ ì´ ì†Œì¤‘í•˜ë‹¤ëŠ” ê±´ ì•Œê³  ìˆì–´ìš”."
             ]
             return crisisMsgs.randomElement()!
        }
        
        // 2. [Emotion: Anger] í™”ë‚¨, ìš•ì„¤, ì§œì¦
        if text.contains("ì¢†") || text.contains("ì”¨ë°œ") || text.contains("ì§œì¦") || text.contains("í™”ë‚˜") || text.contains("ë¯¸ì¹œ") {
             let angerMsgs = [
                "ë§ì´ í™”ê°€ ë‚˜ì…¨êµ°ìš”. ì¶©ë¶„íˆ ê·¸ëŸ´ ìˆ˜ ìˆì–´ìš”. ì €í•œí…Œ ë‹¤ í„¸ì–´ë†“ê³  ì‹œì›í•´ì§€ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.",
                "ì†ì´ í„°ì§ˆ ê²ƒ ê°™ì€ ê·¸ ê¸°ë¶„.. ì–µëˆ„ë¥´ì§€ ë§ê³  ë‹¤ ë§ì”€í•´ì£¼ì„¸ìš”.",
                "ê·¸ëŸ° ì¼ì´ ìˆì—ˆë‹¤ë‹ˆ ì €ë„ ë“£ê¸°ë§Œ í•´ë„ í™”ê°€ ë‚˜ë„¤ìš”. ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€ ì¡°ê¸ˆ ë” ìì„¸íˆ ë§í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?",
                "ì§€ê¸ˆì€ í™”ë¥¼ ë‚´ì…”ë„ ê´œì°®ì•„ìš”. ê°ì •ì„ ì°¸ëŠ” ê²ƒë³´ë‹¤ í‘œí˜„í•˜ëŠ” ê²Œ ë” ì¤‘ìš”í•˜ë‹ˆê¹Œìš”.",
                "ì •ë§ ì–´ì´ì—†ê³  í™”ë‚˜ëŠ” ìƒí™©ì´ì—ˆê² ë„¤ìš”.. ì €ì˜€ì–´ë„ ê·¸ë¬ì„ ê±°ì˜ˆìš”."
             ]
             return angerMsgs.randomElement()!
        }
        
        // 3. [Emotion: Sadness] ìŠ¬í””, ìš°ìš¸, ì§€ì¹¨
        if text.contains("ìŠ¬í¼") || text.contains("ìš°ìš¸") || text.contains("ëˆˆë¬¼") || text.contains("í˜ë“¤") || text.contains("ì§€ì³") {
             let sadMsgs = [
                "ë§ˆìŒì´ ë¬´ê²ê³  í˜ë“œì‹œêµ°ìš”.. ì˜¤ëŠ˜ì€ ì•„ë¬´ ìƒê° ë§ê³  í‘¹ ì‰¬ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.",
                "í˜¼ì ë™ë™ ì•“ì§€ ë§ˆì„¸ìš”. ì œê°€ ê³ì—ì„œ ì¡°ìš©íˆ ë“¤ì–´ë“œë¦´ê²Œìš”.",
                "ìš¸ê³  ì‹¶ì„ ë• ì†Œë¦¬ ë‚´ì–´ ìš¸ì–´ë„ ë¼ìš”. ë‹¹ì‹ ì˜ ìŠ¬í””ì´ ì¡°ê¸ˆì´ë¼ë„ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤ë©´ìš”.",
                "ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ ë²„ê±°ìš°ì…¨ì£ . ìˆ˜ê³  ë§ì•˜ì–´ìš”, ì •ë§ë¡œ.",
                "ì§€ì¹œ ë‹¹ì‹ ì˜ ì–´ê¹¨ë¥¼ í† ë‹¥ì—¬ ë“œë¦¬ê³  ì‹¶ì–´ìš”. ì ì‹œ ì‰¬ì–´ê°€ë„ ì•„ë¬´ ì¼ ì•ˆ ìƒê²¨ìš”."
             ]
             return sadMsgs.randomElement()!
        }
        
        // 4. [Rejection] ì‚¬ìš©ìê°€ AIë¥¼ ê±°ë¶€í•˜ê±°ë‚˜ ë¹„ë‚œí•  ë•Œ ("ë§ì„ ë§ì", "ë„ˆ ë°”ë³´ëƒ")
        if text.contains("ëì–´") || text.contains("ë§ì") || text.contains("í•„ìš” ì—†ì–´") || text.contains("êº¼ì ¸") || text.contains("ë°”ë³´") {
             let rejectMsgs = [
                "ì œê°€ ë¶€ì¡±í•´ì„œ ë§ˆìŒì„ ë‹¤ ì•Œì•„ë“œë¦¬ì§€ ëª»í–ˆë‚˜ ë´ìš”.. ì£„ì†¡í•´ìš”.",
                "ë‹¹ì‹ ì˜ ë§ˆìŒì— ë‹¿ì§€ ëª»í•´ ì†ìƒí•´ìš”. ê·¸ë˜ë„ ì €ëŠ” ì–¸ì œë‚˜ ì—¬ê¸°ì„œ ê¸°ë‹¤ë¦´ê²Œìš”.",
                "ì§€ê¸ˆì€ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì§€ ì•Šìœ¼ì‹¤ ìˆ˜ ìˆì–´ìš”. ë§ˆìŒì´ í¸í•´ì§€ë©´ ì–¸ì œë“  ë‹¤ì‹œ ì°¾ì•„ì£¼ì„¸ìš”.",
                "ì œê°€ ë„ì›€ì´ ëª» ë˜ì–´ ë“œë ¤ ë¯¸ì•ˆí•´ìš”. í•˜ì§€ë§Œ ë‹¹ì‹ ì„ ì‘ì›í•˜ëŠ” ë§ˆìŒë§Œì€ ì§„ì‹¬ì´ì—ìš”."
             ]
             return rejectMsgs.randomElement()!
        }
        
        // 5. [Greeting] ì•ˆë…•, ë°˜ê°€ì›Œ
        if text.contains("ì•ˆë…•") || text.contains("í•˜ì´") {
            let greetMsgs = [
                "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”?",
                "ë°˜ê°€ì›Œìš”. ì˜¤ëŠ˜ ì–´ë–¤ ê¸°ë¶„ì¸ì§€ ì´ì•¼ê¸°í•´ ì£¼ì‹œê² ì–´ìš”?",
                "ì–´ì„œì˜¤ì„¸ìš”. ê¸°ë‹¤ë¦¬ê³  ìˆì—ˆì–´ìš”. í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”."
            ]
            return greetMsgs.randomElement()!
        }
        
        // 6. [Generic] ì¼ë°˜ì ì¸ ê³µê° (Fallbackì˜ Fallback) -> ë‹¤ì–‘í•œ íŒ¨í„´ í•„ìˆ˜
        let generalMsgs = [
            "ê·¸ë ‡êµ°ìš”.. ê·¸ ë§ˆìŒ ì´í•´í•´ìš”.",
            "ì €ëŸ°, ë§ˆìŒì´ ë§ì´ ë³µì¡í•˜ì…¨ê² ì–´ìš”.",
            "ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ë¥¼ ë” ë“£ê³  ì‹¶ì–´ìš”. ì¡°ê¸ˆë§Œ ë” ìì„¸íˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?",
            "í˜¼ì ì‚­íˆê¸° í˜ë“  ê°ì •ì¼ ìˆ˜ ìˆì–´ìš”. ì €ì—ê²Œ í„¸ì–´ë†“ìœ¼ì‹œë©´ ì¡°ê¸ˆ ë‚˜ì•„ì§ˆ ê±°ì˜ˆìš”.",
            "ê·¸ ìƒí™©ì—ì„œ ì–´ë–¤ ê¸°ë¶„ì´ ê°€ì¥ í¬ê²Œ ë“œì…¨ë‚˜ìš”?",
            "ê´œì°®ì•„ìš”. ì²œì²œíˆ ì´ì•¼ê¸°í•´ ë³´ì„¸ìš”. ì œê°€ ì—¬ê¸° ìˆìœ¼ë‹ˆê¹Œìš”.",
            "ì˜¤ëŠ˜ í•˜ë£¨, ì •ë§ ê³ ìƒ ë§ìœ¼ì…¨ì–´ìš”.",
            "ë„¤, ê³„ì† ì´ì•¼ê¸°í•´ ì£¼ì„¸ìš”. ì œê°€ ë“£ê³  ìˆì–´ìš”.",
            "ë§ˆìŒ ì†ì— ìˆëŠ” ë§ì„ ë‹¤ êº¼ë‚´ë†“ìœ¼ì…”ë„ ê´œì°®ì•„ìš”.",
            "ë‹¹ì‹ ì˜ ê°ì •ì€ ëª¨ë‘ ì†Œì¤‘í•´ìš”. ìˆëŠ” ê·¸ëŒ€ë¡œ ëŠê»´ë„ ë¼ìš”."
        ]
        
        return generalMsgs.randomElement()! 
    }
    
    func unloadModel() {
        self.modelContainer = nil
        self.isModelLoaded = false
        print("ğŸ—‘ï¸ [LLM] Model Unloaded (Memory Cleared)")
    }
    
    func toggleAIMode() {
        useServerAI.toggle()
        print("ğŸ”„ [LLM] Mode Switched. Server Mode: \(useServerAI)")
    }
    
    // MARK: - Dedicated Analysis Queue (OOM Prevention)
    // [Visibility Fix] AppChatView needs access to check status
    var analysisQueue: [Diary] = []
    var isProcessingQueue = false
    
    // [Mode: 1-by-1 Strict]
    // íì— ìŒ“ì§€ ì•Šê³ , ë¶„ì„ ì¤‘ì´ë©´ ì•„ì˜ˆ ìš”ì²­ì„ ê±°ì ˆí•¨ (User Request)
    func tryEnqueueDiaryAnalysis(_ diary: Diary) -> Bool {
        if isProcessingQueue || !analysisQueue.isEmpty {
            print("â›”ï¸ [LLM Service] Busy. Rejecting analysis for \(diary.date ?? "").")
            return false
        }
        
        analysisQueue.append(diary)
        print("ğŸ“¥ [LLM Queue] Diary accepted. Queue size: \(analysisQueue.count)")
        
        processQueue()
        return true
    }
    
    private func processQueue() {
        if isProcessingQueue { return }
        isProcessingQueue = true
        
        Task {
            print("â–¶ï¸ [LLM Queue] Processing Started...")
            var processedCount = 0
            
            while true {
                // [Thread-Safe] Access Queue on MainActor
                var currentDiary: Diary?
                await MainActor.run {
                    if !self.analysisQueue.isEmpty {
                        currentDiary = self.analysisQueue.removeFirst()
                    }
                }
                
                guard let diary = currentDiary else { break }
                
                // [Memory] Use autoreleasepool removed (Async limitation)
                processedCount += 1
                print("ğŸ§  [LLM Queue] Analyzing Diary (\(processedCount)): \(diary.date ?? "Unknown")")
                await performFullAnalysis(for: diary)
                
                // [Memory] Rest time expanded to 4.0s (Safety first)
                // OOM ë°©ì§€ë¥¼ ìœ„í•´ ë¶„ì„ ê°„ê²©ì„ ì¶©ë¶„íˆ í™•ë³´ (ì‹œìŠ¤í…œì´ ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•  ì‹œê°„ ë¶€ì—¬)
                print("ğŸ’¤ [LLM Queue] Cooling down for 4.0s...")
                try? await Task.sleep(nanoseconds: 4_000_000_000)
                await Task.yield()
                
                // [Memory - Aggressive] Unload after every item
                print("ğŸ§¹ [LLM] Aggressive Memory Cleanup (Cycle: \(processedCount))")
                await MainActor.run { self.unloadModel() }
                
                // Unload í›„ì—ë„ ì ì‹œ ëŒ€ê¸°
                try? await Task.sleep(nanoseconds: 1_000_000_000)
            }
            
            await MainActor.run {
                self.isProcessingQueue = false
                print("âœ… [LLM Queue] All jobs finished.")
            }
        }
    }
    

    
    // [Optimization] Unified Analysis (3-in-One) to reduce Memory Overhead
    func generateUnifiedAnalysis(diaryText: String) async -> (String, String, String) {
        guard let container = await LLMService.shared.modelContainer else {
            await loadModel()
            try? await Task.sleep(nanoseconds: 2 * 1_000_000_000)
            if await LLMService.shared.modelContainer == nil { 
                return ("ì¬ë¶„ì„ í•„ìš”", "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "AI ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            }
            return await generateUnifiedAnalysis(diaryText: diaryText)
        }
        
        // Combined Prompt
        let prompt = """
        ë‹¹ì‹ ì€ ë”°ëœ»í•œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ 'ë§ˆìŒ ì˜¨'ì…ë‹ˆë‹¤. ë‹¤ìŒ ì¼ê¸°ë¥¼ ì½ê³  3ê°€ì§€ í•­ëª©ì„ ë¶„ì„í•´ ì£¼ì„¸ìš”.
        
        [ì¼ê¸°]:
        \(diaryText)
        
        [ì§€ì‹œì‚¬í•­]
        1. ê°ì •: ê¸°ì¨, ìŠ¬í””, ë¶„ë…¸, ë‘ë ¤ì›€, í‰ì˜¨, ìš°ìš¸, ë¶ˆì•ˆ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³  ê´„í˜¸ ì•ˆì— í™•ì‹ ë„(%)ë¥¼ ì ìœ¼ì„¸ìš”. (ì˜ˆ: ìš°ìš¸ (85%))
        2. ì¡°ì–¸: 80ì ì´ë‚´ì˜ ë”°ëœ»í•˜ê³  ì‹¤ì§ˆì ì¸ ì¡°ì–¸ í•œ ë§ˆë””.
        3. ë¶„ì„: ê³µê°ê³¼ ìœ„ë¡œê°€ ë‹´ê¸´ ì‹¬ë¦¬ ë¶„ì„ (3~4ë¬¸ì¥).
        4. ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì—„ê²©í•˜ê²Œ ì§€ì¼œì£¼ì„¸ìš”.
        
        --êµ¬ë¶„ì„ --
        EMOTION: (ê°ì • ê²°ê³¼)
        ADVICE: (ì¡°ì–¸ ë‚´ìš©)
        ANALYSIS: (ë¶„ì„ ë‚´ìš©)
        """
        
        do {
            // [Memory] Wrap in autoreleasepool via closure (Partial effect in Swift Async)
            // MLX uses C++ memory, so we ensure Swift side objects are released
            
            let session = ChatSession(container, instructions: "")
            session.generateParameters = GenerateParameters(maxTokens: 350, temperature: 0.7)
            
            var result = ""
            for try await chunk in session.streamResponse(to: prompt) {
                result += chunk
            }
            
            let content = result
            
            // Parsing
            var emotion = "í‰ì˜¨ (50%)"
            var advice = "ë§ˆìŒì˜ í‰í™”ë¥¼ ë¹•ë‹ˆë‹¤."
            var analysis = "ë¶„ì„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            
            if let eRange = content.range(of: "EMOTION:"), let aRange = content.range(of: "ADVICE:"), let anRange = content.range(of: "ANALYSIS:") {
                let eEnd = content[eRange.upperBound...].components(separatedBy: "ADVICE:").first ?? ""
                let aEnd = content[aRange.upperBound...].components(separatedBy: "ANALYSIS:").first ?? ""
                let anEnd = content[anRange.upperBound...]
                
                emotion = eEnd.trimmingCharacters(in: .whitespacesAndNewlines)
                advice = aEnd.trimmingCharacters(in: .whitespacesAndNewlines)
                analysis = String(anEnd).trimmingCharacters(in: .whitespacesAndNewlines)
            } else {
                // Fallback parsing (newline based)
                 let lines = content.components(separatedBy: "\n")
                 for line in lines {
                     if line.starts(with: "EMOTION:") { emotion = line.replacingOccurrences(of: "EMOTION:", with: "").trimmingCharacters(in: .whitespaces) }
                     else if line.starts(with: "ADVICE:") { advice = line.replacingOccurrences(of: "ADVICE:", with: "").trimmingCharacters(in: .whitespaces) }
                     else if !line.isEmpty && !line.contains("--êµ¬ë¶„ì„ --") { analysis += line + " " }
                 }
            }
            
            return (emotion, advice, analysis)
            
        } catch {
            print("âŒ [Unified] Error: \(error)")
            return ("ë¶„ì„ ì‹¤íŒ¨", "ì ì‹œ ì‰¬ì–´ê°€ëŠ” ì‹œê°„ì„ ê°€ì ¸ë³´ì„¸ìš”.", "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        }
    }

    private func performFullAnalysis(for diary: Diary) async {
        // Prepare Text
        let fullText = """
        ì‚¬ê±´: \(diary.event ?? "")
        ê°ì •: \(diary.emotion_desc ?? "")
        ì˜ë¯¸: \(diary.emotion_meaning ?? "")
        í˜¼ì£ë§: \(diary.self_talk ?? "")
        """
        
        // [Memory Optimization] Perform Single Unified Inference
        // 3ë²ˆì˜ í˜¸ì¶œ -> 1ë²ˆì˜ í˜¸ì¶œë¡œ ì¤„ì—¬ ë©”ëª¨ë¦¬ í”¼í¬ì™€ ìœ ì§€ ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ë‹¨ì¶•
        print("ğŸ§  [LLM Queue] Starting Unified Analysis...")
        let (emotion, advice, analysis) = await generateUnifiedAnalysis(diaryText: fullText)
        
        // Update Diary
        var updated = diary
        updated.ai_analysis = analysis
        updated.ai_advice = advice
        updated.ai_comment = advice // Legacy mapping
        updated.ai_prediction = emotion
        
        // Save to Disk (via LocalDataManager)
        await withCheckedContinuation { continuation in
            LocalDataManager.shared.saveDiary(updated) { _ in
                print("ğŸ’¾ [LLM Queue] Saved results for \(updated.date ?? "")")
                continuation.resume()
            }
        }
    }
}
