#!/bin/bash

# 0. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í•„ìˆ˜)
export HF_TOKEN="YOUR_HUGGINGFACE_TOKEN_HERE"
export HF_HOME=/Volumes/DATA2/vibe_coding/huggingface_cache

# 1. ëª¨ë¸ ë³‘í•© (Fuse)
# í•™ìŠµëœ ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ê³¼ í•©ì³ì„œ ê³ í•´ìƒë„(F16) ëª¨ë¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
echo "ğŸ”„ [1/2] ëª¨ë¸ ë³‘í•© ì¤‘ (Fusing)..."
~/Library/Python/3.9/bin/mlx_lm.fuse \
  --model google/gemma-2-2b-it \
  --adapter-path adapters_maumon_full \
  --save-path models/maum-on-gemma-2b-v2-full-f16

# 2. 4ë¹„íŠ¸ ì–‘ìí™” (Quantization)
# ë³‘í•©ëœ ëª¨ë¸ì„ ëª¨ë°”ì¼ìš© 1.4GB ì‚¬ì´ì¦ˆë¡œ ì••ì¶•í•©ë‹ˆë‹¤.
echo "ğŸ“¦ [2/2] 4ë¹„íŠ¸ ì–‘ìí™” ì¤‘ (Quantizing)..."
~/Library/Python/3.9/bin/mlx_lm.convert \
  --hf-path models/maum-on-gemma-2b-v2-full-f16 \
  --mlx-path models/maum-on-gemma-2b-v2-full-4bit \
  -q \
  --q-bits 4

echo "âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! 4ë¹„íŠ¸ ëª¨ë¸ ìœ„ì¹˜: models/maum-on-gemma-2b-v2-full-4bit"
